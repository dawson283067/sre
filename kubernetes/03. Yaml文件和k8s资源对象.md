# 一：yaml文件

> 如何写yaml文件？这是重要的一步。
> 如何打镜像？nginx镜像，tomcat镜像，包括很多服务的镜像。都通过dockerfile打好镜像。然后通过yaml文件部署到k8s环境上去。各种场景的服务。

## 1.1：yaml文件基本说明

```sh
# 如何写yaml文件？运行web服务必不可少的环节。
# yaml文件中，缩进表示上下层关系

[root@k8s-master1:~]# cd yaml/20240627/
[root@k8s-master1:~/yaml/20240627]# ls
admin-user.yaml  coredns.yaml  dashboard-v2.5.1.yaml  nginx.yaml  tomcat.yaml

[root@k8s-master1:~/yaml/20240627]# vim tomcat.yaml
kind: Deployment      # 类型
#apiVersion: extensions/v1beta1  
apiVersion: apps/v1   # 不同的k8s不一样，1.18以后就是这个
metadata:      # 元数据，用于定义资源对象的一些属性信息
  labels:      # 用于后期的筛选，谁会用这个deployment呢？HPA控制器，会对你的Pod进行弹性伸缩
    app: myserver-tomcat-app1-deployment-label # 你会有几十个deployment控制，它怎么知道用哪个呢
  name: myserver-tomcat-app1-deployment  # 同一个namespace，这个名字不能重名
  namespace: myserver  # 一个服务可能会多副本，多pod。每个服务一个上面的名字
spec:      # 定义资源对象的详细信息
  replicas: 1    # 副本数
  selector:      # 用于匹配pod的副本。这个地方是用来控制副本数的。控制器会在myserver的ns里面去找这个
    matchLabels: # 这个标签是kv格式的一对数据。一个pod可以有多个标签。这个标签是给deployment用的。
      app: myserver-tomcat-app1-selector
  template:   # pod创建的时候，我怎么知道有哪些标签呢？定义创建pod时的标签。用于给你的pod来继承的。
    metadata: # 凡是通过这个template创建的pod，都会有这个metadata以及labels
      labels:
        app: myserver-tomcat-app1-selector
    spec:     # 这以下是pod的空间
      containers:  # 这是这个pod的容器，注意是个列表，一个pod会有多个容器。
      - name: myserver-tomcat-app1-container
        image: tomcat:7.0.94-alpine 
        #command: ["/apps/tomcat/bin/run_tomcat.sh"]   # 一般会内置一些脚本或者启动命令
        #imagePullPolicy: IfNotPresent                 # 所以这里不需要增加命令
        imagePullPolicy: Always     # 每次重建pod，都会下载镜像。一般用IfNotPresent，不要用Never
        ports:   # 端口声明配置。不是最终决定你的容器暴露在宿主机上。
        - containerPort: 8080
          protocol: TCP
          name: http   # 后面不写，通过service暴露也可以。
        env:   # 给容器定义环境变量。在容器里都可以使用这些环境变量。
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
        resources:  # 定义资源限制的
          limits:  # 在目的主机运行成功后，实际使用的资源。  800m表示0.8核心=80%。1核CPU=1000m毫核
            cpu: 1   # 整数，表示是整数的核心CPU。可以写1.5/1.2浮点数，表示是1.5核CPU，转成150%
            memory: "512Mi"  # 一般都是Gi或者Mi，
          requests:  # 要求目的主机最少要有这些资源。也就是node节点，用于调度的。需要同时符合条件。
            cpu: 500m
            memory: "512Mi"
---
kind: Service   # 用来暴露k8s中容器的服务给k8s的业务访问，或者k8s以外的服务去访问的。
apiVersion: v1
metadata:
  labels:
    app: myserver-tomcat-app1-service-label
  name: myserver-tomcat-app1-service
  namespace: myserver    # 一定要记住：service一定要和pod跑在同一个namespace
spec:
  type: NodePort  # 四种类型：ClusterIP NodePort ExternalName LoadBalancer 
  ports:    # 定义访问端口，可以多个端口
  - name: http
    port: 80      # service端口，可以和pod不一样。service监听的端口。
    protocol: TCP  # 协议类型
    targetPort: 8080  # 目标pod的端口，tomcat就是8080，nginx是80
    nodePort: 40003   # 宿主机的端口
  selector:  # 这里决定service接收到的请求转给哪个目标容器。必须和template中的labels一样。
    app: myserver-tomcat-app1-selector
    
    
如果要自己写yaml文件
kubectl explain deployment 来查

# 如何判断service是否匹配到目的容器
[root@k8s-master1:~/yaml/20240627]# kubectl get svc -n myserver
NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
linux66-tomcat-app1-service   ClusterIP   10.100.254.84   <none>        80/TCP    8h
[root@k8s-master1:~/yaml/20240627]# kubectl get ep -n myserver
NAME                          ENDPOINTS            AGE
linux66-tomcat-app1-service   10.200.55.167:8080   8h

PS：service后面有ep的地址，就说明能匹配到容器。空的话，就是没有容器，或者label写错了，这时候请求一定不通

PS：一般一个项目最多就两三个nodePort。
```

## 1.2：查看API的版本和资源

```sh
[root@k8s-master1:~/yaml/20240627]# kubectl api-versions
admissionregistration.k8s.io/v1
apiextensions.k8s.io/v1
apiregistration.k8s.io/v1
apps/v1
authentication.k8s.io/v1
authorization.k8s.io/v1
autoscaling/v1
autoscaling/v2
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1
coordination.k8s.io/v1
discovery.k8s.io/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1
events.k8s.io/v1beta1
flowcontrol.apiserver.k8s.io/v1beta1
flowcontrol.apiserver.k8s.io/v1beta2
networking.k8s.io/v1
node.k8s.io/v1
node.k8s.io/v1beta1
policy/v1
policy/v1beta1
rbac.authorization.k8s.io/v1
scheduling.k8s.io/v1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1
velero.io/v1

[root@k8s-master1:~/yaml/20240627]# kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v2                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1beta1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
backups                                        velero.io/v1                           true         Backup
backupstoragelocations            bsl          velero.io/v1                           true         BackupStorageLocation
deletebackuprequests                           velero.io/v1                           true         DeleteBackupRequest
downloadrequests                               velero.io/v1                           true         DownloadRequest
podvolumebackups                               velero.io/v1                           true         PodVolumeBackup
podvolumerestores                              velero.io/v1                           true         PodVolumeRestore
resticrepositories                             velero.io/v1                           true         ResticRepository
restores                                       velero.io/v1                           true         Restore
schedules                                      velero.io/v1                           true         Schedule
serverstatusrequests              ssr          velero.io/v1                           true         ServerStatusRequest
volumesnapshotlocations                        velero.io/v1                           true         VolumeSnapshotLocation

# 用NAME和APIVERSION就定义了各种资源。如果依赖于NAMESPACED，那么就是true，如果不依赖，那就是默认default。有的有SHORTNAMES。
# 创建一个容器，如果没有指定namespace，那么默认就是default。
```

## 1.3：生产中分配物理资源

```sh
# yaml文件中资源的分配：上面tomcat的yaml文件。
# 下面是生产中的配置，就是yaml文件中的limits和requests设置。
nginx 静态服务器 2C/2G
               1C/1G（测试环境）
java  动态服务器 2C/2G
			   2C/4G
php            2C/2G			  
go/python/c++  1C/2G  1C/1G   
job/cronjob    0.3C/512Mi     # 本身不需要什么CPU和内存
elasticsearch  4C/12G         # 占用内存更多的服务
mysql          4C/8G

# 比如java服务，加内存还不如增加副本。多个副本比单个容器加配置效果要好。

# 资源不是越多越好。

48C/128G 一台服务器的配置。
CPU分着分着就没了。一个项目有三四十服务。瞬间资源就没了。
如果资源不够，容器就跑不起来了。
```

## 1.4：部署nginx服务

```sh
[root@k8s-master1:~/yaml/20240627]# vim nginx.yaml
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: linux66-nginx-deployment-label
  name: linux66-nginx-deployment
  namespace: linux66
spec:
  replicas: 1
  selector:
    matchLabels:
      app: linux66-nginx-selector
  template:
    metadata:
      labels:
        app: linux66-nginx-selector
    spec:
      containers:
      - name: linux66-nginx-container
        image: nginx
        #command: ["/apps/tomcat/bin/run_tomcat.sh"]
        #imagePullPolicy: IfNotPresent
        imagePullPolicy: Always
        ports:
        - containerPort: 80
          protocol: TCP
          name: http
        - containerPort: 443
          protocol: TCP
          name: https
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
#        resources:
#          limits:
#            cpu: 2
#            memory: 2Gi
#          requests:
#            cpu: 500m
#            memory: 1Gi


---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: linux66-nginx-service-label
  name: linux66-nginx-service
  namespace: linux66
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30006
  - name: https
    port: 443
    protocol: TCP
    targetPort: 443
    nodePort: 30443
  selector:
    app: linux66-nginx-selector

[root@k8s-master1:~/yaml/20240627]# kubectl create ns linux66
namespace/linux66 created

[root@k8s-master1:~/yaml/20240627]# kubectl apply -f nginx.yaml 
deployment.apps/linux66-nginx-deployment created
service/linux66-nginx-service created

[root@k8s-master1:~/yaml/20240627]# kubectl get pod -n linux66
NAME                                        READY   STATUS    RESTARTS   AGE
linux66-nginx-deployment-7d8d8dcdf4-4shrs   1/1     Running   0          45s

[root@k8s-master1:~/yaml/20240627]# kubectl get pod -n linux66 -o wide
NAME                                        READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
linux66-nginx-deployment-7d8d8dcdf4-4shrs   1/1     Running   0          72s   10.200.70.65   172.31.8.113   <none>           <none>

# 访问http://172.31.8.113:30006
```

![](assets/images/nginx01.png)

```sh
# 实际需要一个负载均衡器来访问，我们这里就使用ha1来充当负载均衡器

[root@k8s-ha1:~]# vim /etc/haproxy/haproxy.cfg
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	# Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
	log	global
	mode	http
	option	httplog
	option	dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http

listen k8s-api-6443
  bind 172.31.8.188:6443
  mode tcp
  server master1 172.31.8.101:6443 check inter 3s fall 3 rise 1
  server master2 172.31.8.102:6443 check inter 3s fall 3 rise 1
  server master3 172.31.8.103:6443 check inter 3s fall 3 rise 1

listen sunnycreek-linux66-nginx-80
  bind 172.31.8.189:80
  mode tcp
  server node1 172.31.8.111:30006 check inter 3s fall 3 rise 1
  server node2 172.31.8.112:30006 check inter 3s fall 3 rise 1
  server node3 172.31.8.113:30006 check inter 3s fall 3 rise 1

# 
[root@k8s-ha1:~]# systemctl reload haproxy

[root@k8s-ha1:~]# ss -ntl
State        Recv-Q       Send-Q              Local Address:Port                Peer Address:Port       Process       
LISTEN       0            128                     127.0.0.1:6010                     0.0.0.0:*                        
LISTEN       0            64                        0.0.0.0:2049                     0.0.0.0:*                        
LISTEN       0            4096                      0.0.0.0:31940                    0.0.0.0:*                        
LISTEN       0            4096                      0.0.0.0:35848                    0.0.0.0:*                        
LISTEN       0            20480                172.31.8.188:6443                     0.0.0.0:*                        
LISTEN       0            4096                      0.0.0.0:111                      0.0.0.0:*                        
LISTEN       0            20480                172.31.8.189:80                       0.0.0.0:*                        
LISTEN       0            4096                      0.0.0.0:14288                    0.0.0.0:*                        
LISTEN       0            64                        0.0.0.0:14866                    0.0.0.0:*                        
LISTEN       0            4096                127.0.0.53%lo:53                       0.0.0.0:*                        
LISTEN       0            128                       0.0.0.0:22                       0.0.0.0:*                        
LISTEN       0            128                         [::1]:6010                        [::]:*                        
LISTEN       0            4096                         [::]:45792                       [::]:*                        
LISTEN       0            64                           [::]:2049                        [::]:*                        
LISTEN       0            64                           [::]:25730                       [::]:*                        
LISTEN       0            4096                         [::]:48748                       [::]:*                        
LISTEN       0            4096                         [::]:111                         [::]:*                        
LISTEN       0            128                          [::]:22                          [::]:*                        
LISTEN       0            4096                         [::]:44406                       [::]:*     

访问172.31.8.189，做个域名解析，类似于一个公网地址了。
```

![](assets/images/nginx02.png)

## 1.5：部署tomcat服务

```sh
[root@k8s-master1:~/yaml/20240627]# vim tomcat.yaml
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: linux66-tomcat-app1-deployment-label
  name: linux66-tomcat-app1-deployment
  namespace: linux66 
spec:
  replicas: 1
  selector:
    matchLabels:
      app: linux66-tomcat-app1-selector
  template:
    metadata:
      labels:
        app: linux66-tomcat-app1-selector
    spec:
      containers:
      - name: linux66-tomcat-app1-container
        image: tomcat:7.0.94-alpine 
        #command: ["/apps/tomcat/bin/run_tomcat.sh"]
        #imagePullPolicy: IfNotPresent
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
        resources:
          limits:
            cpu: 1
            memory: "512Mi"
          requests:
            cpu: 500m
            memory: "512Mi"
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: linux66-tomcat-app1-service-label
  name: linux66-tomcat-app1-service
  namespace: linux66
spec:
  #type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    #nodePort: 40003
  selector:
    app: linux66-tomcat-app1-selector

PS：tomcat不需要暴露端口。

[root@k8s-master1:~/yaml/20240627]# kubectl apply -f tomcat.yaml 
deployment.apps/linux66-tomcat-app1-deployment created
service/linux66-tomcat-app1-service created

[root@k8s-master1:~/yaml/20240627]# kubectl get pod -n linux66
NAME                                              READY   STATUS    RESTARTS   AGE
linux66-nginx-deployment-7d8d8dcdf4-4shrs         1/1     Running   0          27m
linux66-tomcat-app1-deployment-667c9cf879-nfc2s   1/1     Running   0          98s


# 接下来，如何让nginx将请求转发给tomcat?
# 这个就是公司里常见的配置。这就和k8s没什么关系了。

# 在tomcat容器中定义一个jsp页面
[root@k8s-master1:~/yaml/20240627]# kubectl exec -it linux66-tomcat-app1-deployment-667c9cf879-nfc2s -n linux66 -- bash
bash-4.4# pwd
/usr/local/tomcat
bash-4.4# ls
BUILDING.txt     NOTICE           RUNNING.txt      include          native-jni-lib   work
CONTRIBUTING.md  README.md        bin              lib              temp
LICENSE          RELEASE-NOTES    conf             logs             webapps
bash-4.4# cd webapps/
bash-4.4# ls
ROOT          docs          examples      host-manager  manager
bash-4.4# mkdir login
bash-4.4# echo "<h1>login web page</h1>" > login/index.jsp
bash-4.4# exit
exit

# 配置nginx容器
[root@k8s-master2:~]# kubectl get svc -n linux66
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
linux66-nginx-service         NodePort    10.100.155.174   <none>        80:30006/TCP,443:30443/TCP   33m
linux66-tomcat-app1-service   ClusterIP   10.100.105.10    <none>        80/TCP                       7m46s

[root@k8s-master1:~/yaml/20240627]# kubectl exec -it linux66-nginx-deployment-7d8d8dcdf4-4shrs -n linux66 -- bash
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/# pwd
/
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/# curl linux66-tomcat-app1-service/login/index.jsp
<h1>login web page</h1>
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/# exit
exit

PS：
宿主机nodeport 30006 --> nginx servcie --> nginx pod --> tomcat service --> tomcat pod1
                                                                           tomcat pod2
宿主机nodeport前面还有LB 
每个node节点都会监听。


# 目前虽然nginx可以访问tomcat，但是由于没有配nginx转发规则。

[root@k8s-master1:~/yaml/20240627]# kubectl exec -it linux66-nginx-deployment-7d8d8dcdf4-4shrs -n linux66 -- bash
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/# cd /etc/nginx/
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx# ls
conf.d	fastcgi_params	mime.types  modules  nginx.conf  scgi_params  uwsgi_params
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx# apt update
......
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx# apt install vim -y
......
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx# cd conf.d
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx/conf.d# vim default.conf 
......
    location /login {
        proxy_pass http://linux66-tomcat-app1-service.linux66.svc.sunnycreek.local;
    }
......
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx/conf.d# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx/conf.d# nginx -s reload
2024/06/29 04:07:07 [notice] 293#293: signal process started
root@linux66-nginx-deployment-7d8d8dcdf4-4shrs:/etc/nginx/conf.d# exit
exit


访问http://
PS：说明转发成功
基于nginx的动静分离
```

![](assets/images/nginx03_tomcat.png)



```sh
很多公司倾向于用ingress，其实里面还是nginx。
通常还是基于域名的转发。转发到service。来识别后面的pod。再把请求转发过去。
service使用ipvs
```

## 1.6：yaml和json文件的区别

```sh
json特点:
json是嵌套key-value数据。
json文件不能加注释，导致可读性比较差。
语法要求很严格。常见用于API返回值，统计数据等。
有的也可以用于配置文件，比如jenkins。但是维护非常麻烦。

yaml格式文件：
大小写敏感
使用缩进表示层级关系，一般是两个空格
缩进时不允许使用Tab键，只允许使用空格
缩进的空格数目不重要，只要相同层级的元素左侧对齐即可
使用“#”表示注释，从这个字符一直到行尾，都会被解析器忽略
比json更适用于配置文件
```

# 二：k8s的API

> 以下内容是将为业务容器化奠定良好基础

## 2.1：k8s逻辑运行环境

图

## 2.2：k8s的设计理念 -- 分层架构

```sh
## 五层架构

# 云原生生态系统（不属于k8s环境内部）
云原生生态系统是在接口层之上的kubernetes集群管理调度生态系统，有两个范畴：
kubernetes内部：CRI、CNI、CSI、镜像仓库
kubernetes外部：监控、日志、CI/CD、配置管理（Apollo）等

# 接口层：客户端库和实用工具
接口层包括kubectl命令行客户端工具和SDK等客户端访问接口

# 管理层：自动化和策略管理
管理层实现自动化（如pod弹性伸缩、动态存储卷创建和回收）和策略管理（资源限制、RBAC授权、NetworkPolicy等，主要解决项目之间的pod隔离，网络安全和自动伸缩功能。）

# 应用层：部署和路由
应用层实现服务部署（无状态应用如nginx使用deployment，有状态应用如MySQL主从使用statefulSet）和路由（通过Service实现服务之间调用）

# 核心层：kubernetes API和执行环境
核心层包括kubernetes的API、控制器、service、namespace、node等，是运行pod的基础环境


有的中小公司，数据库放到k8s中。
大公司，就会把数据库放到物理机中跑。
------------------------------------------------------------------------------------------------------------
# k8s 1.9之后才解决了有状态服务部署的问题。
以MySQL为例：主从
往master写，再同步给slave节点。
那么启动容器的时候，如何控制启动顺序？
先启动master，等master启动后，再启动slave。必须保证主服务正常运行，然后再启动slave节点。
TCP连接建立后，再用数据同步工具进行数据同步。每个slave都会向master同步数据。
Deployment解决不了启动顺序问题。
Statefulset可以解决MySQL的问题。有状态指的是角色不一样。比如一个master，一个slave，这才是状态。
只有第一个启动后，才能有第二个。

有状态服务一般都依赖于共享存储
用共享存储的目的，一个是数据持久化，还一个是数据共享（读同一块数据）
```

## 2.3：k8s的设计理念 -- API 设计原则

```sh
### k8s的设计理念--API设计原则：主要是面向开发的，了解即可。

1、所有API应该是声明式的。
声明式是指不需要关心和定义操作是怎么执行的。

2、API对象是彼此互补而且可组合的。
创建容器的时候，可以使用deployment，如果想要外部访问，创建一个service。这就是互补。

3、高层API以操作意图为基础设计。
面向开发者的。比如端口的设计，拉取镜像等。

4、低层API根据高层API的控制需要设计。
node和master之间的交互，这是低层API，在开发时候比较重要。
kubelet，kube-proxy等。

5、尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制。
完全把k8s的API隐藏了，这样不好。

6、API操作复杂度与对象数量成正比。
开始是deployment，后面还有statefulset。

7、API对象状态不能依赖于网络连接状态。
kube-scheduler调度是逐渐的。发给API后，是内部执行的。要有顺序，不是即时完成的。

8、尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的。
多数据中心内部etcd的同步是有延迟的。etcd和zookeeper的过半机制，一旦完成一半的写入，就算完成。
主的写完，将数据复制到另一个就算写入成功。最后的再慢慢同步。这样能保证效率和完整性。
```

## 2.4：k8s API简介

```sh
## kubernetes API简介

# k8s内置API：部署好k8s集群后自带的API接口
[root@k8s-master1:~]# kubectl api-resources
......

# 自定义资源：CRD（Custom Resource Definition）：部署kubernetes之后通过安装其它组件等方式扩展出来的API。比如大公司自己加的，二次开发的。
velero就是自定义资源，如果没有安装就是没有的

# API命名方式
/apis/myapi/v1/pods
apis  ---- 分类
myapi ---- API组
v1    ---- API版本
pods  ---- 资源对象

如果开发的话，要遵从这些命令规则
对于我们来说就是使用内置API。

------------------------------------------------------------------------------------------------------------
# 安装dashboard的时候，创建过一个admin-user用户
# 这个用户创建好后，k8s会生成一个随机的token，这就是令牌。
[root@k8s-master1:~/yaml/20240627]# vim admin-user.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard


[root@k8s-master1:~]# kubectl get secrets -A | grep admin
kubernetes-dashboard   admin-user-token-cdh8k                           kubernetes.io/service-account-token   3      42h

[root@k8s-master1:~/yaml/20240627]# kubectl describe secrets admin-user-token-cdh8k -n kubernetes-dashboard 
Name:         admin-user-token-cdh8k
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: f6f3f2bd-3247-414b-beae-3530cfe1fe5b

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1302 bytes
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InVHeWpKMFVtTHZZWndyOHNMTURFNUFCeEQ4Zl9EbjdYdnhscXdKclVGaDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNkaDhrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNmYzZjJiZC0zMjQ3LTQxNGItYmVhZS0zNTMwY2ZlMWZlNWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.s6ISwp7q5tqUI10Ztj0k-s35QE1tSim3I0xcq5Re99RRoZISpIOsCqr8Wa8e1veKA3mrHDfoJxEp3jdyJKc659hG0Gu5a4UEIHG41q1KZK4sACNN3K0yEpkVqGY8d_jWS442XqKh-d8QAcIi5Xe1q03I2LzWMlshg2F3U1JJlafM27xbWo-28kxfSHVNnrRNQ9MaIT0mKThiDdbTLJmHBGrSDAq0JC0jcPbO2hpmSRCRIjzy2CwN1pkGaTs6cHWJrrbsu_B_VTMgqo4GW_ZGb4z2HyWVcvkEKVbcSzC5BIM8LBUs7eIuR4Lg8k2xeD-zpQC-oakdD8b1XbjWmeZQjw

# 拼凑curl命令访问
curl --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InVHeWpKMFVtTHZZWndyOHNMTURFNUFCeEQ4Zl9EbjdYdnhscXdKclVGaDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNkaDhrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNmYzZjJiZC0zMjQ3LTQxNGItYmVhZS0zNTMwY2ZlMWZlNWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.s6ISwp7q5tqUI10Ztj0k-s35QE1tSim3I0xcq5Re99RRoZISpIOsCqr8Wa8e1veKA3mrHDfoJxEp3jdyJKc659hG0Gu5a4UEIHG41q1KZK4sACNN3K0yEpkVqGY8d_jWS442XqKh-d8QAcIi5Xe1q03I2LzWMlshg2F3U1JJlafM27xbWo-28kxfSHVNnrRNQ9MaIT0mKThiDdbTLJmHBGrSDAq0JC0jcPbO2hpmSRCRIjzy2CwN1pkGaTs6cHWJrrbsu_B_VTMgqo4GW_ZGb4z2HyWVcvkEKVbcSzC5BIM8LBUs7eIuR4Lg8k2xeD-zpQC-oakdD8b1XbjWmeZQjw" https://172.31.8.101:6443

[root@k8s-master1:~]# curl --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InVHeWpKMFVtTHZZWndyOHNMTURFNUFCeEQ4Zl9EbjdYdnhscXdKclVGaDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNkaDhrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNmYzZjJiZC0zMjQ3LTQxNGItYmVhZS0zNTMwY2ZlMWZlNWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.s6ISwp7q5tqUI10Ztj0k-s35QE1tSim3I0xcq5Re99RRoZISpIOsCqr8Wa8e1veKA3mrHDfoJxEp3jdyJKc659hG0Gu5a4UEIHG41q1KZK4sACNN3K0yEpkVqGY8d_jWS442XqKh-d8QAcIi5Xe1q03I2LzWMlshg2F3U1JJlafM27xbWo-28kxfSHVNnrRNQ9MaIT0mKThiDdbTLJmHBGrSDAq0JC0jcPbO2hpmSRCRIjzy2CwN1pkGaTs6cHWJrrbsu_B_VTMgqo4GW_ZGb4z2HyWVcvkEKVbcSzC5BIM8LBUs7eIuR4Lg8k2xeD-zpQC-oakdD8b1XbjWmeZQjw" https://172.31.8.101:6443{
  "paths": [
    "/.well-known/openid-configuration",
    "/api",
    "/api/v1",
    "/apis",
    "/apis/",
    "/apis/admissionregistration.k8s.io",
    "/apis/admissionregistration.k8s.io/v1",
    "/apis/apiextensions.k8s.io",
    "/apis/apiextensions.k8s.io/v1",
    "/apis/apiregistration.k8s.io",
    "/apis/apiregistration.k8s.io/v1",
    "/apis/apps",
    "/apis/apps/v1",
    "/apis/authentication.k8s.io",
    "/apis/authentication.k8s.io/v1",
    "/apis/authorization.k8s.io",
    "/apis/authorization.k8s.io/v1",
    "/apis/autoscaling",
    "/apis/autoscaling/v1",
    "/apis/autoscaling/v2",
    "/apis/autoscaling/v2beta1",
    "/apis/autoscaling/v2beta2",
    "/apis/batch",
    "/apis/batch/v1",
    "/apis/batch/v1beta1",
    "/apis/certificates.k8s.io",
    "/apis/certificates.k8s.io/v1",
    "/apis/coordination.k8s.io",
    "/apis/coordination.k8s.io/v1",
    "/apis/discovery.k8s.io",
    "/apis/discovery.k8s.io/v1",
    "/apis/discovery.k8s.io/v1beta1",
    "/apis/events.k8s.io",
    "/apis/events.k8s.io/v1",
    "/apis/events.k8s.io/v1beta1",
    "/apis/flowcontrol.apiserver.k8s.io",
    "/apis/flowcontrol.apiserver.k8s.io/v1beta1",
    "/apis/flowcontrol.apiserver.k8s.io/v1beta2",
    "/apis/networking.k8s.io",
    "/apis/networking.k8s.io/v1",
    "/apis/node.k8s.io",
    "/apis/node.k8s.io/v1",
    "/apis/node.k8s.io/v1beta1",
    "/apis/policy",
    "/apis/policy/v1",
    "/apis/policy/v1beta1",
    "/apis/rbac.authorization.k8s.io",
    "/apis/rbac.authorization.k8s.io/v1",
    "/apis/scheduling.k8s.io",
    "/apis/scheduling.k8s.io/v1",
    "/apis/storage.k8s.io",
    "/apis/storage.k8s.io/v1",
    "/apis/storage.k8s.io/v1beta1",
    "/apis/velero.io",
    "/apis/velero.io/v1",
    "/healthz",
    "/healthz/autoregister-completion",
    "/healthz/etcd",
    "/healthz/log",
    "/healthz/ping",
    "/healthz/poststarthook/aggregator-reload-proxy-client-cert",
    "/healthz/poststarthook/apiservice-openapi-controller",
    "/healthz/poststarthook/apiservice-registration-controller",
    "/healthz/poststarthook/apiservice-status-available-controller",
    "/healthz/poststarthook/bootstrap-controller",
    "/healthz/poststarthook/crd-informer-synced",
    "/healthz/poststarthook/generic-apiserver-start-informers",
    "/healthz/poststarthook/kube-apiserver-autoregistration",
    "/healthz/poststarthook/priority-and-fairness-config-consumer",
    "/healthz/poststarthook/priority-and-fairness-config-producer",
    "/healthz/poststarthook/priority-and-fairness-filter",
    "/healthz/poststarthook/rbac/bootstrap-roles",
    "/healthz/poststarthook/scheduling/bootstrap-system-priority-classes",
    "/healthz/poststarthook/start-apiextensions-controllers",
    "/healthz/poststarthook/start-apiextensions-informers",
    "/healthz/poststarthook/start-cluster-authentication-info-controller",
    "/healthz/poststarthook/start-kube-aggregator-informers",
    "/healthz/poststarthook/start-kube-apiserver-admission-initializer",
    "/livez",
    "/livez/autoregister-completion",
    "/livez/etcd",
    "/livez/log",
    "/livez/ping",
    "/livez/poststarthook/aggregator-reload-proxy-client-cert",
    "/livez/poststarthook/apiservice-openapi-controller",
    "/livez/poststarthook/apiservice-registration-controller",
    "/livez/poststarthook/apiservice-status-available-controller",
    "/livez/poststarthook/bootstrap-controller",
    "/livez/poststarthook/crd-informer-synced",
    "/livez/poststarthook/generic-apiserver-start-informers",
    "/livez/poststarthook/kube-apiserver-autoregistration",
    "/livez/poststarthook/priority-and-fairness-config-consumer",
    "/livez/poststarthook/priority-and-fairness-config-producer",
    "/livez/poststarthook/priority-and-fairness-filter",
    "/livez/poststarthook/rbac/bootstrap-roles",
    "/livez/poststarthook/scheduling/bootstrap-system-priority-classes",
    "/livez/poststarthook/start-apiextensions-controllers",
    "/livez/poststarthook/start-apiextensions-informers",
    "/livez/poststarthook/start-cluster-authentication-info-controller",
    "/livez/poststarthook/start-kube-aggregator-informers",
    "/livez/poststarthook/start-kube-apiserver-admission-initializer",
    "/logs",
    "/metrics",
    "/openapi/v2",
    "/openid/v1/jwks",
    "/readyz",
    "/readyz/autoregister-completion",
    "/readyz/etcd",
    "/readyz/informer-sync",
    "/readyz/log",
    "/readyz/ping",
    "/readyz/poststarthook/aggregator-reload-proxy-client-cert",
    "/readyz/poststarthook/apiservice-openapi-controller",
    "/readyz/poststarthook/apiservice-registration-controller",
    "/readyz/poststarthook/apiservice-status-available-controller",
    "/readyz/poststarthook/bootstrap-controller",
    "/readyz/poststarthook/crd-informer-synced",
    "/readyz/poststarthook/generic-apiserver-start-informers",
    "/readyz/poststarthook/kube-apiserver-autoregistration",
    "/readyz/poststarthook/priority-and-fairness-config-consumer",
    "/readyz/poststarthook/priority-and-fairness-config-producer",
    "/readyz/poststarthook/priority-and-fairness-filter",
    "/readyz/poststarthook/rbac/bootstrap-roles",
    "/readyz/poststarthook/scheduling/bootstrap-system-priority-classes",
    "/readyz/poststarthook/start-apiextensions-controllers",
    "/readyz/poststarthook/start-apiextensions-informers",
    "/readyz/poststarthook/start-cluster-authentication-info-controller",
    "/readyz/poststarthook/start-kube-aggregator-informers",
    "/readyz/poststarthook/start-kube-apiserver-admission-initializer",
    "/readyz/shutdown",
    "/version"
  ]
}[root@k8s-master1:~]# 

PS：会显示目前所有的API

[root@k8s-master1:~]# curl --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InVHeWpKMFVtTHZZWndyOHNMTURFNUFCeEQ4Zl9EbjdYdnhscXdKclVGaDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNkaDhrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNmYzZjJiZC0zMjQ3LTQxNGItYmVhZS0zNTMwY2ZlMWZlNWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.s6ISwp7q5tqUI10Ztj0k-s35QE1tSim3I0xcq5Re99RRoZISpIOsCqr8Wa8e1veKA3mrHDfoJxEp3jdyJKc659hG0Gu5a4UEIHG41q1KZK4sACNN3K0yEpkVqGY8d_jWS442XqKh-d8QAcIi5Xe1q03I2LzWMlshg2F3U1JJlafM27xbWo-28kxfSHVNnrRNQ9MaIT0mKThiDdbTLJmHBGrSDAq0JC0jcPbO2hpmSRCRIjzy2CwN1pkGaTs6cHWJrrbsu_B_VTMgqo4GW_ZGb4z2HyWVcvkEKVbcSzC5BIM8LBUs7eIuR4Lg8k2xeD-zpQC-oakdD8b1XbjWmeZQjw" https://172.31.8.101:6443/livez/etcd
ok[root@k8s-master1:~]# 

PS：显示ok，说明从api server到etcd是连接成功的。

[root@k8s-master1:~]# curl --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InVHeWpKMFVtTHZZWndyOHNMTURFNUFCeEQ4Zl9EbjdYdnhscXdKclVGaDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNkaDhrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNmYzZjJiZC0zMjQ3LTQxNGItYmVhZS0zNTMwY2ZlMWZlNWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.s6ISwp7q5tqUI10Ztj0k-s35QE1tSim3I0xcq5Re99RRoZISpIOsCqr8Wa8e1veKA3mrHDfoJxEp3jdyJKc659hG0Gu5a4UEIHG41q1KZK4sACNN3K0yEpkVqGY8d_jWS442XqKh-d8QAcIi5Xe1q03I2LzWMlshg2F3U1JJlafM27xbWo-28kxfSHVNnrRNQ9MaIT0mKThiDdbTLJmHBGrSDAq0JC0jcPbO2hpmSRCRIjzy2CwN1pkGaTs6cHWJrrbsu_B_VTMgqo4GW_ZGb4z2HyWVcvkEKVbcSzC5BIM8LBUs7eIuR4Lg8k2xeD-zpQC-oakdD8b1XbjWmeZQjw" https://172.31.8.101:6443/version
{
  "major": "1",
  "minor": "23",
  "gitVersion": "v1.23.5",
  "gitCommit": "c285e781331a3785a7f436042c65c5641ce8a9e9",
  "gitTreeState": "clean",
  "buildDate": "2022-03-16T15:52:18Z",
  "goVersion": "go1.17.8",
  "compiler": "gc",
  "platform": "linux/amd64"
}[root@k8s-master1:~]# curl --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InVHeWpKMFVtTHZZWndyOHNMTURFNUFCeEQ4Zl9EbjdYdnhscXdKclVGaDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWNkaDhrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNmYzZjJiZC0zMjQ3LTQxNGItYmVhZS0zNTMwY2ZlMWZlNWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.s6ISwp7q5tqUI10Ztj0k-s35QE1tSim3I0xcq5Re99RRoZISpIOsCqr8Wa8e1veKA3mrHDfoJxEp3jdyJKc659hG0Gu5a4UEIHG41q1KZK4sACNN3K0yEpkVqGY8d_jWS442XqKh-d8QAcIi5Xe1q03I2LzWMlshg2F3U1JJlafM27xbWo-28kxfSHVNnrRNQ9MaIT0mKThiDdbTLJmHBGrSDAq0JC0jcPbO2hpmSRCRIjzy2CwN1pkGaTs6cHWJrrbsu_B_VTMgqo4GW_ZGb4z2HyWVcvkEKVbcSzC5BIM8LBUs7eIuR4Lg8k2xeD-zpQC-oakdD8b1XbjWmeZQjw" https://172.31.8.101:6443/api
{
  "kind": "APIVersions",
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "172.31.8.101:6443"
    }
  ]
}[root@k8s-master1:~]# 

PS：很多二次开发就是通过这些API来做处理的，比如增删改查等。
返回都是JSON格式的数据。
```

## 2.5：k8s内置资源对象简介

```sh
## Kubernetes内置资源对象简介：

# 资源对象：
Pod、ReplicaSet、ReplicationController、Deployment、StatefulSet、DaemonSet、Job、CronJob、HorizontalPodAutoscaling、Node、Namespace、Service、Ingress、Label、CustomResourceDefinition

# 存储对象：
Volume、PersistentVolume、PersistentVolumeClaim、Secret、ConfigMap

# 策略对象：
SecurityContext、ResourceQuota、LimitRange

# 身份对象：
ServcieAccount、Role、ClusterRole
```

## 2.6：k8s资源对象操作命令

| 命令集       | 命令                                            | 用途                  |
| ------------ | ----------------------------------------------- | --------------------- |
| 基础命令     | create/delete/edit/get/describe/logs/exec/scale | 增删改查              |
|              | explain                                         | 命令说明              |
| 配置命令     | Label：给node标记label，实现pod与node亲和性     | 标签管理              |
|              | apply                                           | 动态配置              |
| 集群管理命令 | cluster-info/top                                | 集群状态              |
|              | cordon：警戒线，标记node不被调度                | node节点管理          |
|              | uncordon：取消警戒线标记为cordon的node          | node节点管理          |
|              | drain：驱逐node上的pod，用于node下线等场景      | node节点管理          |
|              | taint：给node标记污点，实现pod与node反亲和性    | node节点管理          |
|              | api-resources/api-versions/version              | api资源               |
|              | config                                          | 客户端kube-config配置 |

```sh
[root@k8s-master1:~]# kubectl cluster-info
Kubernetes control plane is running at https://127.0.0.1:6443
CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

[root@k8s-master1:~]# kubectl top node
error: Metrics API not available

# cordon和uncordon
[root@k8s-master1:~]# kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
172.31.8.101   Ready,SchedulingDisabled   master   2d    v1.23.5
172.31.8.102   Ready,SchedulingDisabled   master   2d    v1.23.5
172.31.8.103   Ready,SchedulingDisabled   master   14h   v1.23.5
172.31.8.111   Ready                      node     2d    v1.23.5
172.31.8.112   Ready                      node     2d    v1.23.5
172.31.8.113   Ready                      node     15h   v1.23.5
[root@k8s-master1:~]# kubectl cordon 172.31.8.113
node/172.31.8.113 cordoned
[root@k8s-master1:~]# kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
172.31.8.101   Ready,SchedulingDisabled   master   2d    v1.23.5
172.31.8.102   Ready,SchedulingDisabled   master   2d    v1.23.5
172.31.8.103   Ready,SchedulingDisabled   master   14h   v1.23.5
172.31.8.111   Ready                      node     2d    v1.23.5
172.31.8.112   Ready                      node     2d    v1.23.5
172.31.8.113   Ready,SchedulingDisabled   node     15h   v1.23.5
[root@k8s-master1:~]# kubectl uncordon 172.31.8.113
node/172.31.8.113 uncordoned
[root@k8s-master1:~]# kubectl get nodes
NAME           STATUS                     ROLES    AGE   VERSION
172.31.8.101   Ready,SchedulingDisabled   master   2d    v1.23.5
172.31.8.102   Ready,SchedulingDisabled   master   2d    v1.23.5
172.31.8.103   Ready,SchedulingDisabled   master   14h   v1.23.5
172.31.8.111   Ready                      node     2d    v1.23.5
172.31.8.112   Ready                      node     2d    v1.23.5
172.31.8.113   Ready                      node     15h   v1.23.5

# 显示客户端信息
[root@k8s-master1:~]# kubectl config get-clusters
NAME
cluster1
[root@k8s-master1:~]# kubectl config get-users
NAME
admin
[root@k8s-master1:~]# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    user: admin
  name: context-cluster1
current-context: context-cluster1
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
```

# 三：k8s资源对象详解和示例

## 3.1：k8s的几个重要概念

```sh
https://landscape.cncf.io/ CNCF景观图

## k8s的几个重要概念

# 资源对象：k8s基于声明式API，和资源对象进行交互。

# yaml文件：为了方便后期管理，通过使用yaml文件通过API管理资源对象。

# yaml必需字段，五大顶级字段
1、apiVersion -- 创建该对象所使用的Kubernetes API的版本
如果不写，会报告请求资源不合法的错误。

2、kind -- 想要创建的对象的类型
是service，deployment等。

3、metadata -- 定义识别对象唯一性的数据，包括一个name名称、可选的namespace
namespace是可选的。同一个项目的资源对象，都要指定同一个namespace。如果不指定，就创建到default里。
name必须写的。
labels是可选的。类似于人的外号。目的是方便其它资源对象对这个资源对象进行筛选，比如HPA，通过labels来对它进行伸缩等。

4、spec -- 定义资源对象的详细规范信息（统一的label标签、容器名称、镜像、端口映射等）

5、status (Pod创建完成后k8s自动生成status状态)
```

## 3.2：资源对象的定义说明（重要）

```sh
[root@k8s-master1:~]# cd yaml/20240627/
[root@k8s-master1:~/yaml/20240627]# cat nginx.yaml 
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: linux66-nginx-deployment-label
  name: linux66-nginx-deployment
  namespace: linux66
spec:
  replicas: 1
  selector:
    matchLabels:
      app: linux66-nginx-selector
  template:
    metadata:
      labels:
        app: linux66-nginx-selector
    spec:
      containers:
      - name: linux66-nginx-container
        image: nginx
        #command: ["/apps/tomcat/bin/run_tomcat.sh"]
        #imagePullPolicy: IfNotPresent
        imagePullPolicy: Always
        ports:
        - containerPort: 80
          protocol: TCP
          name: http
        - containerPort: 443
          protocol: TCP
          name: https
        env:
        - name: "password"
          value: "123456"
        - name: "age"
          value: "18"
#        resources:
#          limits:
#            cpu: 2
#            memory: 2Gi
#          requests:
#            cpu: 500m
#            memory: 1Gi


---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: linux66-nginx-service-label
  name: linux66-nginx-service
  namespace: linux66
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30006
  - name: https
    port: 443
    protocol: TCP
    targetPort: 443
    nodePort: 30443
  selector:
    app: linux66-nginx-selector

## yaml文件重要说明
# selector是个筛选器，matchlabels是做标签匹配的。
# Deployment控制器就是通过这个selector来筛选有几个pod正在运行的。
# 如果副本数是1，就正确。如果副本数是2，就是删除一个pod。

# 如果副本数是0，就少了，需要创建。按照template下面定义的字段创建。
# pod要打个label。主要是方便Deployment控制器进行筛选的。
# template会被pod继承。
# 在spec定义pod的详细信息。一个pod可以有多个container。
# 所有pod都会继承template中metadata里的labels信息。每个pod都会有这里的labels信息。
# 一旦pod启动后，Deployment控制器发现有了，就维持副本数不动，直到发生异常。
# 一个pod可以有多个容器。它是个列表格式。

## yaml文件及必需字段：
# 每个API对象都有3大类属性：元数据metadata，规范spec和状态status
# spec和status的区别：spec是期望状态，status是实际状态
```

## 3.3：资源对象状态的查看

```sh
[root@k8s-master1:~/yaml/20240627]# kubectl get deploy -n linux66
NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
linux66-nginx-deployment         1/1     1            1           13h
linux66-tomcat-app1-deployment   1/1     1            1           13h

[root@k8s-master1:~/yaml/20240627]# kubectl describe deploy -n linux66 linux66-nginx-deployment 
Name:                   linux66-nginx-deployment
Namespace:              linux66
CreationTimestamp:      Fri, 28 Jun 2024 23:13:19 -0400
Labels:                 app=linux66-nginx-deployment-label
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=linux66-nginx-selector
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=linux66-nginx-selector
  Containers:
   linux66-nginx-container:
    Image:       nginx
    Ports:       80/TCP, 443/TCP
    Host Ports:  0/TCP, 0/TCP
    Environment:
      password:  123456
      age:       18
    Mounts:      <none>
  Volumes:       <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   linux66-nginx-deployment-7d8d8dcdf4 (1/1 replicas created)
Events:          <none>
```

## 3.4：所有的示例文件

```sh
[root@k8s-master1:~/yaml/20240629]# ll
total 0
drwxr-xr-x 15 root root 292 Jun 29 13:18 ./
drwxr-xr-x  4 root root  38 Jun 29 13:17 ../
drwxr-xr-x  2 root root  32 Jun 29 13:16 case1-pod/
drwxr-xr-x  2 root root  70 Jun 29 13:16 case10-configmap/
drwxr-xr-x  3 root root 181 Jun 29 13:16 case11-secret/
drwxr-xr-x  2 root root  32 Jun 29 13:16 case12-Statefulset/
drwxr-xr-x  2 root root 108 Jun 29 13:16 case13-DaemonSet/
drwxr-xr-x  2 root root  46 Jun 29 13:16 case2-job/
drwxr-xr-x  2 root root  56 Jun 29 13:16 case3-controller/
drwxr-xr-x  2 root root  82 Jun 29 13:16 case4-service/
drwxr-xr-x  2 root root  33 Jun 29 13:16 case5-emptyDir/
drwxr-xr-x  2 root root  33 Jun 29 13:16 case6-hostPath/
drwxr-xr-x  2 root root  51 Jun 29 13:16 case7-nfs/
drwxr-xr-x  2 root root 115 Jun 29 13:16 case8-pv-static/
drwxr-xr-x  2 root root 169 Jun 29 13:16 case9-pv-dynamic-nfs/
```



## 3.5：Pod

### 3.5.1：概念

```sh
# 概述：
1、pod是k8s中的最小单元
2、一个pod中可以运行一个容器，也可以运行多个容器
3、运行多个容器的话，这些容器是一起被调度的（在同一个节点上）
4、Pod的生命周期是短暂的，不会自愈，是用完就销毁的实体（自愈是通过别的控制器来实现的）
5、一般我们是通过Controller来创建和管理pod的生命周期

切记：它的网络是同一个
```

### 3.5.2：示例

```sh
[root@k8s-master1:~/yaml/20240629]# cd case1-pod/
[root@k8s-master1:~/yaml/20240629/case1-pod]# ll
total 4
drwxr-xr-x  2 root root  32 Jun 29 13:16 ./
drwxr-xr-x 15 root root 292 Jun 29 13:18 ../
-rw-r--r--  1 root root 151 Jun 29 13:16 pod-test-case.yaml

[root@k8s-master1:~/yaml/20240629/case1-pod]# cat pod-test-case.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.20.2-alpine
    ports:
    - containerPort: 80

[root@k8s-master1:~/yaml/20240629/case1-pod]# kubectl get pod
NAME        READY   STATUS    RESTARTS       AGE
net-test1   1/1     Running   2 (3h3m ago)   15h
net-test2   1/1     Running   7 (3h3m ago)   2d1h

[root@k8s-master1:~/yaml/20240629/case1-pod]# kubectl apply -f pod-test-case.yaml 
pod/nginx created

[root@k8s-master1:~/yaml/20240629/case1-pod]# kubectl get pod
NAME        READY   STATUS    RESTARTS       AGE
net-test1   1/1     Running   2 (3h3m ago)   15h
net-test2   1/1     Running   7 (3h3m ago)   2d1h
nginx       1/1     Running   0              15s

PS：
pod很少创建，因为状态不可控。
如果它被删除，就直接删除了。只是用于验证，测试，很少上生产。

[root@k8s-master1:~/yaml/20240629/case1-pod]# kubectl delete pod nginx
pod "nginx" deleted
[root@k8s-master1:~/yaml/20240629/case1-pod]# kubectl get pods
NAME        READY   STATUS    RESTARTS       AGE
net-test1   1/1     Running   2 (3h5m ago)   15h
net-test2   1/1     Running   7 (3h5m ago)   2d1h
```

## 3.6：Job 与 Cronjob

### 3.6.1：Job概念

```sh
## job和cronjob

# job是任务，执行一次。用于环境初始化。
# cronjob是周期任务，周期执行

# 一些项目初始化，比如初始化elasticsearch或者数据库。
# 每个项目都要单独初始化数据库。这不能放到同一个镜像里。必须单独处理。

# 解决办法：创建job容器，这个容器启动后就是执行脚本。
# 连接到数据库上，把sql语句执行一下：创建数据库，创建表，导入数据。初始化后，再创建索引，这就是job。
# 这就是job，特点是只执行一次，就退出了，后期再也不会启动。
# 用于做单次执行的场景。用完就退出，执行后就不会再启动。
# 服务先启动，再部署这些job进行初始化。
```

### 3.6.2：Job示例

```sh
[root@k8s-master1:~/yaml/20240629]# cd case2-job/
[root@k8s-master1:~/yaml/20240629/case2-job]# ll
total 8
drwxr-xr-x  2 root root  46 Jun 29 13:16 ./
drwxr-xr-x 15 root root 292 Jun 29 13:18 ../
-rw-r--r--  1 root root 516 Jun 29 13:16 1.job.yaml
-rw-r--r--  1 root root 711 Jun 29 13:16 2.cronjob.yaml

[root@k8s-master1:~/yaml/20240629/case2-job]# cat 1.job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  name: job-mysql-init
  namespace: linux66
spec:
  template:
    spec:
      containers:
      - name: job-mysql-init-container
        image: centos:7.9.2009
        command: ["/bin/sh"]
        args: ["-c", "echo data init job at `date +%Y-%m-%d_%H-%M-%S` >> /cache/data.log"]
        volumeMounts:
        - mountPath: /cache
          name: cache-volume
      volumes:
      - name: cache-volume
        hostPath:
          path: /tmp/jobdata
      restartPolicy: Never

# 该yaml文件说明： 
# 启动容器执行shell命令
# 挂载到宿主机的/tmp/jobdata目录

# 启动该容器
[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl apply -f 1.job.yaml 
job.batch/job-mysql-init created
[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl get pods -n linux66
NAME                                              READY   STATUS      RESTARTS        AGE
job-mysql-init-pgblm                              0/1     Completed   0               8s
linux66-nginx-deployment-7d8d8dcdf4-4shrs         1/1     Running     1 (3h17m ago)   14h
linux66-tomcat-app1-deployment-667c9cf879-nfc2s   1/1     Running     1 (3h17m ago)   13h

# 宿主机查看文件是否创建成功
[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl get pods -n linux66 -o wide
NAME                                              READY   STATUS      RESTARTS        AGE   IP               NODE           NOMINATED NODE   READINESS GATES
job-mysql-init-pgblm                              0/1     Completed   0               65s   10.200.155.146   172.31.8.111   <none>           <none>
linux66-nginx-deployment-7d8d8dcdf4-4shrs         1/1     Running     1 (3h18m ago)   14h   10.200.70.68     172.31.8.113   <none>           <none>
linux66-tomcat-app1-deployment-667c9cf879-nfc2s   1/1     Running     1 (3h18m ago)   13h   10.200.70.67     172.31.8.113   <none>           <none>

# 在node1节点上
[root@k8s-node1:~]# ls /tmp/jobdata
data.log
[root@k8s-node1:~]# cat /tmp/jobdata/data.log 
data init job at 2024-06-29_17-36-05

PS：执行完任务后，该Job就退出了。
```

### 3.6.3：Cronjob概念

```sh
## cronjob: 用于数据同步，归档操作，备份等操作。
```

### 3.6.4：Cronjob示例

```sh
[root@k8s-master1:~/yaml/20240629/case2-job]# cat 2.cronjob.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-mysql-databackup
spec:
  #schedule: "30 2 * * *"
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cronjob-mysql-databackup-pod
            image: centos:7.9.2009
            #imagePullPolicy: IfNotPresent
            command: ["/bin/sh"]
            args: ["-c", "echo mysql databackup cronjob at `date +%Y-%m-%d_%H-%M-%S` >> /cache/data.log"]
            volumeMounts: 
            - mountPath: /cache
              name: cache-volume
          volumes:
          - name: cache-volume
            hostPath:
              path: /tmp/cronjobdata
          restartPolicy: OnFailure

PS：
schedule 容器执行后退出，到时间再运行容器。
现在是1分钟运行一次。

[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl apply -f 2.cronjob.yaml 
cronjob.batch/cronjob-mysql-databackup created

[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl get pod
NAME                                      READY   STATUS      RESTARTS        AGE
cronjob-mysql-databackup-28661387-4xk4d   0/1     Completed   0               9s
net-test1                                 1/1     Running     2 (3h28m ago)   15h
net-test2                                 1/1     Running     7 (3h28m ago)   2d2h

[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl get pod -o wide
NAME                                      READY   STATUS      RESTARTS        AGE    IP               NODE           NOMINATED NODE   READINESS GATES
cronjob-mysql-databackup-28661387-4xk4d   0/1     Completed   0               4s     10.200.155.147   172.31.8.111   <none>           <none>
net-test1                                 1/1     Running     2 (3h28m ago)   15h    10.200.155.144   172.31.8.111   <none>           <none>
net-test2                                 1/1     Running     7 (3h28m ago)   2d2h   10.200.55.172   172.31.8.112   <none>           <none>

# node1上查看备份数据
[root@k8s-node1:~]# ls /tmp/jobdata
data.log

[root@k8s-node1:~]# cat /tmp/cronjobdata/data.log
mysql databackup cronjob at 2024-06-29_17-47-00
mysql databackup cronjob at 2024-06-29_17-48-00
mysql databackup cronjob at 2024-06-29_17-49-00

[root@k8s-node1:~]# cat /tmp/cronjobdata/data.log
mysql databackup cronjob at 2024-06-29_17-47-00
mysql databackup cronjob at 2024-06-29_17-48-00
mysql databackup cronjob at 2024-06-29_17-49-00
mysql databackup cronjob at 2024-06-29_17-50-00
mysql databackup cronjob at 2024-06-29_17-51-00
mysql databackup cronjob at 2024-06-29_17-52-00

# cronjob只保留最近三个cronjob
[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl get pods
NAME                                      READY   STATUS      RESTARTS        AGE
cronjob-mysql-databackup-28661387-4xk4d   0/1     Completed   0               2m54s
cronjob-mysql-databackup-28661388-njh4j   0/1     Completed   0               114s
cronjob-mysql-databackup-28661389-2np8q   0/1     Completed   0               54s
net-test1                                 1/1     Running     2 (3h31m ago)   15h
net-test2                                 1/1     Running     7 (3h31m ago)   2d2h

PS：每次执行完毕后都是重新创建容器，也有可能会调度到别的主机，所以数据一般都会放到共享存储。

[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl delete -f 2.cronjob.yaml 
cronjob.batch "cronjob-mysql-databackup" deleted
[root@k8s-master1:~/yaml/20240629/case2-job]# kubectl get pods
NAME        READY   STATUS    RESTARTS        AGE
net-test1   1/1     Running   2 (3h34m ago)   15h
net-test2   1/1     Running   7 (3h34m ago)   2d2h
```

## 3.7：Replication Controller 与 ReplicaSet

### 3.7.1：RC和RS概念

```sh
## RC/RS 副本控制器
## Deployment 副本控制器
## 把你的容器始终控制在你想要的副本的数量

# 第一代副本控制器是ReplicationController，通常都是严格匹配，或者取反。一般就是严格匹配。1.5/1.6版本才有，现在接触不到了。
确保一个Pod或一组同类的Pod都是可用的。
以上是早期的控制器。
如果Pod所在的主机宕机，pod级别是不能自愈的，但是RC可以。
RC是监控多个节点上的pod，不管pod在哪个主机。

# ReplicaSet控制器，1.9版本开始有。对选择器的支持（selector 还支持 in notin），这是第二代副本控制器。
主要升级了对pod的匹配，也很少用，因为匹配的不是很精确

其实就是控制副本数，RC/RS/Deployment这三个都是。
```

### 3.7.2：RC示例

```sh
[root@k8s-master1:~/yaml/20240629/case2-job]# cd ../case3-controller/
[root@k8s-master1:~/yaml/20240629/case3-controller]# ll
total 12
drwxr-xr-x  2 root root  56 Jun 29 13:16 ./
drwxr-xr-x 15 root root 292 Jun 29 13:18 ../
-rw-r--r--  1 root root 509 Jun 29 13:16 deployment.yml
-rw-r--r--  1 root root 366 Jun 29 13:16 rc.yml
-rw-r--r--  1 root root 438 Jun 29 13:16 rs.yml
 
[root@k8s-master1:~/yaml/20240629/case3-controller]# cat rc.yml 
apiVersion: v1  
kind: ReplicationController  
metadata:  
  name: ng-rc
spec:  
  replicas: 2
  selector:  
    app: ng-rc-80 
    #app1: ng-rc-81
  template:   
    metadata:  
      labels:  
        app: ng-rc-80
        #app1: ng-rc-81
    spec:  
      containers:  
      - name: ng-rc-80 
        image: nginx  
        ports:  
        - containerPort: 80 

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl apply -f rc.yml
replicationcontroller/ng-rc created

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME          READY   STATUS    RESTARTS        AGE
net-test1     1/1     Running   2 (4h37m ago)   16h
net-test2     1/1     Running   7 (4h37m ago)   2d3h
ng-rc-4khkq   1/1     Running   0               2m33s
ng-rc-pb75t   1/1     Running   0               2m33s

# 一旦删除某个pod，就会立即重建pod，始终保持是两个pod
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl delete pod ng-rc-pb75t
pod "ng-rc-pb75t" deleted
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME          READY   STATUS    RESTARTS        AGE
net-test1     1/1     Running   2 (4h38m ago)   16h
net-test2     1/1     Running   7 (4h38m ago)   2d3h
ng-rc-4khkq   1/1     Running   0               3m51s
ng-rc-lbt5m   1/1     Running   0               4s

# 删除rc
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl delete -f rc.yml 
replicationcontroller "ng-rc" deleted
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME        READY   STATUS    RESTARTS        AGE
net-test1   1/1     Running   2 (4h39m ago)   16h
net-test2   1/1     Running   7 (4h39m ago)   2d3h
```

### 3.7.3：RS示例

```sh
[root@k8s-master1:~/yaml/20240629/case3-controller]# cat rs.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1 
kind: ReplicaSet
metadata:
  name: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ng-rs-80
    #matchExpressions:
    #  - {key: app, operator: In, values: [ng-rs-80,ng-rs-81]}          // 这里增加了新的匹配方法
  template:
    metadata:
      labels:
        app: ng-rs-80
    spec:  
      containers:  
      - name: ng-rs-80 
        image: nginx  
        ports:  
        - containerPort: 80

PS：注意增加了新的匹配方法，很少用。

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl apply -f rs.yml 
replicaset.apps/frontend created

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME             READY   STATUS    RESTARTS        AGE
frontend-dzkgj   1/1     Running   0               13s
frontend-jtvdt   1/1     Running   0               13s
net-test1        1/1     Running   2 (4h45m ago)   16h
net-test2        1/1     Running   7 (4h45m ago)   2d3h

# 如果删除一个pod
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl delete pods frontend-jtvdt
pod "frontend-jtvdt" deleted

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME             READY   STATUS    RESTARTS        AGE
frontend-dzkgj   1/1     Running   0               64s
frontend-wb6r2   1/1     Running   0               5s
net-test1        1/1     Running   2 (4h46m ago)   16h
net-test2        1/1     Running   7 (4h46m ago)   2d3h

# 删除
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl delete -f rs.yml 
replicaset.apps "frontend" deleted
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME        READY   STATUS    RESTARTS        AGE
net-test1   1/1     Running   2 (4h48m ago)   16h
net-test2   1/1     Running   7 (4h48m ago)   2d3h

# 可以修改为matchExpressions的写法，但是很少用。
[root@k8s-master1:~/yaml/20240629/case3-controller]# cat rs.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1 
kind: ReplicaSet
metadata:
  name: frontend
spec:
  replicas: 2
  selector:
    #matchLabels:
    #  app: ng-rs-80
    matchExpressions:
      - {key: app, operator: In, values: [ng-rs-80,ng-rs-81]}          
  template:
    metadata:
      labels:
        app: ng-rs-80
    spec:  
      containers:  
      - name: ng-rs-80 
        image: nginx  
        ports:  
        - containerPort: 80

PS：这种也可以使用，但是非常少用。
```

## 3.8：Deployment

### 3.8.1：概念

```sh
# k8s v1.11后就都改成Deployment控制器。向下兼容RC/RS副本控制器。
# Deployment增加很多新的功能，比如滚动升级（部分容器的升级，比如25%）、更新镜像，配置更新，还能实现回滚（记住上一个版本）等。升级时候能暂停，还能实现灰度。
# Deployment是间接管理Pod副本，目的是后期回滚的时候会用。它会建立一个RS控制器来替他干活，由RS来控制pod副本数。

# Deployment是用于升级回滚，暂停，灰度等，都是Deployment控制器来完成的。管理副本还是用ReplicaSet控制器。

# 升级时，Deployment会拉起一个新的ReplicaSet，来新建一个pod，新建后会回收旧的pod。直到副本数满足要求。
# 用户就会往新的pod上转请求。

如果做代码升级，怎么办？
镜像更新到新的版本后，Deployment会拉一个新的ReplicaSet控制器，会根据新镜像来拉起来pod。等新的pod后，会回收旧的pod。它是一部分一部分更新的，直到副本数满足要求。
正常用户请求肯定是有负载均衡器，service后面的所有pod，新启动pod后，之前的旧pod会被删除，这就完成了滚动升级。
如果新的版本有问题呢？会基于之前的旧ReplicaSet来拉起来旧的pod，这是Deployment来控制的。

什么是滚动升级？可以不影响用户的访问。
如果新的版本有问题呢？撤销。
一旦撤销，就基于旧版本的控制器来回滚。就又切到旧版本的pod上了。
这是由Deployment实现的。
```

### 3.8.2：示例

```sh
[root@k8s-master1:~/yaml/20240629/case3-controller]# cat deployment.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    #app: ng-deploy-80 #rc
    matchLabels: #rs or deployment
      app: ng-deploy-80
    
    #matchExpressions:
    #  - {key: app, operator: In, values: [ng-deploy-80,ng-rs-81]}
  template:
    metadata:
      labels:
        app: ng-deploy-80
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx:1.16.0
        ports:
        - containerPort: 80

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl apply -f deployment.yml 
deployment.apps/nginx-deployment created

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME                                READY   STATUS    RESTARTS       AGE
net-test1                           1/1     Running   2 (5h1m ago)   17h
net-test2                           1/1     Running   7 (5h1m ago)   2d3h
nginx-deployment-84cf55b695-25p49   1/1     Running   0              36s
nginx-deployment-84cf55b695-tdzh9   1/1     Running   0              36s

PS：84cf55b695是RS的名字

# 更改deployment.yml中nginx版本为1.20.0
[root@k8s-master1:~/yaml/20240629/case3-controller]# vim deployment.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    #app: ng-deploy-80 #rc
    matchLabels: #rs or deployment
      app: ng-deploy-80
    
    #matchExpressions:
    #  - {key: app, operator: In, values: [ng-deploy-80,ng-rs-81]}
  template:
    metadata:
      labels:
        app: ng-deploy-80
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx:1.20.0
        ports:
        - containerPort: 80

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl apply -f deployment.yml 
deployment.apps/nginx-deployment configured

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME                                READY   STATUS              RESTARTS       AGE
net-test1                           1/1     Running             2 (5h5m ago)   17h
net-test2                           1/1     Running             7 (5h5m ago)   2d3h
nginx-deployment-585c9848bb-zm5gw   0/1     ContainerCreating   0              4s
nginx-deployment-84cf55b695-25p49   1/1     Running             0              4m28s
nginx-deployment-84cf55b695-tdzh9   1/1     Running             0              4m28s

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME                                READY   STATUS              RESTARTS       AGE
net-test1                           1/1     Running             2 (5h6m ago)   17h
net-test2                           1/1     Running             7 (5h6m ago)   2d4h
nginx-deployment-585c9848bb-dt4pm   0/1     ContainerCreating   0              13s
nginx-deployment-585c9848bb-zm5gw   1/1     Running             0              29s
nginx-deployment-84cf55b695-tdzh9   1/1     Running             0              4m53s

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME                                READY   STATUS             RESTARTS       AGE
net-test1                           1/1     Running            2 (5h6m ago)   17h
net-test2                           1/1     Running            7 (5h6m ago)   2d4h
nginx-deployment-585c9848bb-dt4pm   0/1     ImagePullBackOff   0              61s
nginx-deployment-585c9848bb-zm5gw   1/1     Running            0              77s
nginx-deployment-84cf55b695-tdzh9   1/1     Running            0              5m41s

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME                                READY   STATUS    RESTARTS       AGE
net-test1                           1/1     Running   2 (5h7m ago)   17h
net-test2                           1/1     Running   7 (5h7m ago)   2d4h
nginx-deployment-585c9848bb-dt4pm   1/1     Running   0              84s
nginx-deployment-585c9848bb-zm5gw   1/1     Running   0              100s

PS：上面可以看到升级过程，RS的更换。可以看到新的pod是基于新的RS控制器
这就是滚动升级。

# 可以看到升级过程的记录
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sat, 29 Jun 2024 15:19:37 -0400
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=ng-deploy-80
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=ng-deploy-80
  Containers:
   ng-deploy-80:
    Image:        nginx:1.20.0
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-585c9848bb (2/2 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  7m54s  deployment-controller  Scaled up replica set nginx-deployment-84cf55b695 to 2
  Normal  ScalingReplicaSet  3m30s  deployment-controller  Scaled up replica set nginx-deployment-585c9848bb to 1
  Normal  ScalingReplicaSet  3m14s  deployment-controller  Scaled down replica set nginx-deployment-84cf55b695 to 1
  Normal  ScalingReplicaSet  3m14s  deployment-controller  Scaled up replica set nginx-deployment-585c9848bb to 2
  Normal  ScalingReplicaSet  2m8s   deployment-controller  Scaled down replica set nginx-deployment-84cf55b695 to 0
  
  
## 注意Deployment的名字
nginx-deployment-rs名字-随机数名字
rs名字一样，因为都是由它来控制的pod

# 基于原来的镜像回滚就可以了。kubectl rollout undo deployment nginx-deployment  # 回滚
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl rollout undo deployment nginx-deployment
deployment.apps/nginx-deployment rolled back

[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME                                READY   STATUS    RESTARTS        AGE
net-test1                           1/1     Running   2 (5h11m ago)   17h
net-test2                           1/1     Running   7 (5h11m ago)   2d4h
nginx-deployment-84cf55b695-7l8ch   1/1     Running   0               16s
nginx-deployment-84cf55b695-mncfl   1/1     Running   0               18s

PS：滚动升级和回滚用的是Deployment控制器用的最多的功能！！！

# 删除
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl delete -f deployment.yml 
deployment.apps "nginx-deployment" deleted
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get pods
NAME        READY   STATUS    RESTARTS        AGE
net-test1   1/1     Running   2 (5h13m ago)   17h
net-test2   1/1     Running   7 (5h13m ago)   2d4h
```

## 3.9：Service

### 3.9.1：概念

```sh
## Service简介

pod重建之后ip就会变化，因此pod之间使用pod的IP直接访问的话，就会出现无法访问的问题。而service则解耦了服务和应用，Service的实现方式就是通过label标签动态匹配后端endpoint。

代码变更或者，都是基于pod重建的。升级一旦pod重建，地址都是不固定。用户的访问不需要关心容器地址，解耦了宿主机和容器的关联关系。宿主机只要把请求转给service就可以。

原理：
kube-proxy监听着k8s-apiserver，一旦service资源发生变化（调k8s-api修改service信息，比如调整pod副本数），kube-proxy就会生成对应的负载均衡调度的调整，这样就保证service的最新状态。

kube-proxy有三种调度模型：
userspace: k8s v1.1之前，性能太差，需要从内核空间到用户空间的两次拷贝。
iptables: k8s v1.2 -- v1.11之前，在超大规模的k8s情况下，如果节点有 5000 pod 以上，iptables规则就太多，影响性能。
ipvs: k8s v1.11之后，如果没有开启ipvs，则自动降级为iptables，解决大规模环境下iptables规则比较多，性能会下降的问题。

PS：使用ipvs，需要新的内核支持。推荐7.5以上的内核。需要安装相关的ipvs模块。>CentOS7.2都支持。内核不能太低。

### 匹配过程：
第一步：创建ingress，监听在哪个端口，是在宿主机的。

第二步：发送到api server，然后把它关联到ingress里，其实就是创建一个规则，目前ingress用nginx比较多。

第三步：客户端访问这个ingressController，它来匹配请求的目的服务。一旦访问某个域名，和ingress规则匹配成功，这个规则后面是个service，ingressController也有NodePort。

第四步：转给service的端口，比如30044。这个service后面有多个pod，跨主机的pod，甚至是跨网段的pod

第五步：service通过label去匹配pod，所以selector非常重要。可以有多个条件，app=web，project=magedu，可同时满足多个条件。
只有匹配成功，才能找到。在当前namespace去找，找到后，自动关联后端。就像在负载均衡器中配置upstream一样。只不过在nginx或者lvs是自己手动加后端服务器的。service是动态匹配的。如果pod增减，service里也会随机变更。

PS：转发策略默认是轮询。
### 介绍结束

# 判断service后面的pod
[root@k8s-master1:~/yaml/20240629/case3-controller]# kubectl get endpoints -n linux66
NAME                          ENDPOINTS                          AGE
linux66-nginx-service         10.200.70.68:443,10.200.70.68:80   16h
linux66-tomcat-app1-service   10.200.70.67:8080                  16h
```

### 3.9.2：service类型

```sh
## Service类型：
用来解决不同场合下，服务的暴露方式。
有四种。
先讲两种，后两种要结合ingress来讲。
kubectl explain service.spec.type

CluserIP: 默认值。用于k8s内部服务基于service name的访问。创建一个service，通过标签来发现副本，只能在k8s内部。
（微服务场景+zookeeper注册中心，是可以完美解决pod和service调用关系的。比如：pod信息送到注册中心，很多大公司都是这样方式运行）nginx不能发现对方情况下，比如没有使用lua语言情况下？那就用service，这就是ClusterIP来访问。
这时候宿主机是没有监听端口的。

NodePort: 用于kubernetes集群以外的服务主动访问运行在kubernetes集群内部的服务。
一个项目不会有太多的NodePort，只需要暴露前端服务就行了。

ExternalName: 这个几乎用不上。用于将k8s集群外部的服务映射至k8s集群内部访问，从而让集群内部的pod能够通过固定的service name 访问集群外部的服务，有时候也用于将不同namespace之间的pod通过ExternalName进行访问。

LoadBalancer: 用于公有云环境的服务暴露。service和公有云的负载均衡器结合的场景。私有云要自己部署负载均衡器。直接到service了。一旦service的类型是LoadBalancer，就会在公有云环境上自动创建负载均衡器。通过公网地址直接访问svc后面的服务。
1.如果是私有云要自己部署LB，在LB里把请求指向某个NodePort，NodePort再转给某个service。service再转给后面的nginx啊，后者别的业务。
2.如果是LoadBalancer，需要公有云的接口就是API支持。公有云内部也有service，但这个service就是LoadBalancer。它会和外部的公有云的LB进行关联。这个LB有两个地址，一个是公网地址，一个是内部地址。内部地址和k8s保持通讯，
```



### 3.9.3：示例

```sh
[root@k8s-master1:~/yaml/20240629/case4-service]# ll
total 12
drwxr-xr-x  2 root root  82 Jun 29 13:16 ./
drwxr-xr-x 15 root root 292 Jun 29 13:18 ../
-rw-r--r--  1 root root 521 Jun 29 13:16 1-deploy_node.yml
-rw-r--r--  1 root root 193 Jun 29 13:16 2-svc_service.yml
-rw-r--r--  1 root root 212 Jun 29 13:16 3-svc_NodePort.yml

# 第一步：创建deployment
[root@k8s-master1:~/yaml/20240629/case4-service]# cat 1-deploy_node.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    #matchLabels: #rs or deployment
    #  app: ng-deploy3-80
    matchExpressions:
      - {key: app, operator: In, values: [ng-deploy-80,ng-rs-81]}
  template:
    metadata:
      labels:
        app: ng-deploy-80          #这个要和service里的标签匹配，这样service才能找到这个pod。
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx:1.17.5 
        ports:
        - containerPort: 80
      #nodeSelector:
      #  env: group1

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl apply -f 1-deploy_node.yml 
deployment.apps/nginx-deployment created

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get pods
NAME                               READY   STATUS    RESTARTS        AGE
net-test1                          1/1     Running   2 (5h44m ago)   17h
net-test2                          1/1     Running   7 (5h44m ago)   2d4h
nginx-deployment-79bcbf4f5-5j5b8   1/1     Running   0               17s

# 第二步：创建service
[root@k8s-master1:~/yaml/20240629/case4-service]# cat 2-svc_service.yml 
apiVersion: v1
kind: Service
metadata:
  name: ng-deploy-80 
spec:
  ports:
  - name: http           #名字自己写
    port: 80             #这是service的port
    targetPort: 80       #容器中的服务端口，一定要写容器的端口 
    protocol: TCP        #协议
  type: ClusterIP
  selector:
    app: ng-deploy-80    #service后面挂了哪些pod，是通过这个标签匹配的。要和上面的deployment中的某个标签匹配。

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl apply -f 2-svc_service.yml 
service/ng-deploy-80 created

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get svc
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes     ClusterIP   10.100.0.1      <none>        443/TCP   2d4h
ng-deploy-80   ClusterIP   10.100.233.11   <none>        80/TCP    9s

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get ep
NAME           ENDPOINTS                                               AGE
kubernetes     172.31.8.101:6443,172.31.8.102:6443,172.31.8.103:6443   2d4h
ng-deploy-80   10.200.155.160:80                                       6m23s

PS：这个10.200.155.160是谁呢？

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS        AGE     IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                          1/1     Running   2 (5h54m ago)   18h     10.200.155.144   172.31.8.111   <none>           <none>
net-test2                          1/1     Running   7 (5h54m ago)   2d4h    10.200.55.172    172.31.8.112   <none>           <none>
nginx-deployment-79bcbf4f5-5j5b8   1/1     Running   0               9m33s   10.200.155.160   172.31.8.111   <none>           <none>


# 目前只能从k8s里的容器进行访问，通过这个service来访问后端的nginx容器。
# 我们从一个net-test1容器进行测试
[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl exec -it net-test1 -- bash
[root@net-test1 /]# curl ng-deploy-80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@net-test1 /]# exit
exit
[root@k8s-master1:~/yaml/20240629/case4-service]# 

PS：这种情况只能从k8s内部进行访问，如果想从k8s外部访问是不行的。
因为没有任何一个端口能够让从k8s以外的主机进行访问。

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl delete -f 2-svc_service.yml 
service "ng-deploy-80" deleted

# 编辑文件，改为NodePort类型，这里使用新文件，改过的。
[root@k8s-master1:~/yaml/20240629/case4-service]# cat 3-svc_NodePort.yml 
apiVersion: v1
kind: Service
metadata:
  name: ng-deploy-80 
spec:
  ports:
  - name: http
    port: 81
    targetPort: 80
    nodePort: 30012
    protocol: TCP
  type: NodePort
  selector:
    app: ng-deploy-80

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl apply -f 3-svc_NodePort.yml 
service/ng-deploy-80 created

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get svc
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes     ClusterIP   10.100.0.1      <none>        443/TCP        2d5h
ng-deploy-80   NodePort    10.100.184.81   <none>        81:30012/TCP   11s

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get ep
NAME           ENDPOINTS                                               AGE
kubernetes     172.31.8.101:6443,172.31.8.102:6443,172.31.8.103:6443   2d5h
ng-deploy-80   10.200.155.160:80                                       20s

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS       AGE    IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                          1/1     Running   2 (6h4m ago)   18h    10.200.155.144   172.31.8.111   <none>           <none>
net-test2                          1/1     Running   7 (6h4m ago)   2d4h   10.200.55.172    172.31.8.112   <none>           <none>
nginx-deployment-79bcbf4f5-5j5b8   1/1     Running   0              19m    10.200.155.160   172.31.8.111   <none>           <none>

PS：
此时访问http://172.31.8.111:30012
可以看到nginx页面

用户访问流程：
用户请求先发送给负载均衡器（haproxy或者lvs），LB转给NodePort，它其实就是service的规则，宿主机的30012，每个主机都会监听相同的NodePort，但是它们的规则是不一样的。service是一个逻辑的规则（ipvs或者iptables规则），它不是一个实体。容器是一个实体。这里service端口是80，是逻辑规则，并不真实存在。根据规则再匹配pod。这样用户就请求到里面的服务。


# 改haproxy里的配置
[root@k8s-ha1:~]# vim /etc/haproxy/haproxy.cfg
......
listen sunnycreek-linux66-nginx-80
  bind 172.31.8.189:80
  mode tcp
  server node1 172.31.8.111:30012 check inter 3s fall 3 rise 1
  server node2 172.31.8.112:30012 check inter 3s fall 3 rise 1
  server node3 172.31.8.113:30012 check inter 3s fall 3 rise 1
......

[root@k8s-ha1:~]# systemctl reload haproxy

PS：
此时访问http://172.31.8.189
可以看到nginx页面

经过几层转发，就会带来性能损耗。

[root@k8s-node3:~]# ipvsadm -Ln | grep 30012
TCP  172.31.8.113:30012 rr
TCP  10.200.70.64:30012 rr

[root@k8s-node3:~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  172.31.8.113:30004 rr
  -> 10.200.55.174:8443           Masq    1      0          0         
TCP  172.31.8.113:30006 rr
  -> 10.200.70.68:80              Masq    1      0          0         
TCP  172.31.8.113:30012 rr
  -> 10.200.155.160:80            Masq    1      20         0   
......

PS：注意10.200.155.160
------------------------------------------------------------------------------------------------------------
## 这里还可能出现一种情况：
# 
[root@k8s-ha1:~]# vim /etc/haproxy/haproxy.cfg
......
listen sunnycreek-linux66-nginx-80
  bind 172.31.8.189:80
  mode tcp
  server node1 172.31.8.112:30012 check inter 3s fall 3 rise 1
  server node2 172.31.8.113:30012 check inter 3s fall 3 rise 1  
  
[root@k8s-ha1:~]# systemctl reload haproxy

[root@k8s-master1:~/yaml/20240629/case4-service]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS        AGE    IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                          1/1     Running   2 (6h33m ago)   18h    10.200.155.144   172.31.8.111   <none>           <none>
net-test2                          1/1     Running   7 (6h33m ago)   2d5h   10.200.55.172    172.31.8.112   <none>           <none>
nginx-deployment-79bcbf4f5-5j5b8   1/1     Running   0               48m    10.200.155.160   172.31.8.111   <none>           <none>

此时访问http://172.31.8.189
可以看到nginx页面

就是haproxy转发的后端node2和3上并没有这个deployment，现在是node1节点上有deployment，那就要看转发后端的路由，route -n，就可能通过路由来通过宿主机之间的tunnel0来直接转发到存在deployment的路由了。走的是物理网络，是calico的隧道。
这个service就为了打通请求在不同主机之间的转发。

# 在node3节点上查看ipvs规则
[root@k8s-node3:~]# ipvsadm -Ln | grep 30012
TCP  172.31.8.113:30012 rr
TCP  10.200.70.64:30012 rr
[root@k8s-node3:~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  172.31.8.113:30004 rr
  -> 10.200.55.174:8443           Masq    1      0          0         
TCP  172.31.8.113:30006 rr
  -> 10.200.70.68:80              Masq    1      0          0         
TCP  172.31.8.113:30012 rr
  -> 10.200.155.160:80
......

[root@k8s-node3:~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.31.8.2      0.0.0.0         UG    0      0        0 eth0
10.200.55.128   172.31.8.112    255.255.255.192 UG    0      0        0 tunl0
10.200.70.64    0.0.0.0         255.255.255.192 U     0      0        0 *
10.200.70.67    0.0.0.0         255.255.255.255 UH    0      0        0 cali52939b36f0d
10.200.70.68    0.0.0.0         255.255.255.255 UH    0      0        0 cali7ab90cc1a63
10.200.92.64    172.31.8.103    255.255.255.192 UG    0      0        0 tunl0
10.200.155.128  172.31.8.111    255.255.255.192 UG    0      0        0 tunl0
10.200.175.192  172.31.8.101    255.255.255.192 UG    0      0        0 tunl0
10.200.241.192  172.31.8.102    255.255.255.192 UG    0      0        0 tunl0
172.31.8.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0

PS：查看路由发现规则去往10.200.155.160这个地址是符合10.200.155.128/26这个网段，所以转发给tunl0


# 在node2节点上查看ipvs规则
[root@k8s-node2:~]# ipvsadm -Ln | grep 30012
TCP  172.31.8.112:30012 rr
TCP  10.200.55.128:30012 rr
[root@k8s-node2:~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  172.31.8.112:30004 rr
  -> 10.200.55.174:8443           Masq    1      0          0         
TCP  172.31.8.112:30006 rr
  -> 10.200.70.68:80              Masq    1      0          0         
TCP  172.31.8.112:30012 rr
  -> 10.200.155.160:80            Masq    1      20         0   
......

[root@k8s-node2:~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.31.8.2      0.0.0.0         UG    0      0        0 eth0
10.200.55.128   0.0.0.0         255.255.255.192 U     0      0        0 *
10.200.55.170   0.0.0.0         255.255.255.255 UH    0      0        0 cali857f6526974
10.200.55.171   0.0.0.0         255.255.255.255 UH    0      0        0 cali27bdaf49fec
10.200.55.172   0.0.0.0         255.255.255.255 UH    0      0        0 cali2b2e7c9e43e
10.200.55.173   0.0.0.0         255.255.255.255 UH    0      0        0 calid211549cef9
10.200.55.174   0.0.0.0         255.255.255.255 UH    0      0        0 calieb086996b71
10.200.55.175   0.0.0.0         255.255.255.255 UH    0      0        0 cali62b0fd1a73f
10.200.70.64    172.31.8.113    255.255.255.192 UG    0      0        0 tunl0
10.200.92.64    172.31.8.103    255.255.255.192 UG    0      0        0 tunl0
10.200.155.128  172.31.8.111    255.255.255.192 UG    0      0        0 tunl0
10.200.175.192  172.31.8.101    255.255.255.192 UG    0      0        0 tunl0
10.200.241.192  172.31.8.102    255.255.255.192 UG    0      0        0 tunl0
172.31.8.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0

PS：查看路由发现规则去往10.200.155.160这个地址是符合10.200.155.128/26这个网段，所以转发给tunl0

# 查看node1节点的路由
[root@k8s-node1:~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.31.8.2      0.0.0.0         UG    0      0        0 eth0
10.200.55.128   172.31.8.112    255.255.255.192 UG    0      0        0 tunl0
10.200.70.64    172.31.8.113    255.255.255.192 UG    0      0        0 tunl0
10.200.92.64    172.31.8.103    255.255.255.192 UG    0      0        0 tunl0
10.200.155.128  0.0.0.0         255.255.255.192 U     0      0        0 *
10.200.155.144  0.0.0.0         255.255.255.255 UH    0      0        0 cali073b411309a
10.200.155.160  0.0.0.0         255.255.255.255 UH    0      0        0 calida4ab2bc3ef
10.200.175.192  172.31.8.101    255.255.255.192 UG    0      0        0 tunl0
10.200.241.192  172.31.8.102    255.255.255.192 UG    0      0        0 tunl0
172.31.8.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0

PS：这样就将转到10.200.155.160的通讯转到calida4ab2bc3ef网卡上了。

# 查看node1节点的规则
[root@k8s-node1:~]# ipvsadm -Ln | grep 30012
TCP  172.31.8.111:30012 rr
TCP  10.200.155.128:30012 rr
[root@k8s-node1:~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  172.31.8.111:30004 rr
  -> 10.200.55.174:8443           Masq    1      0          0         
TCP  172.31.8.111:30006 rr
  -> 10.200.70.68:80              Masq    1      0          0         
TCP  172.31.8.111:30012 rr
  -> 10.200.155.160:80            Masq    1      0          0  

这样就完成了跨主机间的通讯。这就是calico网络完成的功能。
```

![](assets/images/service_deployment_01.png)

![](assets/images/service_deployment_02.png)

## 3.10：Volume

### 3.10.1：概念

```sh
## Volume-存储卷简介

# Volume将容器中的指定数据（指定的目录，用户上传，或者MySQL，或者Kafka中的数据）和容器解耦，并将数据存储到指定的位置，通常是网络存储。不同的存储卷功能不一样。如果是基于网络存储的存储卷可以实现容器间的数据共享和持久化。

# 静态存储卷需要在使用前手动创建PV和PVC，然后绑定至pod使用。
# 动态存储卷。

# 常用的几种volume：都需要挂载使用，都是volume的范畴。
# 一个容器可以挂载N多存储。

emptyDir: 本地临时卷。容器在哪里，emptyDir就在容器所在的宿主机。它是临时的，意味着数据不会被持久化。
使用场景：同一个pod里，多个容器之间数据的共享。
比如：a和b容器实现数据共享，日志共享等，宿主机文件夹挂在a容器里，b容器也挂载这个宿主机文件夹。就这样实现共享。a和b容器的文件系统是隔离的。容器删除后，数据同时删除。一个pod里多个容器，实现容器之间数据共享。a可能是centos，b是alpine。

hostPath: 本地存储卷。容器删除后，数据会保留，数据可以持久化。
这是和emptyDir的不同，但是用的不多。因为从A主机迁移到B主机后，之前的数据在A主机，这样就无法实现数据和容器的绑定关系。
使用场景：有些特定的主机就运行特定的服务。
比如：亲和。容器指定到哪些主机上运行，这样容器不会随便跑。容器只会运行在某个主机上。

configmap: 配置文件。既不放在镜像，也不放在配置中心，而是放在configmap这种资源对象里。容器启动的时候加载这个configmap。
比如：nginx，mysql等，写好这些配置后，让pod挂载这个配置。这样，将应用和配置解耦。
A服务需要这个配置，就创建A服务的configmap。同理，B服务以此类推。
这就方便配置的更新和后期的维护，只改配置，不用改镜像。需要重启服务。

Secret: 是一种包含少量敏感信息的数据。例如密码、令牌或密钥的对象。
以base64加密形式存储到k8s中，其实没什么卵用，只是一定程度上是安全的。

nfs: 网络存储卷。这是真正意义上的业务存储。

这些都需要挂载使用，都称为volume。


hostPath: 类似于docker run -v /data:/data 。数据不会丢失。但是只是在这个主机上。
数据不能随着容器迁移，用的也不多。
它的使用和emptyDir没啥太大区别。
```

### 3.10.2：emptyDir

```sh
## emptyDir，实际上就是宿主机的一个目录
当Pod被分配给节点时，首先创建emptyDir卷，并且只要该Pod在该结点上运行，该卷就会存在。
正如卷的名字所述，它最初是空的，就是个空的目录。Pod中的容器可以读取和写入emptyDir卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除Pod时，emptyDir中的数据将被永久删除。

emptyDir: 这里只能放缓存，日志等。不要放数据库的文件。

[root@k8s-master1:~/yaml/20240629]# cd case5-emptyDir/
[root@k8s-master1:~/yaml/20240629/case5-emptyDir]# ll
total 4
drwxr-xr-x  2 root root  33 Jun 29 13:16 ./
drwxr-xr-x 15 root root 292 Jun 29 13:18 ../
-rw-r--r--  1 root root 523 Jun 29 13:16 deploy_emptyDir.yml

[root@k8s-master1:~/yaml/20240629/case5-emptyDir]# cat deploy_emptyDir.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels: #rs or deployment
      app: ng-deploy-80
  template:
    metadata:
      labels:
        app: ng-deploy-80
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx 
        ports:
        - containerPort: 80
        volumeMounts:             #容器中挂载
        - mountPath: /cache       #容器中的路径
          name: cache-volume      #通过name来引用volume
      volumes:
      - name: cache-volume        #声明，指定name
        emptyDir: {}

[root@k8s-master1:~/yaml/20240629/case5-emptyDir]# kubectl apply -f deploy_emptyDir.yml 
deployment.apps/nginx-deployment created

[root@k8s-master1:~/yaml/20240629/case5-emptyDir]# kubectl get pods
NAME                              READY   STATUS    RESTARTS      AGE
net-test1                         1/1     Running   4 (26m ago)   23h
net-test2                         1/1     Running   9 (26m ago)   2d10h
nginx-deployment-cf796fbd-78ksf   1/1     Running   0             4s

[root@k8s-master1:~/yaml/20240629/case5-emptyDir]# kubectl get pods -o wide
NAME                              READY   STATUS    RESTARTS      AGE     IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                         1/1     Running   4 (30m ago)   23h     10.200.155.163   172.31.8.111   <none>           <none>
net-test2                         1/1     Running   9 (30m ago)   2d10h   10.200.55.185    172.31.8.112   <none>           <none>
nginx-deployment-cf796fbd-78ksf   1/1     Running   0             3m51s   10.200.155.165   172.31.8.111   <none>           <none>

# 可以在node1节点上查看
[root@k8s-node1:~]# find /var/lib/ -name cache-volume
/var/lib/kubelet/pods/4f74cd3c-7e9d-45d2-ac8a-d5091aa9aa63/volumes/kubernetes.io~empty-dir/cache-volume
/var/lib/kubelet/pods/4f74cd3c-7e9d-45d2-ac8a-d5091aa9aa63/plugins/kubernetes.io~empty-dir/cache-volume

[root@k8s-node1:~]# ll /var/lib/kubelet/pods/4f74cd3c-7e9d-45d2-ac8a-d5091aa9aa63/volumes/kubernetes.io~empty-dir/cache-volume/
total 0
drwxrwxrwx 2 root root 21 Jun 29 22:15 ./
drwxr-xr-x 3 root root 26 Jun 29 22:05 ../

PS：注意，这里查到的是两个不同的目录路径。

# 进入到容器中产生一个数据
[root@k8s-master1:~/yaml/20240629/case5-emptyDir]# kubectl exec -it nginx-deployment-cf796fbd-78ksf -- bash
root@nginx-deployment-cf796fbd-78ksf:/# cd /cache/
root@nginx-deployment-cf796fbd-78ksf:/cache# df -TH
Filesystem     Type     Size  Used Avail Use% Mounted on
overlay        overlay  108G   11G   98G  10% /
tmpfs          tmpfs     68M     0   68M   0% /dev
tmpfs          tmpfs    2.1G     0  2.1G   0% /sys/fs/cgroup
/dev/sda3      xfs      108G   11G   98G  10% /cache
shm            tmpfs     68M     0   68M   0% /dev/shm
tmpfs          tmpfs    3.8G   13k  3.8G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs          tmpfs    2.1G     0  2.1G   0% /proc/acpi
tmpfs          tmpfs    2.1G     0  2.1G   0% /proc/scsi
tmpfs          tmpfs    2.1G     0  2.1G   0% /sys/firmware
root@nginx-deployment-cf796fbd-78ksf:/cache# ls
root@nginx-deployment-cf796fbd-78ksf:/cache# touch 123.txt
root@nginx-deployment-cf796fbd-78ksf:/cache# exit
exit

PS：由于不是网络存储，所以它不是那种能产生挂载点的效果。nfs挂载就能用df -TH看到。看不到哪个目录挂到/cache里

[root@k8s-node1:~]# find / -name 123.txt
/var/lib/kubelet/pods/4f74cd3c-7e9d-45d2-ac8a-d5091aa9aa63/volumes/kubernetes.io~empty-dir/cache-volume/123.txt
[root@k8s-node1:~]# ll /var/lib/kubelet/pods/4f74cd3c-7e9d-45d2-ac8a-d5091aa9aa63/volumes/kubernetes.io~empty-dir/cache-volume/
total 0
drwxrwxrwx 2 root root 21 Jun 29 22:15 ./
drwxr-xr-x 3 root root 26 Jun 29 22:05 ../
-rw-r--r-- 1 root root  0 Jun 29 22:15 123.txt

# 删除
[root@k8s-master1:~/yaml/20240629/case5-emptyDir]# kubectl delete -f deploy_emptyDir.yml
deployment.apps "nginx-deployment" deleted
# 查看node1
[root@k8s-node1:~]# ll /var/lib/kubelet/pods/4f74cd3c-7e9d-45d2-ac8a-d5091aa9aa63/volumes/kubernetes.io~empty-dir/cache-volume/
ls: cannot access '/var/lib/kubelet/pods/4f74cd3c-7e9d-45d2-ac8a-d5091aa9aa63/volumes/kubernetes.io~empty-dir/cache-volume/': No such file or directory

PS：
此时，容器删除后，宿主机目录就被回收。这种emptyDir不要放数据库里的数据。要注意。放日志可以。
```

### 3.10.3：hostPath

```sh
## hostPath
hostPath卷将主机节点的文件系统中的文件或目录挂载到集群中，pod删除的时候，卷不会被删除
类似于docker run -v /data:/data，容器被删除时，宿主机的数据还在。
使用上和emptyDir没啥太大区别。

场景：容器不会迁移，就会在某个固定宿主机。就可以使用hostPath。

[root@k8s-master1:~/yaml/20240629/case6-hostPath]# ll
total 4
drwxr-xr-x  2 root root  33 Jun 29 13:16 ./
drwxr-xr-x 15 root root 292 Jun 29 13:18 ../
-rw-r--r--  1 root root 531 Jun 29 13:16 deploy_hostPath.yml

[root@k8s-master1:~/yaml/20240629/case6-hostPath]# cat deploy_hostPath.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-deploy-80
  template:
    metadata:
      labels:
        app: ng-deploy-80
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx 
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /cache
          name: cache-volume
      volumes:
      - name: cache-volume
        hostPath:
          path: /tmp/linux66

[root@k8s-master1:~/yaml/20240629/case6-hostPath]# kubectl apply -f deploy_hostPath.yml 
deployment.apps/nginx-deployment created

[root@k8s-master1:~/yaml/20240629/case6-hostPath]# kubectl get pods
NAME                                READY   STATUS    RESTARTS      AGE
net-test1                           1/1     Running   4 (47m ago)   24h
net-test2                           1/1     Running   9 (47m ago)   2d11h
nginx-deployment-7496494767-h7f56   1/1     Running   0             5s

[root@k8s-master1:~/yaml/20240629/case6-hostPath]# kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS      AGE     IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                           1/1     Running   4 (47m ago)   24h     10.200.155.163   172.31.8.111   <none>           <none>
net-test2                           1/1     Running   9 (47m ago)   2d11h   10.200.55.185    172.31.8.112   <none>           <none>
nginx-deployment-7496494767-h7f56   1/1     Running   0             10s     10.200.155.166   172.31.8.111   <none>           <none>

# 在node1创建了目录linux66
[root@k8s-node1:~]# ll /tmp/
total 4
drwxrwxrwt 15 root root 4096 Jun 29 22:27 ./
drwxr-xr-x 19 root root  291 Jan  3 14:55 ../
drwxrwxrwt  2 root root    6 Jun 29 21:38 .ICE-unix/
drwxrwxrwt  2 root root    6 Jun 29 21:38 .Test-unix/
drwxrwxrwt  2 root root    6 Jun 29 21:38 .X11-unix/
drwxrwxrwt  2 root root    6 Jun 29 21:38 .XIM-unix/
drwxrwxrwt  2 root root    6 Jun 29 21:38 .font-unix/
drwxr-xr-x  2 root root    6 Jun 29 22:26 linux66/
......

# 容器内创建测试数据
[root@k8s-master1:~/yaml/20240629/case6-hostPath]# kubectl exec -it nginx-deployment-7496494767-h7f56 -- bash
root@nginx-deployment-7496494767-h7f56:/# cd /cache/
root@nginx-deployment-7496494767-h7f56:/cache# echo data1 > log.txt
root@nginx-deployment-7496494767-h7f56:/cache# exit
exit

# node1查看数据
[root@k8s-node1:~]# cat /tmp/linux66/log.txt
data1

# 删除容器
[root@k8s-master1:~/yaml/20240629/case6-hostPath]# kubectl delete -f deploy_hostPath.yml 
deployment.apps "nginx-deployment" deleted

# node1查看数据
[root@k8s-node1:~]# cat /tmp/linux66/log.txt
data1

PS：数据还在。
它的使用和emtpyDir没啥区别。就是数据不被删除。
再使用相同的数据把服务跑起来，这样数据就恢复了。
无法在相同的主机上保持数据，用的不多。
```

### 3.10.4：nfs

#### 3.10.4.1：概念

```sh
## nfs等共享存储
nfs卷允许将现有的NFS（网络文件系统）共享挂载到您的容器中。它不像emptyDir，当删除Pod时，nfs卷的内容被保留，卷仅仅是被卸载。这意味着NFS卷可以预填充数据，并且可以在pod之间“切换”数据，NFS可以被多个写入者同时挂载。

我们的数据持久化是存储服务器在后台实现的。它可能会把数据放到不同的磁盘上，比如说RAID10，通过RAID卡来实现数据的冗余。
商业的NAS或者ceph，数据安全性会更高。数据是三个副本，坏一个磁盘或者坏一个服务器，根本不会受到影响。

[root@k8s-master1:~/yaml/20240629/case7-nfs]# ll
total 8
drwxr-xr-x  2 root root   51 Jun 29 13:16 ./
drwxr-xr-x 15 root root  292 Jun 29 13:18 ../
-rw-r--r--  1 root root  806 Jun 29 13:16 deploy_nfs.yml
-rw-r--r--  1 root root 1009 Jun 29 13:16 deploy_nfs2.yml

deploy_nfs.yml 创建多个pod测试挂载同一个NFS
deploy_nfs2.yml 创建多个pod测试每个pod挂载多个NFS
```

#### 3.10.4.2：安装nfs服务器

> 安装在ha1上

```sh
# 安装nfs-server
[root@k8s-ha1:~]# apt install nfs-server -y

# 创建共享目录
[root@k8s-ha1:~]# mkdir /data/k8sdata/linux66 -p

# 编辑nfs配置文件
[root@k8s-ha1:~]# vim /etc/exports
/data/k8sdata *(rw,no_root_squash)

PS：
rw 读写
no_root_squash 不做权限映射：普通用户的文件的属主和属组，不改成root用户
特别要注意：授权的网段和权限之间没有空格，否则nfs服务就不能启动
特别要注意：有些存储的授权是IP地址，不会是*。这个*代表的网段，一定要把宿主机的地址加上，如果不加上，是挂载不了的。比如这里是10.200.0.0/16，容器的地址在出宿主机的时候，是被替换成宿主机的地址出去。所以这个授权就没有包括宿主机，没有包括宿主机就挂不上。

[root@k8s-ha1:~]# systemctl restart nfs-server.service 
[root@k8s-ha1:~]# systemctl enable nfs-server.service

# 检查能否看到共享
[root@k8s-master1:~/yaml/20240629/case7-nfs]# showmount -e 172.31.8.109
Export list for 172.31.8.109:
/data/k8sdata *

[root@k8s-node1:~]# showmount -e 172.31.8.109
Export list for 172.31.8.109:
/data/k8sdata *

PS：可能需要apt install nfs-common，否则可能挂不上，不识别nfs。就是装个驱动的意思。或者能挂上，但是速度很慢。生产中可能会导致问题。

# 现在挂的是共享目录里头的linux66，能挂上吗？
[root@k8s-master1:~/yaml/20240629/case7-nfs]# mount -t nfs 172.31.8.109:/data/k8sdata /mnt/
[root@k8s-master1:~/yaml/20240629/case7-nfs]# df -TH
Filesystem                 Type      Size  Used Avail Use% Mounted on
udev                       devtmpfs  2.0G     0  2.0G   0% /dev
tmpfs                      tmpfs     408M  1.9M  406M   1% /run
/dev/sda3                  xfs       108G   17G   92G  16% /
tmpfs                      tmpfs     2.1G     0  2.1G   0% /dev/shm
tmpfs                      tmpfs     5.3M     0  5.3M   0% /run/lock
tmpfs                      tmpfs     2.1G     0  2.1G   0% /sys/fs/cgroup
/dev/loop1                 squashfs   68M   68M     0 100% /snap/core20/2318
/dev/loop2                 squashfs   97M   97M     0 100% /snap/lxd/24061
/dev/loop0                 squashfs   66M   66M     0 100% /snap/core20/1611
/dev/loop3                 squashfs   72M   72M     0 100% /snap/lxd/22753
/dev/loop4                 squashfs   41M   41M     0 100% /snap/snapd/21759
/dev/sda5                  xfs        54G  450M   54G   1% /data
/dev/sda2                  xfs       1.1G  263M  802M  25% /boot
tmpfs                      tmpfs     3.8G   13k  3.8G   1% /var/lib/kubelet/pods/74c3e6f9-7b4b-4f02-963f-6abff7e9db64/volumes/kubernetes.io~secret/etcd-certs
tmpfs                      tmpfs     3.8G   13k  3.8G   1% /var/lib/kubelet/pods/74c3e6f9-7b4b-4f02-963f-6abff7e9db64/volumes/kubernetes.io~projected/kube-api-access-kwfl7
shm                        tmpfs      68M     0   68M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/3b0c094d525226145ec430b7f2c37bc05fea50fc47743fdbf8394c2be0d3f1f2/shm
overlay                    overlay   108G   17G   92G  16% /run/containerd/io.containerd.runtime.v2.task/k8s.io/3b0c094d525226145ec430b7f2c37bc05fea50fc47743fdbf8394c2be0d3f1f2/rootfs
overlay                    overlay   108G   17G   92G  16% /run/containerd/io.containerd.runtime.v2.task/k8s.io/73fb710a1aa686b2042e258a0cb7b5dbcbaebd018469ac53849d7c16fff7adf2/rootfs
tmpfs                      tmpfs     408M     0  408M   0% /run/user/0
172.31.8.109:/data/k8sdata nfs4       54G  408M   54G   1% /mnt

[root@k8s-master1:~/yaml/20240629/case7-nfs]# umount /mnt/

PS：直接挂载/data/k8sdata，肯定可以

[root@k8s-master1:~/yaml/20240629/case7-nfs]# mount -t nfs 172.31.8.109:/data/k8sdata/linux66 /mnt/
[root@k8s-master1:~/yaml/20240629/case7-nfs]# umount /mnt/

PS：挂载共享目录的子目录也是可以的。
生产环境下，一个项目有一个子目录，这就简化了共享的配置。

------------------------------------------------------------------------------------------------------------
## 增加一个测试
[root@k8s-ha1:~]# vim /etc/exports
/data/k8sdata 10.200.0.0/16(rw,no_root_squash)

[root@k8s-ha1:~]# systemctl restart nfs-server.service

PS：这个授权没有包括宿主机，挂不上的。

# 发现主机也挂不上。
[root@k8s-master1:~/yaml/20240629/case7-nfs]# mount -t nfs 172.31.8.109:/data/k8sdata/linux66 /mnt/
mount.nfs: access denied by server while mounting 172.31.8.109:/data/k8sdata/linux66

# 容器中的nfs地址是172.31.8.109
[root@k8s-master1:~/yaml/20240629/case7-nfs]# vim deploy_nfs.yml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-deploy-80
  template:
    metadata:
      labels:
        app: ng-deploy-80
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx 
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /usr/share/nginx/html/mysite
          name: my-nfs-volume
      volumes:
      - name: my-nfs-volume
        nfs:
          server: 172.31.8.109
          path: /data/k8sdata/linux66

---
apiVersion: v1
kind: Service
metadata:
  name: ng-deploy-80
spec:
  ports:
  - name: http
    port: 81
    targetPort: 80
    nodePort: 30016
    protocol: TCP
  type: NodePort
  selector:
    app: ng-deploy-80

[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl apply -f deploy_nfs.yml
deployment.apps/nginx-deployment created
service/ng-deploy-80 created

[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl get pods -o wide
NAME                                READY   STATUS              RESTARTS      AGE     IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                           1/1     Running             4 (98m ago)   25h     10.200.155.163   172.31.8.111   <none>           <none>
net-test2                           1/1     Running             9 (98m ago)   2d11h   10.200.55.185    172.31.8.112   <none>           <none>
nginx-deployment-6bdfb774d8-2dlgx   0/1     ContainerCreating   0             8s      <none>           172.31.8.111   <none>           <none>

PS：出现问题，容器启动不了。

# 查找原因
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl describe pod nginx-deployment-6bdfb774d8-2dlgx
Name:           nginx-deployment-6bdfb774d8-2dlgx
Namespace:      default
Priority:       0
Node:           172.31.8.111/172.31.8.111
Start Time:     Sat, 29 Jun 2024 23:17:09 -0400
Labels:         app=ng-deploy-80
                pod-template-hash=6bdfb774d8
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6bdfb774d8
Containers:
  ng-deploy-80:
    Container ID:   
    Image:          nginx
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /usr/share/nginx/html/mysite from my-nfs-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2k7c8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  my-nfs-volume:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    172.31.8.109
    Path:      /data/k8sdata/linux66
    ReadOnly:  false
  kube-api-access-2k7c8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age               From               Message
  ----     ------       ----              ----               -------
  Normal   Scheduled    68s               default-scheduler  Successfully assigned default/nginx-deployment-6bdfb774d8-2dlgx to 172.31.8.111
  Warning  FailedMount  4s (x8 over 68s)  kubelet            MountVolume.SetUp failed for volume "my-nfs-volume" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs 172.31.8.109:/data/k8sdata/linux66 /var/lib/kubelet/pods/13628b85-6a84-4ddc-abe8-8111e14e32d5/volumes/kubernetes.io~nfs/my-nfs-volume
Output: mount.nfs: access denied by server while mounting 172.31.8.109:/data/k8sdata/linux66

PS：我们用的是overlay网络，跨网段，在出k8s环境时，nfs不在k8s以内。哪怕是同一个子网，但是是在k8s以外的服务器，在出去的时候，容器会检查它的路由，会去网关，就会使用宿主机的eth0出去，做了源地址替换，目的是为了数据能回来。看下面地址，这个地址是172.31.8.111，这个地址不在授权范围内，所以就挂载不上。
这个地址和pod地址没有关系，是不会把k8s地址透传出去的。

[root@k8s-node1:~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.31.8.2      0.0.0.0         UG    0      0        0 eth0
10.200.55.128   172.31.8.112    255.255.255.192 UG    0      0        0 tunl0
10.200.70.64    172.31.8.113    255.255.255.192 UG    0      0        0 tunl0
10.200.92.64    172.31.8.103    255.255.255.192 UG    0      0        0 tunl0
10.200.155.128  0.0.0.0         255.255.255.192 U     0      0        0 *
10.200.155.163  0.0.0.0         255.255.255.255 UH    0      0        0 cali073b411309a
10.200.175.192  172.31.8.101    255.255.255.192 UG    0      0        0 tunl0
10.200.241.192  172.31.8.102    255.255.255.192 UG    0      0        0 tunl0
172.31.8.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0
[root@k8s-node1:~]# ifconfig eth0
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.31.8.111  netmask 255.255.255.0  broadcast 172.31.8.255
        inet6 fe80::20c:29ff:fe4d:b886  prefixlen 64  scopeid 0x20<link>
        ether 00:0c:29:4d:b8:86  txqueuelen 1000  (Ethernet)
        RX packets 39205  bytes 9165780 (9.1 MB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 25946  bytes 2545539 (2.5 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

# 清除
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl delete -f deploy_nfs.yml
deployment.apps "nginx-deployment" deleted
service "ng-deploy-80" deleted

[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl get pods
NAME        READY   STATUS    RESTARTS       AGE
net-test1   1/1     Running   4 (114m ago)   25h
net-test2   1/1     Running   9 (114m ago)   2d12h
```

#### 3.10.4.3：nfs挂载示例：单个存储

```sh
[root@k8s-ha1:~]# vim /etc/exports
/data/k8sdata 172.31.8.0/24(rw,no_root_squash)

[root@k8s-ha1:~]# systemctl restart nfs-server.service 

# 只要在宿主机上能挂上，就没有问题
[root@k8s-master1:~/yaml/20240629/case7-nfs]# mount -t nfs 172.31.8.109:/data/k8sdata/linux66 /mnt/
[root@k8s-master1:~/yaml/20240629/case7-nfs]# umount /mnt

[root@k8s-node1:~]# showmount -e 172.31.8.109
Export list for 172.31.8.109:
/data/k8sdata 172.31.8.0/24

# 创建pod
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl apply -f deploy_nfs.yml
deployment.apps/nginx-deployment created
service/ng-deploy-80 created

[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS       AGE     IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                           1/1     Running   4 (122m ago)   25h     10.200.155.163   172.31.8.111   <none>           <none>
net-test2                           1/1     Running   9 (122m ago)   2d12h   10.200.55.185    172.31.8.112   <none>           <none>
nginx-deployment-6bdfb774d8-vmltg   1/1     Running   0              13s     10.200.155.167   172.31.8.111   <none>           <none>

# 进入pod
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl exec -it nginx-deployment-6bdfb774d8-vmltg -- bash
root@nginx-deployment-6bdfb774d8-vmltg:/# df    
Filesystem                         1K-blocks    Used Available Use% Mounted on
overlay                            104806400 9935324  94871076  10% /
tmpfs                                  65536       0     65536   0% /dev
tmpfs                                1990820       0   1990820   0% /sys/fs/cgroup
shm                                    65536       0     65536   0% /dev/shm
/dev/sda3                          104806400 9935324  94871076  10% /etc/hosts
tmpfs                                3674444      12   3674432   1% /run/secrets/kubernetes.io/serviceaccount
172.31.8.109:/data/k8sdata/linux66  52403200  398336  52004864   1% /usr/share/nginx/html/mysite
tmpfs                                1990820       0   1990820   0% /proc/acpi
tmpfs                                1990820       0   1990820   0% /proc/scsi
tmpfs                                1990820       0   1990820   0% /sys/firmware
root@nginx-deployment-6bdfb774d8-vmltg:/# exit
exit

PS：
这个挂载并不是在容器内挂的，因为容器就根本没有内核。宿主机在没有nfs驱动的时候都挂不上，更别提容器没有内核了。
容器到底是怎么挂的呢？实际是映射过去的。
# 先查看容器在哪个主机，这里是node1
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS       AGE     IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                           1/1     Running   4 (126m ago)   25h     10.200.155.163   172.31.8.111   <none>           <none>
net-test2                           1/1     Running   9 (126m ago)   2d12h   10.200.55.185    172.31.8.112   <none>           <none>
nginx-deployment-6bdfb774d8-vmltg   1/1     Running   0              4m36s   10.200.155.167   172.31.8.111   <none>           <none>

[root@k8s-node1:~]# df -TH
Filesystem                         Type      Size  Used Avail Use% Mounted on
udev                               devtmpfs  2.0G     0  2.0G   0% /dev
tmpfs                              tmpfs     408M  2.2M  406M   1% /run
/dev/sda3                          xfs       108G   11G   98G  10% /
tmpfs                              tmpfs     2.1G     0  2.1G   0% /dev/shm
tmpfs                              tmpfs     5.3M     0  5.3M   0% /run/lock
tmpfs                              tmpfs     2.1G     0  2.1G   0% /sys/fs/cgroup
/dev/loop1                         squashfs   97M   97M     0 100% /snap/lxd/24061
/dev/loop0                         squashfs   66M   66M     0 100% /snap/core20/1611
/dev/loop2                         squashfs   41M   41M     0 100% /snap/snapd/21759
/dev/loop3                         squashfs   72M   72M     0 100% /snap/lxd/22753
/dev/loop4                         squashfs   68M   68M     0 100% /snap/core20/2318
/dev/sda5                          xfs        54G  408M   54G   1% /data
/dev/sda2                          xfs       1.1G  263M  801M  25% /boot
......
172.31.8.109:/data/k8sdata/linux66 nfs4       54G  408M   54G   1% /var/lib/kubelet/pods/ea362c06-599d-4c28-a20a-24e120985f6c/volumes/kubernetes.io~nfs/my-nfs-volume
......

实际上是在宿主机把你的挂载，挂到容器的一个工作目录。再使用联合文件系统，映射到你的容器中去使用。是这样的方式实现的。因为宿主机没有内核，它根本就不识别nfs驱动。

# 在nfs服务器共享目录上下载一张图片
[root@k8s-ha1:/data/k8sdata/linux66]# wget https://s3v2.53kf.com/kfxt/2023/08/image/72145423_setting_1692090188_64db3f4c03b0c.jpg
[root@k8s-ha1:/data/k8sdata/linux66]# mv 72145423_setting_1692090188_64db3f4c03b0c.jpg 1.jpg

# 容器中可以看到这张下载的图片
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl exec -it nginx-deployment-6bdfb774d8-vmltg -- bash
root@nginx-deployment-6bdfb774d8-vmltg:/# df
Filesystem                         1K-blocks    Used Available Use% Mounted on
overlay                            104806400 9943060  94863340  10% /
tmpfs                                  65536       0     65536   0% /dev
tmpfs                                1990820       0   1990820   0% /sys/fs/cgroup
shm                                    65536       0     65536   0% /dev/shm
/dev/sda3                          104806400 9943060  94863340  10% /etc/hosts
tmpfs                                3674444      12   3674432   1% /run/secrets/kubernetes.io/serviceaccount
172.31.8.109:/data/k8sdata/linux66  52403200  398592  52004608   1% /usr/share/nginx/html/mysite
tmpfs                                1990820       0   1990820   0% /proc/acpi
tmpfs                                1990820       0   1990820   0% /proc/scsi
tmpfs                                1990820       0   1990820   0% /sys/firmware
root@nginx-deployment-6bdfb774d8-vmltg:/# ls /usr/share/nginx/html/mysite
1.jpg
root@nginx-deployment-6bdfb774d8-vmltg:/# exit
exit

访问http://172.31.8.109:300016/mysite/1.jpg，可以看到图片

# 删除
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl delete -f deploy_nfs.yml
deployment.apps "nginx-deployment" deleted
service "ng-deploy-80" deleted

```

![](assets/images/nfs_volume.png)



#### 3.10.4.4：nfs挂载示例：多个存储

```sh
# 编辑yaml文件
[root@k8s-master1:~/yaml/20240629/case7-nfs]# vim deploy_nfs2.yml
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-site2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-deploy-81
  template:
    metadata:
      labels:
        app: ng-deploy-81
    spec:
      containers:
      - name: ng-deploy-81
        image: nginx 
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /usr/share/nginx/html/mysite
          name: my-nfs-volume
        - mountPath: /usr/share/nginx/html/magedu
          name: magedu-statics-volume
      volumes:
      - name: my-nfs-volume
        nfs:
          server: 172.31.8.109
          path: /data/k8sdata/linux66
      - name: magedu-statics-volume
        nfs:
          server: 172.31.8.110
          path: /data/magedu

---
apiVersion: v1
kind: Service
metadata:
  name: ng-deploy-81
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30017
    protocol: TCP
  type: NodePort
  selector:
    app: ng-deploy-81


# 在ha2上安装和配置nfs
[root@k8s-ha2:~]# apt install nfs-server -y

[root@k8s-ha2:~]# mkdir /data/magedu -p

[root@k8s-ha2:~]# vim /etc/exports
/data/magedu *(rw,no_root_squash)

[root@k8s-ha2:~]# systemctl restart nfs-server.service 
[root@k8s-ha2:~]# systemctl enable nfs-server.service

# 执行yaml
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl apply -f deploy_nfs2.yml
deployment.apps/nginx-deployment-site2 created
service/ng-deploy-81 created

[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl get pods -o wide
NAME                                      READY   STATUS    RESTARTS       AGE     IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                                 1/1     Running   4 (158m ago)   26h     10.200.155.163   172.31.8.111   <none>           <none>
net-test2                                 1/1     Running   9 (158m ago)   2d12h   10.200.55.185    172.31.8.112   <none>           <none>
nginx-deployment-site2-6f9c4b9d87-9n6z5   1/1     Running   0              13s     10.200.155.168   172.31.8.111   <none>           <none>

# 进入到容器中查看挂载
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl exec -it nginx-deployment-site2-6f9c4b9d87-9n6z5 -- bash
root@nginx-deployment-site2-6f9c4b9d87-9n6z5:/# df -TH
Filesystem                         Type     Size  Used Avail Use% Mounted on
overlay                            overlay  108G   11G   98G  10% /
tmpfs                              tmpfs     68M     0   68M   0% /dev
tmpfs                              tmpfs    2.1G     0  2.1G   0% /sys/fs/cgroup
/dev/sda3                          xfs      108G   11G   98G  10% /etc/hosts
shm                                tmpfs     68M     0   68M   0% /dev/shm
tmpfs                              tmpfs    3.8G   13k  3.8G   1% /run/secrets/kubernetes.io/serviceaccount
172.31.8.109:/data/k8sdata/linux66 nfs4      54G  409M   54G   1% /usr/share/nginx/html/mysite
172.31.8.110:/data/magedu          nfs4      54G  408M   54G   1% /usr/share/nginx/html/magedu
tmpfs                              tmpfs    2.1G     0  2.1G   0% /proc/acpi
tmpfs                              tmpfs    2.1G     0  2.1G   0% /proc/scsi
tmpfs                              tmpfs    2.1G     0  2.1G   0% /sys/firmware
root@nginx-deployment-site2-6f9c4b9d87-9n6z5:/# exit
exit

# 多个存储，如何挂载？
再找个服务器
yml文件中再声明一个存储。volumes下多添加一个值。volumeMounts下也得增加一个值。
# 这种场景不多。我们知道定义多个挂载卷就可以。

# 删除
[root@k8s-master1:~/yaml/20240629/case7-nfs]# kubectl delete -f deploy_nfs2.yml
deployment.apps "nginx-deployment-site2" deleted
service "ng-deploy-81" deleted
```

#### 3.10.4.5：总结

```sh
如果是商业ceph，是可以限制大小的。还有权限审核。
nfs没有空间的限制。


nfs等共享存储：
准备个nfs存储，如下：
nfs服务的配置：vim /etc/exports --> /data/k8sdata *(rw,no_root_squash) # 注意*和权限没有空格！
重启nfs服务。启动后 showmount -e  # 这个命令需要安装
apt install nfs-common  # 这个是nfs的通用组件，实际就是驱动。或者挂不上，或者访问很慢。
然后：mount -t nfs 172.31.7.109:/data/k8sdata/linux66 /mnt/ 测试是否能挂载

容器不能直接挂载，是宿主机挂载，映射到宿主机的一个目录。通过联合文件驱动给容器使用。

同一个容器里挂载多个nfs。

nfs这种存储没有做权限审核和空间限制。
如何限制呢？pvc

没有额外需求，nfs也就足够！！！
```

## 3.11：PV和PVC 

### 3.11.1：概念

```sh
## PV/PVC  PersistentVolume PersistentVolumeClaim  它们是逻辑组件，并不是真正存数据的服务器
# 存数据还是要往ceph或者nfs上存储。
# 有些场景下，需要进行更加精细的空间和权限限制。

用于实现pod和storage的解耦，这样我们修改storage的时候不需要修改pod，后端服务器做一些变更，就不用改pod。
与NFS的区别，可以在PV和PVC层面实现对存储服务器的空间分配、存储的访问权限管理等，但是数据终还是存储到物理硬盘。
k8s从1.0版本开始支持PV/PVC

为什么还有个PVC呢？
这个PV用于和硬件对接的。硬件服务器就是存储，比如NAS，需要有个组件和它对接，因为涉及到一些驱动。k8s里定义了一些规范，PV里定义了接口标准，凡是提供存储的组件，比如，ceph和NAS都需要符合这些标准，就具备了硬件连接的功能。

通常情况下，一个redis 2G，kafka 3G ，数据库20G。空间不够可以扩容。
PV是直接和硬件对接。
PV不能和pod直接挂载。
PV不能提供挂载功能。

那么，PV如何挂载呢？封装为PVC。
容器挂载PVC。但是PVC也存储不了数据。
把PVC绑定PV上，PV关联到后端存储。
pod --> PVC --绑定--> PV --关联或者绑定--> NAS或者ceph，如果是ceph，数据都在OSD1，OSD2，OSD3

PV一定是指定了后端服务器的类型，指定了后端服务器的地址。

# 帮助文档，中文
https://kubernetes.io/zh-cn/docs/concepts/storage/volumes/

# 访问的四种权限：要去帮助里去查到底哪个存储支持这些权限，具体取决于存储。
ReadWriteOnce 单个pod以读写权限挂载。这种适合有状态服务挂载。场景：MySQL 主从。主库和从库都是不同的PV和PVC。只能挂一个pod。
ReadOnlyMany 只读。场景：动静分离，比如：nginx，前端是nginx，可以只读。可以挂多个pod。
ReadWriteMany 场景：Java服务器。java或者go后端服务，要写。可以挂多个pod。
ReadWriteOncePod 是后来加的。类似于job，挂载一次就结束了，再往后挂不上了。几乎用不上。
```

### 3.11.2：简介

```sh
## PV/PVC简介
PV：是集群中已经由kubernetes管理员配置的一个网络存储，集群中的存储资源是一个集群资源，即不隶属于任何namespace，PV的数据最终存储在硬件存储，pod不能直接挂载PV，PV需要绑定给PVC并最终由pod挂载PVC使用，PV其支持Ceph、商业存储或云提供商的特定的存储等，可以自定义PV的类型是块还是文件存储、存储空间大小、访问模式等，PV的生命周期独立于pod，即当使用PV的pod被删除时可以对PV中的数据没有影响。
PV是全局资源。PV可以关联到很多硬件存储，nfs，ceph，公有云，或者商业存储。

PVC：是pod对存储的请求，pod挂载PVC并将数据存储在PVC，而PVC需要绑定到PV才能使用，另外PVC在创建的时候要指定namespace，即pod要和PVC必须运行在同一个namespace，如果不在就找不到。可以对PVC设置特定的空间大小和访问模式，使用PVC的pod在删除时也可以对PVC中的数据没有影响。

Ceph和NFS是文件的。
RBD和iSCSI是块的。

# 注意：pvc是pod对存储的请求。pvc要指定namespace。pvc要和pod运行在同一个namespace。

# 有些存储时需要认证的，比如，Ceph是需要认证的，认证过不了，PV也挂不了，那PVC也挂不了。

# 一个服务可以使用独立的PV。或者把一个服务的数据隔离开：MySQL主从，可以让master使用Ceph，slave使用nfs，完全没有问题。

## 总结：
PV是对底层网络存储的抽象，即将网络存储定义为一种存储资源，将一个整体的存储资源拆分成多份后给不同的业务使用。
PVC是对PV资源的申请调用，pod是通过PVC将数据保存至PV，PV再把数据保存至真正的硬件存储。

## PersistVolume参数：
# 使用命令去查：kubectl explain PersistentVolume

Capacity: # 当前PV空间大小，kubectl explain PersistentVolume.spec.capacity

accessModes: # 访问模式， # kubectl explain PersistentVolume.spec.accessModes
  ReadWriteOnce -- PV只能被单个节点以读写权限挂载，RWO
  ReadOnlyMany  -- PV可以被多个节点挂载，但是权限是只读的，ROX
  ReadWriteMany -- PV可以被多个节点以读写方式挂载使用，RWX
  
persistentVolumeReclaimPolicy # 回收策略，即删除存储卷的时候（主要是删除pod），已经创建好的存储卷是保留还是删除？
  # kubectl explain PersistentVolume.spec.persistentVolumeReclaimPolicy
  Retain  -- 删除pod、PV、PVC后保持原状（在硬件nfs，ceph的数据保留），最后需要管理员手动删除。一般情况下使用这个选项！！！
  Recycle -- 空间回收，及删除存储卷上的所有数据（包括目录和隐藏文件），目前仅支持NFS和hostPath。新版本k8s没有这个功能。
  Delete  -- 自动删除存储卷（就是存储上的目录），包括公有云的。相当于nfs，ceph的目录或者卷就被删除。
 
volumeMode # 卷类型 kubectl explain PersistentVolume.spec.volumeMode
  定义存储卷使用的文件系统是块设备还是文件系统。默认为文件系统。
  
mountOptions # 附加的挂载选项列表，实现更精细的权限控制
  ro #等 一般不在这里加，在accessModes中加权限多见。
  
## PV参数介绍结束

## PersistentVolumeClaim 创建参数
# 使用命令去查：kubectl explain PersistentVolumeClaim

accessModes: PVC访问模式， # kubectl explain PersistentVolumeClaim.spec.volumeMode
  ReadWriteOnce -- PVC只能被单个节点以读写权限挂载，RWO
  ReadOnlyMany  -- PVC可以被多个节点挂载，但是权限是只读的，ROX
  ReadWriteMany -- PVC可以被多个节点以读写方式挂载使用，RWX

resources: # 定义PVC创建存储卷的空间大小。它的大小要 <= PV。

selector: # 标签选择器，通过它去选择要绑定的PV
  matchLabels # 匹配标签名称（类似于service找pod）
  matchExpressions # 基于正则表达式匹配
  
volumeName: # 要绑定的PV名称（这里是精确指定）

volumeMode: # 卷类型
  定义PVC使用的文件系统是块设备还是文件系统。默认为文件系统。
  
## PVC参数介绍结束

## Volume存储卷类型：
static: 静态存储卷，需要在使用前手动创建PV，然后手动创建PVC并绑定到PV，然后挂载至pod使用，适用于PV和PVC相对比较固定的业务场景。
dynamic: 动态存储卷，先创建一个存储类storageclass（其实就是个模版，指定了后端存储是什么类型的，是ceph或者是NAS），后期pod在使用PVC的时候可以通过storageclass动态创建PV（Ceph还要配置认证，认证失败也创建不了）。
适用于有状态服务集群，比如MySQL的一主多从，zookeeper集群等。启动一个pod，它会先申请PVC，然后找到storageclass（有认证信息，后端服务器的存储信息），动态生成PV，PV和真正的硬件存储关联起来。这种情况下是每个pod都有一个独立的PVC。

注意：但是大部分场景，都是静态的。动静分离的场景，pod有几十个，上百个。但是它们挂载的都是同一个PVC，这样nginx可以拿到同样的数据。动态服务场景，也是使用同一个PVC，要不数据写到别的地方了，这样也不行。所以，这种情况下手动创建就足够，易于管理。
有些动态服务，可能要使用动态存储卷，这种也是有的。

PS：
通常情况下PVC和PV的accessModes要设置为一样的。
```

### 3.11.3：static volume 示例

#### 3.11.3.1：准备nfs存储

```sh
[root@k8s-ha1:~]# mkdir /data/k8sdata/myserver/myappdata -p

[root@k8s-ha1:~]# vim /etc/exports
/data/k8sdata *(rw,no_root_squash)
/data/k8sdata/myserver/myappdata *(rw,no_root_squash)

[root@k8s-ha1:~]# systemctl restart nfs-server.service
```

#### 3.11.3.2：创建PV

```sh
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# ll
total 12
drwxr-xr-x  2 root root  115 Jun 29 13:16 ./
drwxr-xr-x 15 root root  292 Jun 29 13:18 ../
-rw-r--r--  1 root root  227 Jun 29 13:16 1-myapp-persistentvolume.yaml
-rw-r--r--  1 root root  236 Jun 29 13:16 2-myapp-persistentvolumeclaim.yaml
-rw-r--r--  1 root root 1002 Jun 29 13:16 3-myapp-webserver.yaml

# 创建PV
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# vim 1-myapp-persistentvolume.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myserver-myapp-static-pv
spec:
  capacity:                   #PV的size，这个空间不能超过后端存储的空间
    storage: 10Gi
  accessModes:                #单个pod以读写权限挂载
    - ReadWriteOnce
  nfs:
    path: /data/k8sdata/myserver/myappdata   #要保证这个路径是后期可以挂载使用
    server: 172.31.8.109      #指定后端存储的nfs服务器是哪个

PS：这里不用指定namespace，因为PV是全局的。

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl apply -f 1-myapp-persistentvolume.yaml 
persistentvolume/myserver-myapp-static-pv created

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl get pv
NAME                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
myserver-myapp-static-pv   10Gi       RWO            Retain           Available                                   5s

PS：到此，PV创建成功。
状态是Available，意味着PV和nfs或者是后端的存储服务绑定好了，它们的网络是可以连接的，权限是可以对接的。包括大小等等，都是符合要求的。它们两个之间已经建立好连接关系了。

# 查看PV是全局的资源
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
......

PS：NAMESPACED这一列，PV的值是false。就说明PV是全局的。
```

#### 3.11.3.3：创建PVC

```sh
# 创建PVC
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# vim 2-myapp-persistentvolumeclaim.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myserver-myapp-static-pvc
  namespace: myserver
spec:
  volumeName: myserver-myapp-static-pv    #通过卷名字和PV关联
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi                       #大小是10G，不能超过PV

PS：PVC就需要指定namespace，你的业务容器在哪个ns下，PVC就必须在哪个ns下。

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl apply -f 2-myapp-persistentvolumeclaim.yaml 
persistentvolumeclaim/myserver-myapp-static-pvc created

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl get pvc -n myserver
NAME                        STATUS   VOLUME                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myserver-myapp-static-pvc   Bound    myserver-myapp-static-pv   10Gi       RWO                           7s

PS：
BOUND表示PVC和PV已经关联好了。
RWO表示被单个pod以读写权限挂载。
```

#### 3.11.3.4：创建单副本业务容器

```sh
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# vim 3-myapp-webserver.yaml 
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: myserver-myapp 
  name: myserver-myapp-deployment-name
  namespace: myserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myserver-myapp-frontend
  template:
    metadata:
      labels:
        app: myserver-myapp-frontend
    spec:
      containers:
        - name: myserver-myapp-container
          image: nginx:1.20.0 
          #imagePullPolicy: Always
          volumeMounts:
          - mountPath: "/usr/share/nginx/html/statics"    #默认的nginx的web路径
            name: statics-datadir
      volumes:
        - name: statics-datadir
          persistentVolumeClaim:
            claimName: myserver-myapp-static-pvc          #PVC的名字

---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: myserver-myapp-service
  name: myserver-myapp-service-name
  namespace: myserver
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30080
  selector:
    app: myserver-myapp-frontend

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl apply -f 3-myapp-webserver.yaml 
deployment.apps/myserver-myapp-deployment-name created
service/myserver-myapp-service-name created

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl get deploy -n myserver
NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
myserver-myapp-deployment-name   1/1     1            1           9s

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl get pods -n myserver -o wide
NAME                                             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-deployment-name-fb44b4447-m6nnc   1/1     Running   0          16s   10.200.155.173   172.31.8.111   <none>           <none>

# 进入到容器内部
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl exec -it myserver-myapp-deployment-name-fb44b4447-m6nnc -n myserver -- bash
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/# df -TH
Filesystem                                    Type     Size  Used Avail Use% Mounted on
overlay                                       overlay  108G   11G   98G  10% /
tmpfs                                         tmpfs     68M     0   68M   0% /dev
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /sys/fs/cgroup
shm                                           tmpfs     68M     0   68M   0% /dev/shm
/dev/sda3                                     xfs      108G   11G   98G  10% /etc/hosts
tmpfs                                         tmpfs    3.8G   13k  3.8G   1% /run/secrets/kubernetes.io/serviceaccount
172.31.8.109:/data/k8sdata/myserver/myappdata nfs4      54G  409M   54G   1% /usr/share/nginx/html/statics
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /proc/acpi
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /proc/scsi
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /sys/firmware
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/# exit
exit

PS：看到的还是把nfs挂载到容器中。只是中间有逻辑的PV和PVC来控制大小和权限而已。

# 在容器内部wget一个图片
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl exec -it myserver-myapp-deployment-name-fb44b4447-m6nnc -n myserver -- bash
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/# cd /usr/share/nginx/html/statics/                     
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/usr/share/nginx/html/statics# apt update
......
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/usr/share/nginx/html/statics# apt install wget -y
......
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/usr/share/nginx/html/statics# wget https://www.magedu.com/wp-content/uploads/2022/06/2022062105534384.jpg
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/usr/share/nginx/html/statics# ls
2022062105534384.jpg
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/usr/share/nginx/html/statics# mv 2022062105534384.jpg 1.jpg
root@myserver-myapp-deployment-name-fb44b4447-m6nnc:/usr/share/nginx/html/statics# exit
exit

# 在nfs存储上查看
[root@k8s-ha1:~]# ls /data/k8sdata/myserver/myappdata/
1.jpg

分别访问：
http://172.31.8.111:30080
http://172.31.8.111:30080/statics/1.jpg
```

![](assets/images/static_pv01.png)

![](assets/images/static_pv02.png)

#### 3.11.3.5：创建多副本业务容器

```sh
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# vim 3-myapp-webserver.yaml 
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: myserver-myapp 
  name: myserver-myapp-deployment-name
  namespace: myserver
spec:
  replicas: 3      #改为3
  selector:
    matchLabels:
      app: myserver-myapp-frontend
  template:
    metadata:
      labels:
        app: myserver-myapp-frontend
    spec:
      containers:
        - name: myserver-myapp-container
          image: nginx:1.20.0 
          #imagePullPolicy: Always
          volumeMounts:
          - mountPath: "/usr/share/nginx/html/statics"
            name: statics-datadir
      volumes:
        - name: statics-datadir
          persistentVolumeClaim:
            claimName: myserver-myapp-static-pvc 

---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: myserver-myapp-service
  name: myserver-myapp-service-name
  namespace: myserver
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30080
  selector:
    app: myserver-myapp-frontend

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl apply -f 3-myapp-webserver.yaml 
deployment.apps/myserver-myapp-deployment-name configured
service/myserver-myapp-service-name unchanged

[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl get pods -n myserver -o wide
NAME                                             READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-deployment-name-fb44b4447-bktvd   1/1     Running   0          32s   10.200.55.133    172.31.8.112   <none>           <none>
myserver-myapp-deployment-name-fb44b4447-m6nnc   1/1     Running   0          59m   10.200.155.173   172.31.8.111   <none>           <none>
myserver-myapp-deployment-name-fb44b4447-mjgzz   1/1     Running   0          32s   10.200.70.83     172.31.8.113   <none>           <none>

# 进入新创建的一个副本容器，看是否能挂载
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl exec -it myserver-myapp-deployment-name-fb44b4447-bktvd -n myserver -- bash
root@myserver-myapp-deployment-name-fb44b4447-bktvd:/# df -TH
Filesystem                                    Type     Size  Used Avail Use% Mounted on
overlay                                       overlay  108G  9.9G   98G  10% /
tmpfs                                         tmpfs     68M     0   68M   0% /dev
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /sys/fs/cgroup
shm                                           tmpfs     68M     0   68M   0% /dev/shm
/dev/sda3                                     xfs      108G  9.9G   98G  10% /etc/hosts
tmpfs                                         tmpfs    3.8G   13k  3.8G   1% /run/secrets/kubernetes.io/serviceaccount
172.31.8.109:/data/k8sdata/myserver/myappdata nfs4      54G  409M   54G   1% /usr/share/nginx/html/statics
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /proc/acpi
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /proc/scsi
tmpfs                                         tmpfs    2.1G     0  2.1G   0% /sys/firmware
root@myserver-myapp-deployment-name-fb44b4447-bktvd:/# ls /usr/share/nginx/html/statics/
1.jpg
root@myserver-myapp-deployment-name-fb44b4447-bktvd:/# exit
exit

PS：这三个pod不在同一node上。

可以看到三个pod调度到不同的宿主机。

访问
http://172.31.8.112:30080/statics/1.jpg
http://172.31.8.112:30080/statics/1.jpg
http://172.31.8.113:30080/statics/1.jpg
都可以看到图片。说明不影响挂载PVC。三个pod都可以挂载同一个PVC。

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl delete -f 3-myapp-webserver.yaml 
deployment.apps "myserver-myapp-deployment-name" deleted
service "myserver-myapp-service-name" deleted
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl delete -f 2-myapp-persistentvolumeclaim.yaml 
persistentvolumeclaim "myserver-myapp-static-pvc" deleted
[root@k8s-master1:~/yaml/20240629/case8-pv-static]# kubectl delete -f 1-myapp-persistentvolume.yaml 
persistentvolume "myserver-myapp-static-pv" deleted

PS：删除的时候要倒着删除，因为有关联关系。
```

### 3.11.4：dynamic volume 示例

#### 3.1.1.4.1：动态存储类简介

```sh
核心是创建storageclass。后端是nfs或者ceph，不同的后端配置方法不一样，它们的性能和高可用效果肯定不一样，可以自己去定。storageClass为管理员描述了不同存储类的方法。k8s并不知道后端服务器存储的持久化和策略是什么样子的，以及性能是如何的。k8s只是负责和后端存储做对接。先创建一个storageclass，它来指定后端存储是什么类型。storageclass的名字一定要记住，后面配置要用。

1、创建账户和授权
  kubectl apply -f 1-rbac.yaml
2、创建storageclass
  kubectl apply -f 2-storageclass.yaml
3、创建NFS provisioner
  kubectl apply -f 3-nfs-provisioner.yaml
4、创建PVC
  kubectl apply -f 4-create-pvc.yaml
5、创建web服务
  kubectl apply -f 5-myapp-webserver.yaml
```

#### 3.11.4.2：创建sa账户和授权

```sh
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# ll
total 28
drwxr-xr-x  2 root root  169 Jun 29 13:16 ./
drwxr-xr-x 15 root root  292 Jun 29 13:18 ../
-rw-r--r--  1 root root 1937 Jun 29 13:16 1-rbac.yaml
-rw-r--r--  1 root root  691 Jun 29 13:16 2-storageclass.yaml
-rw-r--r--  1 root root 1174 Jun 29 13:16 3-nfs-provisioner.yaml
-rw-r--r--  1 root root  308 Jun 29 13:16 4-create-pvc.yaml
-rw-r--r--  1 root root 1004 Jun 29 13:16 5-myapp-webserver.yaml

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# vim 1-rbac.yaml 
apiVersion: v1
kind: Namespace
metadata:
  name: nfs
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner       #sa账号，在k8s内部负责PV创建时的认证。
  # replace with namespace where provisioner is deployed
  namespace: nfs
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: nfs
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: nfs
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: nfs
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: nfs
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io


PS：
pod要去创建PVC，需要权限。
PVC要和PV关联，PV又要关联后端的nfs。
一旦创建好PV后，在nfs上会多一个目录，在你的共享目录里多了一个很长名字的目录，它是k8s动态创建的。
所以这些操作都是需要交互的。这就用到sa账号。

创建好sa后，要做rolebinding绑定的。所以先给它一个role，再做rolebinding。

PS：这是官方给的，不要改动。

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl apply -f 1-rbac.yaml 
namespace/nfs created
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
```

#### 3.11.4.3：创建StorageClass

```sh
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# vim 2-storageclass.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage   #名字，它可以自己改
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
reclaimPolicy: Retain #PV的删除策略，默认为delete，删除PV后立即删除NFS server的数据
mountOptions:
  - noresvport #告知NFS客户端在重新建立网络连接时，使用新的传输控制协议源端口
  - noatime #访问文件时不更新文件inode中的时间戳，高并发环境可提高性能，就是降低IO的。写的时候要改时间戳的。
parameters:
  mountOptions: "vers=4.1,noresvport,noatime"
  archiveOnDelete: "true"  #删除pod时保留pod数据，默认为false时为不保留数据 

PS：
k8s-sigs.io/nfs-subdir-external-provisioner，它来负责和nfs存储对接，来创建PV。注意要和deployment里的名字一样。
这是给后面的控制器，来通过它使用的。

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl apply -f 2-storageclass.yaml 
storageclass.storage.k8s.io/managed-nfs-storage created
```

#### 3.11.4.4：创建nfs制备器

```sh
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# vim 3-nfs-provisioner.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: nfs
spec:
  replicas: 1
  strategy: #部署策略
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 
          #image: registry.cn-qingdao.aliyuncs.com/zhangshijie/nfs-subdir-external-provisioner:v4.0.2 
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.31.8.109
            - name: NFS_PATH
              value: /data/volumes
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.31.8.109
            path: /data/volumes

PS：
这个容器在创建的时候，会到nfs的后端存储上/data/volumes，帮你创建这个PV。
这个容器需要知道你的后端存储服务器是哪个，所以在volumes里指定。

# 需要在nfs上创建/data/volumes的共享
[root@k8s-ha1:~]# mkdir /data/volumes/

[root@k8s-ha1:~]# vim /etc/exports
/data/k8sdata *(rw,no_root_squash)
/data/k8sdata/myserver/myappdata *(rw,no_root_squash)
/data/volumes *(rw,no_root_squash)

[root@k8s-ha1:~]# systemctl restart nfs-server.service

# 创建nfs制备器
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl apply -f 3-nfs-provisioner.yaml 
deployment.apps/nfs-client-provisioner created

# 查看nfs制备器是否创建成功
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl get pods -n nfs
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-78c668c5cb-xd7l4   1/1     Running   0          58s

PS：这个创建与否很重要，它关系到能否在nfs中动态创建PV。

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl logs -f nfs-client-provisioner-78c668c5cb-xd7l4 -n nfs

PS：执行这个命令没有报错，就说明没有问题。
```

#### 3.11.4.5：创建PVC 

```sh
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# vim 4-create-pvc.yaml 
# Test PVC
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myserver-myapp-dynamic-pvc
  namespace: myserver
spec:
  storageClassName: managed-nfs-storage #调用的storageclass 名称
  accessModes:
    - ReadWriteMany #访问权限
  resources:
    requests:
      storage: 500Mi #空间大小

PS：这个PVC就不需要指定PV了。不需要通过label或者正则来匹配PV。
直接写一个StorageClass就行。

所以，在k8s环境里一定有一个storageclass。这个storageclass会调用制备器去创建PV。
这里，有一套关联关系：
pod ----> storageclass ----> provisioner ----> 这个provisoner再去nfs上帮你创建PV。

# 查看目前的PVC和PV，应该都没有！！！
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl get pvc -n myserver
No resources found in myserver namespace.
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl get pv
No resources found

# 创建PVC
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl apply -f 4-create-pvc.yaml 
persistentvolumeclaim/myserver-myapp-dynamic-pvc created

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl get pvc -n myserver
NAME                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
myserver-myapp-dynamic-pvc   Bound    pvc-eb1e049f-cdd6-4265-8722-ecdf7558725c   500Mi      RWX            managed-nfs-storage   7s

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                 STORAGECLASS          REASON   AGE
pvc-eb1e049f-cdd6-4265-8722-ecdf7558725c   500Mi      RWX            Retain           Bound    myserver/myserver-myapp-dynamic-pvc   managed-nfs-storage            11s

PS：此时，PVC和PV都已经创建好。

# 在nfs中查看
[root@k8s-ha1:~]# ll /data/volumes/
total 0
drwxr-xr-x 3 root root 90 Jun 30 16:16 ./
drwxr-xr-x 4 root root 36 Jun 30 16:01 ../
drwxrwxrwx 2 root root  6 Jun 30 16:16 myserver-myserver-myapp-dynamic-pvc-pvc-eb1e049f-cdd6-4265-8722-ecdf7558725c/

PS：这是制备器自动创建的。名字显示pvc开头，但是它是PV。
```

#### 3.11.4.6：创建业务容器

```sh
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# vim 5-myapp-webserver.yaml 
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  labels:
    app: myserver-myapp 
  name: myserver-myapp-deployment-name
  namespace: myserver
spec:
  replicas: 1 
  selector:
    matchLabels:
      app: myserver-myapp-frontend
  template:
    metadata:
      labels:
        app: myserver-myapp-frontend
    spec:
      containers:
        - name: myserver-myapp-container
          image: nginx:1.20.0 
          #imagePullPolicy: Always
          volumeMounts:
          - mountPath: "/usr/share/nginx/html/statics"
            name: statics-datadir
      volumes:
        - name: statics-datadir
          persistentVolumeClaim:
            claimName: myserver-myapp-dynamic-pvc 

---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: myserver-myapp-service
  name: myserver-myapp-service-name
  namespace: myserver
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30080
  selector:
    app: myserver-myapp-frontend

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl apply -f 5-myapp-webserver.yaml 
deployment.apps/myserver-myapp-deployment-name created
service/myserver-myapp-service-name created

[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl get pods -n myserver -o wide
NAME                                              READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-deployment-name-7c855dc86d-swl46   1/1     Running   0          26s   10.200.155.175   172.31.8.111   <none>           <none>

# 进入到pod查看挂载
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl exec -it myserver-myapp-deployment-name-7c855dc86d-swl46 -n myserver -- bash
root@myserver-myapp-deployment-name-7c855dc86d-swl46:/# df -TH
Filesystem                                                                                              Type     Size  Used Avail Use% Mounted on
overlay                                                                                                 overlay  108G   11G   98G  10% /
tmpfs                                                                                                   tmpfs     68M     0   68M   0% /dev
tmpfs                                                                                                   tmpfs    2.1G     0  2.1G   0% /sys/fs/cgroup
shm                                                                                                     tmpfs     68M     0   68M   0% /dev/shm
/dev/sda3                                                                                               xfs      108G   11G   98G  10% /etc/hosts
tmpfs                                                                                                   tmpfs    3.8G   13k  3.8G   1% /run/secrets/kubernetes.io/serviceaccount
172.31.8.109:/data/volumes/myserver-myserver-myapp-dynamic-pvc-pvc-eb1e049f-cdd6-4265-8722-ecdf7558725c nfs4      54G  409M   54G   1% /usr/share/nginx/html/statics
tmpfs                                                                                                   tmpfs    2.1G     0  2.1G   0% /proc/acpi
tmpfs                                                                                                   tmpfs    2.1G     0  2.1G   0% /proc/scsi
tmpfs                                                                                                   tmpfs    2.1G     0  2.1G   0% /sys/firmware
root@myserver-myapp-deployment-name-7c855dc86d-swl46:/# exit
exit

# 能不能写数据呢？进入到容器
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl exec -it myserver-myapp-deployment-name-7c855dc86d-swl46 -n myserver -- bash
root@myserver-myapp-deployment-name-7c855dc86d-swl46:/# cd /usr/share/nginx/html/statics/
root@myserver-myapp-deployment-name-7c855dc86d-swl46:/usr/share/nginx/html/statics# echo "statics" > index.html
root@myserver-myapp-deployment-name-7c855dc86d-swl46:/usr/share/nginx/html/statics# exit
exit

访问
http://172.31.8.111:30080/statics
可以显示内容。

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl delete -f 5-myapp-webserver.yaml 
deployment.apps "myserver-myapp-deployment-name" deleted
service "myserver-myapp-service-name" deleted
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl delete -f 4-create-pvc.yaml 
persistentvolumeclaim "myserver-myapp-dynamic-pvc" deleted
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl delete -f 3-nfs-provisioner.yaml 
deployment.apps "nfs-client-provisioner" deleted
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl delete -f 2-storageclass.yaml 
storageclass.storage.k8s.io "managed-nfs-storage" deleted
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl delete -f 1-rbac.yaml 
namespace "nfs" deleted
serviceaccount "nfs-client-provisioner" deleted
clusterrole.rbac.authorization.k8s.io "nfs-client-provisioner-runner" deleted
clusterrolebinding.rbac.authorization.k8s.io "run-nfs-client-provisioner" deleted
role.rbac.authorization.k8s.io "leader-locking-nfs-client-provisioner" deleted
rolebinding.rbac.authorization.k8s.io "leader-locking-nfs-client-provisioner" deleted
```

![](assets/images/dynamic_pv01.png)

## 3.12：ConfigMap

### 3.12.1：概念

```sh
## Configmap: 配置信息和镜像解耦，实现方式为将配置信息放到configmap对象中。在pod中，将configmap作为Volume挂载到pod中，从而实现导入外部的配置的目的。给容器加载去使用，而容器里不用事先准备配置。

使用场景：
1、通过Configmap给pod定义全局环境变量。这个用的不多。
2、通过Configmap给pod传递命令行参数，如mysql -u -p中的账户名密码可以通过Configmap传递，类似于环境变量
3、通过Configmap给pod中的容器服务提供配置文件，配置文件以挂载到容器的形式使用。这种使用比较常见。

# 安装k8s后就会有configmap存在
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl get cm -A
NAMESPACE              NAME                                 DATA   AGE
default                kube-root-ca.crt                     1      3d6h
kube-node-lease        kube-root-ca.crt                     1      3d6h
kube-public            kube-root-ca.crt                     1      3d6h
kube-system            calico-config                        8      3d6h
kube-system            coredns                              1      3d6h
kube-system            extension-apiserver-authentication   6      3d6h
kube-system            kube-root-ca.crt                     1      3d6h
kubernetes-dashboard   kube-root-ca.crt                     1      3d1h
kubernetes-dashboard   kubernetes-dashboard-settings        0      3d1h
linux66                kube-root-ca.crt                     1      42h
myserver               kube-root-ca.crt                     1      2d3h
nfs                    kube-root-ca.crt                     1      127m
velero-system          kube-root-ca.crt                     1      2d4h

# 可以看看里面的配置，不要轻易动里面的配置
[root@k8s-master1:~/yaml/20240629/case9-pv-dynamic-nfs]# kubectl edit configmap coredns -n kube-system

# 如何提供环境变量：
1.configmap
2.yaml，通过这里来定义最好。最直接有效。
3.打到镜像里，但是想改还比较麻烦。还要重新打镜像。
4,配置中心 appolo。这个比较麻烦。让容器启动时去配置中心拿配置。

# Configmap的注意事项：
1、Configmap需要在pod使用它之前创建
2、pod只能使用位于同一个namespace的Configmap，Configmap不能跨namespace使用
3、通常用于非安全加密的配置场景
4、Configmap通常是小于1MB的配置。不能把它当做存储东西的地方。

PS：
一般是传递配置文件。nginx和mysql等服务。
账号名和密码一般用Secret提供。
大公司一般就是自己弄配置中心，都不会用configmap。配置中心肯定比configmap好。
```

### 3.12.2：通过configmap给容器提供配置

```sh
[root@k8s-master1:~/yaml/20240629/case10-configmap]# ll
total 8
drwxr-xr-x  2 root root   70 Jun 29 13:16 ./
drwxr-xr-x 15 root root  292 Jun 29 13:18 ../
-rw-r--r--  1 root root 1233 Jun 29 13:16 1-deploy_configmap.yml
-rw-r--r--  1 root root  820 Jun 29 13:16 2-deploy_configmap_env.yml

[root@k8s-master1:~/yaml/20240629/case10-configmap]# vim 1-deploy_configmap.yml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config     #Configmap的名字，这个必须准备好。其实就是配置文件的名字。
data:
 default: |              #可以有多个。这里只有一个。
    server {
       listen       80;
       server_name  www.mysite.com;
       index        index.html index.php index.htm;

       location / {
           root /data/nginx/html;
           if (!-e $request_filename) {
               rewrite ^/(.*) /index.html last;
           }
       }
    }

---
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-deploy-80
  template:
    metadata:
      labels:
        app: ng-deploy-80
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx:1.20.0
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-config
          mountPath:  /etc/nginx/conf.d/mysite   #注意，一定要这么写！！！否则会把/etc/nginx/conf.d/里的default覆盖
      volumes:
      - name: nginx-config    #这个名字是对应volumeMounts的名字
        configMap:            #volume的类型
          name: nginx-config  #这个名字是对应Configmap的名字
          items:              #有哪些项？
             - key: default   #通过default去找
               path: mysite.conf   #把这个值挂到nginx的/etc/nginx/conf.d/mysite.conf文件中

---
apiVersion: v1
kind: Service
metadata:
  name: ng-deploy-80
spec:
  ports:
  - name: http
    port: 81
    targetPort: 80
    nodePort: 30019
    protocol: TCP
  type: NodePort
  selector:
    app: ng-deploy-80
    
PS：
上面配置中需要注意/etc/nginx/conf.d/mysite的写法。
如果是/etc/nginx/conf.d，就会把它里面的default.conf给强制覆盖了。
一定要挂在新的不存在的目录才可以，切记！！！

[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl apply -f 1-deploy_configmap.yml 
configmap/nginx-config created
deployment.apps/nginx-deployment created
service/ng-deploy-80 created

[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS         AGE    IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                           1/1     Running   5 (5h29m ago)    43h    10.200.155.169   172.31.8.111   <none>           <none>
net-test2                           1/1     Running   10 (5h29m ago)   3d6h   10.200.55.190    172.31.8.112   <none>           <none>
nginx-deployment-779cb4cbd8-qm7f9   1/1     Running   0                11s    10.200.155.176   172.31.8.111   <none>           <none>

# 进入容器查看
[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl exec -it nginx-deployment-779cb4cbd8-qm7f9 -- bash
root@nginx-deployment-779cb4cbd8-qm7f9:/# ls /etc/nginx/conf.d/       
default.conf  mysite
root@nginx-deployment-779cb4cbd8-qm7f9:/# ls /etc/nginx/conf.d/mysite/
mysite.conf
root@nginx-deployment-779cb4cbd8-qm7f9:/# cat /etc/nginx/conf.d/mysite/mysite.conf
server {
   listen       80;
   server_name  www.mysite.com;
   index        index.html index.php index.htm;

   location / {
       root /data/nginx/html;
       if (!-e $request_filename) {
           rewrite ^/(.*) /index.html last;
       }
   }
}
root@nginx-deployment-779cb4cbd8-qm7f9:/# exit
exit

# 重启容器内的nginx服务
[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl exec -it nginx-deployment-779cb4cbd8-qm7f9 -- bash
root@nginx-deployment-779cb4cbd8-qm7f9:/# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
root@nginx-deployment-779cb4cbd8-qm7f9:/# nginx -s reload
2024/06/30 22:21:59 [notice] 66#66: signal process started
root@nginx-deployment-779cb4cbd8-qm7f9:/# exit
exit

访问：
http://172.31.8.113:30019/
可以看到nginx的首页。

# 但是，需要注意，这个配置文件并没有被加载，还要在nginx的主配置文件中include进去
[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl exec -it nginx-deployment-779cb4cbd8-qm7f9 -- bash
root@nginx-deployment-779cb4cbd8-qm7f9:/# apt update
......
root@nginx-deployment-779cb4cbd8-qm7f9:/# apt install vim -y
......
root@nginx-deployment-779cb4cbd8-qm7f9:/# vim /etc/nginx/nginx.conf
增加 include /etc/nginx/conf.d/mysite/*.conf;
root@nginx-deployment-779cb4cbd8-qm7f9:/# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
root@nginx-deployment-779cb4cbd8-qm7f9:/# nginx -s reload
2024/06/30 22:32:45 [notice] 465#465: signal process started
root@nginx-deployment-779cb4cbd8-qm7f9:/# cat /etc/nginx/conf.d/mysite/mysite.conf
server {
   listen       80;
   server_name  www.mysite.com;
   index        index.html index.php index.htm;

   location / {
       root /data/nginx/html;
       if (!-e $request_filename) {
           rewrite ^/(.*) /index.html last;
       }
   }
}
root@nginx-deployment-779cb4cbd8-qm7f9:/# mkdir -p /data/nginx/html
root@nginx-deployment-779cb4cbd8-qm7f9:/# cd /data/nginx/html/
root@nginx-deployment-779cb4cbd8-qm7f9:/data/nginx/html# echo "www.mysite.com" > index.html
root@nginx-deployment-779cb4cbd8-qm7f9:/data/nginx/html# ls
index.html
root@nginx-deployment-779cb4cbd8-qm7f9:/data/nginx/html# exit
exit

PS：
正常的你打好的nginx镜像，应该把include /etc/nginx/conf.d/mysite/*.conf;加进去


我们需要访问这个域名来看到自己的页面。
如何访问这个域名呢？通过负载均衡器就可以。然后改本地host解析。nginx就会拿到请求。
# 修改ha
[root@k8s-ha1:~]# vim /etc/haproxy/haproxy.cfg 
listen sunnycreek-linux66-nginx-80
  bind 172.31.8.189:80
  mode tcp
  server node1 172.31.8.112:30019 check inter 3s fall 3 rise 1
  server node2 172.31.8.113:30019 check inter 3s fall 3 rise 1 

[root@k8s-ha1:~]# systemctl restart haproxy.service

# 配置本机解析
172.31.8.189 www.mysite.com

访问：
http://www.mysite.com

增强实验：
可以通过nfs提供数据，configmap提供nginx配置。

# 可以通过监控日志来看是否访问这个服务
[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl logs -f nginx-deployment-779cb4cbd8-qm7f9
......
172.31.8.111 - - [30/Jun/2024:22:36:45 +0000] "GET / HTTP/1.1" 304 0 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36" "-"
10.200.70.64 - - [30/Jun/2024:22:49:30 +0000] "GET / HTTP/1.1" 200 15 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36" "-"
10.200.70.64 - - [30/Jun/2024:22:49:31 +0000] "GET /favicon.ico HTTP/1.1" 200 15 "http://www.mysite.com/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36" "-"
10.200.70.64 - - [30/Jun/2024:22:55:24 +0000] "GET / HTTP/1.1" 304 0 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36" "-"
10.200.70.64 - - [30/Jun/2024:22:55:25 +0000] "GET /favicon.ico HTTP/1.1" 304 0 "http://www.mysite.com/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36" "-"
10.200.70.64 - - [30/Jun/2024:22:55:27 +0000] "GET / HTTP/1.1" 304 0 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36" "-"

PS：
注意200和304的区别。
为了尽快响应客户端请求，会比较Etag值，如果内容没变，就会使用客户端的内容，也就是缓存，这个状态码就是304。

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl delete -f 1-deploy_configmap.yml 
configmap "nginx-config" deleted
deployment.apps "nginx-deployment" deleted
service "ng-deploy-80" deleted
```

![](assets/images/configmap01.png)

![](assets/images/configmap02.png)

### 3.12.3：通过configmap给容器提供环境变量

```sh
[root@k8s-master1:~/yaml/20240629/case10-configmap]# vim 2-deploy_configmap_env.yml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:  #提供两个数据
  username: "user1"
  password: "12345678"

---
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ng-deploy-80
  template:
    metadata:
      labels:
        app: ng-deploy-80
    spec:
      containers:
      - name: ng-deploy-80
        image: nginx 
        env:                      #通常都应该是这样来传环境变量，常用写法
        - name: magedu
          value: linux66 
        - name: MY_USERNAME
          valueFrom:              #指定值的来源
            configMapKeyRef:
              name: nginx-config
              key: username      
        - name: MY_PASSWORD
          valueFrom:              #指定值的来源
            configMapKeyRef:
              name: nginx-config
              key: password
        ports:
        - containerPort: 80

PS：这种方式使用不多。一般都是在yaml文件里传环境变量。更方便。

[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl apply -f 2-deploy_configmap_env.yml 
configmap/nginx-config created
deployment.apps/nginx-deployment created

[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS         AGE    IP               NODE           NOMINATED NODE   READINESS GATES
net-test1                           1/1     Running   5 (6h26m ago)    44h    10.200.155.169   172.31.8.111   <none>           <none>
net-test2                           1/1     Running   10 (6h26m ago)   3d7h   10.200.55.190    172.31.8.112   <none>           <none>
nginx-deployment-8458d9cf48-p5tk5   1/1     Running   0                14s    10.200.155.177   172.31.8.111   <none>           <none>

# 进入容器查看
[root@k8s-master1:~/yaml/20240629/case10-configmap]# kubectl exec -it nginx-deployment-8458d9cf48-p5tk5 -- bash
root@nginx-deployment-8458d9cf48-p5tk5:/# env
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_PORT=443
HOSTNAME=nginx-deployment-8458d9cf48-p5tk5
PWD=/
PKG_RELEASE=2~bookworm
HOME=/root
KUBERNETES_PORT_443_TCP=tcp://10.100.0.1:443
magedu=linux66
NJS_VERSION=0.8.4
TERM=xterm
MY_PASSWORD=12345678
SHLVL=1
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.100.0.1
MY_USERNAME=user1
KUBERNETES_SERVICE_HOST=10.100.0.1
KUBERNETES_PORT=tcp://10.100.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
NGINX_VERSION=1.27.0
NJS_RELEASE=2~bookworm
_=/usr/bin/env
root@nginx-deployment-8458d9cf48-p5tk5:/# mysql -u${MY_USERNAME}
bash: mysql: command not found
root@nginx-deployment-8458d9cf48-p5tk5:/# exit
exit
command terminated with exit code 127
[root@k8s-master1:~/yaml/20240629/case10-configmap]# 

PS：可以在容器内部使用这个变量，比如 mysql -u${MY_USERNAME}，这里就是没有mysql命令。
可以用于容器内的服务启动，通过环境变量启动。或者做些配置都可以。但是这种方式几乎没有用过。

# 通常都是通过环境变量env方式直接传，在yaml文件中直接写，这样更直观：
上面的已经存在。“magedu,linux66”


```

### 3.12.4：总结

```sh
configmap 主要是给服务提供额外的配置的。
以下是给容器提供配置的最常见的三种方式。
1、打镜像：把配置打到镜像里。
2、configmap：把配置和镜像解耦。
3、配置中心提供：程序怎么去配置中心去拿？这需要开发结合才能实现。运维自己做不了。
```

## 3.13：Secret

### 3.13.1：概念

```sh
## Secret简介
Secret的功能类似于ConfigMap给pod提供额外的配置信息，但是Secret是一种包含少量敏感信息，例如密码、令牌或密钥的对象。
Secret和configmap很类似。给pod提供额外的配置信息。
Secret是和认证，加密数据相关的。少量敏感信息，例如密码、令牌或密钥的对象。

Secret的名称必须是合法的DNS子域名。

每个Secret的大小最多为1MiB，主要是为了避免用户创建非常大的Secret进而导致API服务器和kubelet内存耗尽，不过创建很多小的Secret也可能耗尽内存，可以使用资源配额来约束每个名字空间中Secret的个数。（Secret不能放共享数据和消息队列的数据，否则消耗内存太大）

在通过yaml文件创建Secret时，可以设置data或stringData字段，data和stringData字段都是可选的。
data字段中所有键值都必须是base64编码的字符串，如果不希望执行这种base64字符串的转换操作，也可以选择设置stringData字段，其中可以使用任何非加密的字符串作为其取值。

Pod可以用三种方式的任意一种来使用Secret：
1、作为挂载到一个或多个容器上的卷中的文件（crt文件、key文件）。
2、作为容器的环境变量。
3、由kubelet在为Pod拉取镜像使使用（与镜像仓库的认证，有些公有云镜像下载需要认证，我们不想在每个node节点的docker中登录一下，我们就会把认证信息通过Secret传递到宿主机，在拉取镜像的时候，指定只用哪个secret，secret就是你的harbor或者公有云镜像仓库的阿认证信息。拿到这些认证信息，你的宿主机就不需要登录，下载私有仓库的镜像）。

## Secret简介类型
Secret类型                              使用场景
Opaque					               用户定义的任意数据 * 
Kubernetes.io/service-account-token    ServiceAccount令牌
kubernetes.io/dockercfg                ~/.dockercfg文件的序列化形式（其实就是加密）
kubernetes.io/dockerconfigjson        ~/.docker/config.json文件的序列化形式（拉镜像用） *
kubernetes.io/basic-auth               用于基本身份认证的凭据（很少用）
kubernetes.io/ssh-auth                 用于SSH身份认证的凭据
kubernetes.io/tls                      用于TLS环境，保存crt证书和key证书（常用，镜像不用证书） * 
bootstrap.kubernetes.io/token          启动引导令牌数据（也几乎不用）
* 代表常用

PS：
前面说的第3种使用方式，就是dockerconfigjson格式，用来拉镜像时使用。
tls就是网站的证书。这样，在镜像里就不需要包含证书。
```

### 3.13.2：base64加解密

```sh
[root@k8s-master1:~/yaml/20240629/case11-secret]# ll
total 20
drwxr-xr-x  3 root root  181 Jun 29 13:16 ./
drwxr-xr-x 15 root root  292 Jun 29 13:18 ../
-rw-r--r--  1 root root  144 Jun 29 13:16 1-secret-Opaque-data.yaml
-rw-r--r--  1 root root  150 Jun 29 13:16 2-secret-Opaque-stringData.yaml
-rw-r--r--  1 root root  895 Jun 29 13:16 3-secret-Opaque-mount.yaml
-rw-r--r--  1 root root 1917 Jun 29 13:16 4-secret-tls.yaml
-rw-r--r--  1 root root  834 Jun 29 13:16 5-secret-imagePull.yaml
drwxr-xr-x  2 root root   88 Jun 29 13:16 certs/

[root@k8s-master1:~/yaml/20240629/case11-secret]# echo "admin" | base64
YWRtaW4K
[root@k8s-master1:~/yaml/20240629/case11-secret]# echo "123456" | base64
MTIzNDU2Cg==
[root@k8s-master1:~/yaml/20240629/case11-secret]# echo YWRtaW4K | base64 -d
admin
[root@k8s-master1:~/yaml/20240629/case11-secret]# echo MTIzNDU2Cg== | base64 -d
123456
```

### 3.13.3：opaque

```sh
## opaque-data，这是加密的数据
[root@k8s-master1:~/yaml/20240629/case11-secret]# vim 1-secret-Opaque-data.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysecret-data
  namespace: myserver
type: Opaque
data:
  user: YWRtaW4K
  password: MTIzNDU2Cg==
  
PS：在yaml文件里，一定要是加密的数据

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl apply -f 1-secret-Opaque-data.yaml 
secret/mysecret-data created

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get secrets -n myserver
NAME                  TYPE                                  DATA   AGE
default-token-kjcvf   kubernetes.io/service-account-token   3      2d5h
mysecret-data         Opaque                                2      9s

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get secrets mysecret-data -n myserver
NAME            TYPE     DATA   AGE
mysecret-data   Opaque   2      35s

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl describe secrets mysecret-data -n myserver
Name:         mysecret-data
Namespace:    myserver
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
password:  7 bytes
user:      6 bytes

# 显示详细信息。用-o yaml选项，显示的数据更加多一些。
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get secrets mysecret-data -n myserver -o yaml
apiVersion: v1
data:
  password: MTIzNDU2Cg==
  user: YWRtaW4K
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"password":"MTIzNDU2Cg==","user":"YWRtaW4K"},"kind":"Secret","metadata":{"annotations":{},"name":"mysecret-data","namespace":"myserver"},"type":"Opaque"}
  creationTimestamp: "2024-06-30T23:43:50Z"
  name: mysecret-data
  namespace: myserver
  resourceVersion: "302105"
  uid: 1013a9b4-f681-4242-bc8b-53fad9b0d5c1
type: Opaque

PS：Secret和pod必须在同一个namespace内引用。

比如：nginx提供https服务。


## Opaque-stringData，这里是非加密的。
[root@k8s-master1:~/yaml/20240629/case11-secret]# vim 2-secret-Opaque-stringData.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysecret-stringdata
  namespace: myserver
type: Opaque
stringData:
  user: 'admin'
  password: '123456'

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl apply -f 2-secret-Opaque-stringData.yaml 
secret/mysecret-stringdata created

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get secrets -n myserver
NAME                  TYPE                                  DATA   AGE
default-token-kjcvf   kubernetes.io/service-account-token   3      2d5h
mysecret-data         Opaque                                2      4m36s
mysecret-stringdata   Opaque                                2      29s

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get secrets mysecret-stringdata -n myserver
NAME                  TYPE     DATA   AGE
mysecret-stringdata   Opaque   2      37s

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get secrets mysecret-stringdata -n myserver -o yaml
apiVersion: v1
data:
  password: MTIzNDU2
  user: YWRtaW4=
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Secret","metadata":{"annotations":{},"name":"mysecret-stringdata","namespace":"myserver"},"stringData":{"password":"123456","user":"admin"},"type":"Opaque"}
  creationTimestamp: "2024-06-30T23:47:57Z"
  name: mysecret-stringdata
  namespace: myserver
  resourceVersion: "302602"
  uid: 6d682c10-2b28-47d9-bcdd-4b189f4956fc
type: Opaque

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl delete -f 1-secret-Opaque-data.yaml 
secret "mysecret-data" deleted
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl delete -f 2-secret-Opaque-stringData.yaml 
secret "mysecret-stringdata" deleted
```

### 3.13.4：opaque使用示例

```sh
[root@k8s-master1:~/yaml/20240629/case11-secret]# vim 3-secret-Opaque-mount.yaml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myserver-myapp-app1-deployment
  namespace: myserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myserver-myapp-app1
  template:
    metadata:
      labels:
        app: myserver-myapp-app1
    spec:
      containers:
      - name: myserver-myapp-app1
        image: tomcat:7.0.94-alpine
        ports:
        - containerPort: 8080
        volumeMounts:
        - mountPath: /data/myserver/auth  #挂载目录。就从这里找账号和密码
          name: myserver-auth-secret      #这个名字和volumes里的一致
      volumes:
      - name: myserver-auth-secret        #这个名字和volumeMounts里的一致
        secret:
          secretName: mysecret-data       #这里使用secret，是它的名字

---
apiVersion: v1
kind: Service
metadata:
  name: myserver-myapp-app1
  namespace: myserver
spec:
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    nodePort: 30018
    protocol: TCP
  type: NodePort
  selector:
    app: myserver-myapp-app1

PS：这里使用的是mysecret-data这个secret。

# 创建secret
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl apply -f 1-secret-Opaque-data.yaml 
secret/mysecret-data created

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get secrets -n myserver
NAME                  TYPE                                  DATA   AGE
default-token-kjcvf   kubernetes.io/service-account-token   3      2d5h
mysecret-data         Opaque                                2      40s

# 将这个刚创建的secret给pod使用，让tomcat容器运行起来
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl apply -f 3-secret-Opaque-mount.yaml 
deployment.apps/myserver-myapp-app1-deployment created
service/myserver-myapp-app1 created

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get pods -n myserver
NAME                                             READY   STATUS    RESTARTS   AGE
myserver-myapp-app1-deployment-9c46fdb4d-lknkx   1/1     Running   0          46s

# 进入容器查看
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl exec -it myserver-myapp-app1-deployment-9c46fdb4d-lknkx -n myserver -- bash
bash-4.4# ls /data/myserver/auth/
password  user
bash-4.4# cat /data/myserver/auth/user
admin
bash-4.4# cat /data/myserver/auth/password 
123456
bash-4.4# exit
exit

PS：看到secret已挂载进去了。会显示是一个个文件。但是挂载后加密过的数据就转成明文。
挂载的时候，把key当成文件，这里就是user和password都是文件。它们都存储了各自的value值。
为什么是明文？挂载的时候，用base64进行解码，就变成了明文。
服务要使用的时候，就不需要解密，直接从挂载点找到文件后就拿到了用户名和密码。

## secret到底怎么挂的呢？它一定会在宿主机有些痕迹。
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get pod -n myserver -o wide
NAME                                             READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-app1-deployment-9c46fdb4d-lknkx   1/1     Running   0          6m39s   10.200.155.178   172.31.8.111   <none>           <none>

# 在宿主机node1上搜索
[root@k8s-node1:~]# find / -name password
......
/var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret/password
/var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret/..2024_06_30_23_57_28.4220814557/password

[root@k8s-node1:~]# ls /var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret/
password  user

PS：myserver-auth-secret就是deployment里的volume名字。

[root@k8s-node1:~]# cd /var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret/

[root@k8s-node1:/var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret]# ll
total 0
drwxrwxrwt 3 root root 120 Jun 30 19:57 ./
drwxr-xr-x 3 root root  34 Jun 30 19:57 ../
drwxr-xr-x 2 root root  80 Jun 30 19:57 ..2024_06_30_23_57_28.4220814557/
lrwxrwxrwx 1 root root  32 Jun 30 19:57 ..data -> ..2024_06_30_23_57_28.4220814557/
lrwxrwxrwx 1 root root  15 Jun 30 19:57 password -> ..data/password
lrwxrwxrwx 1 root root  11 Jun 30 19:57 user -> ..data/user

[root@k8s-node1:/var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret]# cd ..2024_06_30_23_57_28.4220814557/

[root@k8s-node1:/var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret/..2024_06_30_23_57_28.4220814557]# ll
total 8
drwxr-xr-x 2 root root  80 Jun 30 19:57 ./
drwxrwxrwt 3 root root 120 Jun 30 19:57 ../
-rw-r--r-- 1 root root   7 Jun 30 19:57 password
-rw-r--r-- 1 root root   6 Jun 30 19:57 user

[root@k8s-node1:/var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret/..2024_06_30_23_57_28.4220814557]# cat password
123456

[root@k8s-node1:/var/lib/kubelet/pods/cdec81ec-97f9-44f6-b7bb-e3abe1f7e1e1/volumes/kubernetes.io~secret/myserver-auth-secret/..2024_06_30_23_57_28.4220814557]# cat user
admin

PS：这是联合文件系统起作用导致的。

这个数据是kubelet生成的。
kubelet把数据拿过来，之后在初始化容器的时候，放到这些文件里，然后再取挂载到容器里，给容器去用。
所以这些密钥一定要看好。一定要保护好k8s的权限。

# 实际上，secret和configmap都是放到etcd
[root@k8s-etcd1:~]# etcdctl get / --keys-only --prefix | grep mysecret
/registry/secrets/myserver/mysecret-data

[root@k8s-etcd1:~]# etcdctl get /registry/secrets/myserver/mysecret-data
/registry/secrets/myserver/mysecret-data
k8s


v1SecretЄ
¢ 
mysecret-datmyserver"*$389adfbc-23c8-4e18-be7d-ea1fc81cb2972ᇴbB
0kubectl.kubernetes.io/last-applied-configurationµ{"apiVersion":"v1","data":{"password":"MTIzNDU2Cg==","user":"YWRtaW4K"},"kind":"Secret","metadata":{"annotations":{},"name":"mysecret-data","namespace":"myserver"},"type":"Opaque"}
z܁
kubectl-client-side-applyUpdatevᇴFieldsV1: 
{"f:data":{".":{},"f:password":{},"f:user":{}},"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:type":{}}B                                                                               password123456
 
useradmin
Opaque"
[root@k8s-etcd1:~]# 

# Secret的挂载流程：
你创建一个secret，发给apiserver，apiserver把它写到etcd。
服务在使用的时候，是宿主机的kubelet，把数据拿过来，放到你的宿主机，然后再挂到你的容器里去使用。

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl delete -f 1-secret-Opaque-data.yaml -f 2-secret-Opaque-stringData.yaml -f 3-secret-Opaque-mount.yaml
```

### 3.13.5：kubernetes.io/tls使用示例

```sh
## 结合nginx来实现自签名证书，实现域名的https

[root@k8s-master1:~/yaml/20240629/case11-secret]# cd certs/

# 第一步：生成CA证书。CA的私钥和公钥。
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# openssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 3650 -nodes -subj '/CN=www.ca.com'

# 第二步：生成域名私钥，并用私钥生成csr文件
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# openssl req -new -newkey rsa:4096 -keyout server.key -out server.csr -nodes -subj '/CN=www.mysite.com'

[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# ll
total 16
drwxr-xr-x 2 root root   70 Jun 30 20:28 ./
drwxr-xr-x 4 root root  201 Jun 30 20:25 ../
-rw-r--r-- 1 root root 1809 Jun 30 20:26 ca.crt
-rw------- 1 root root 3276 Jun 30 20:26 ca.key
-rw-r--r-- 1 root root 1590 Jun 30 20:28 server.csr
-rw------- 1 root root 3272 Jun 30 20:28 server.key

# 第三步：使用csr请求CA颁发证书。这属于自签名。
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# openssl x509 -req -sha256 -days 3650 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt
Signature ok
subject=CN = www.mysite.com
Getting CA Private Key
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# ll
total 20
drwxr-xr-x 2 root root   88 Jun 30 20:30 ./
drwxr-xr-x 4 root root  201 Jun 30 20:25 ../
-rw-r--r-- 1 root root 1809 Jun 30 20:26 ca.crt
-rw------- 1 root root 3276 Jun 30 20:26 ca.key
-rw-r--r-- 1 root root 1667 Jun 30 20:30 server.crt
-rw-r--r-- 1 root root 1590 Jun 30 20:28 server.csr
-rw------- 1 root root 3272 Jun 30 20:28 server.key

PS：server.crt就是申请得到的自签名证书。

# 创建tls类型的secret
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# kubectl create secret tls myserver-tls-key --cert=./server.crt --key=./server.key -n myserver
secret/myserver-tls-key created

[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# kubectl get secrets -n myserver
NAME                  TYPE                                  DATA   AGE
default-token-kjcvf   kubernetes.io/service-account-token   3      2d6h
myserver-tls-key      kubernetes.io/tls                     2      10s

PS：
kubectl create secret tls myserver-tls-key --cert=./server.crt --key=./server.key -n myserver
tls是secret的类型
myserver-tls-key是secret的名字
--cert 公钥来源于哪里
--key 私钥来源于哪里
-n 创建到哪个ns中

# 查看创建secret的命令帮助，三种不同的secret类型。
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# kubectl create secret --help
Create a secret using specified subcommand.

Available Commands:
  docker-registry Create a secret for use with a Docker registry
  generic         Create a secret from a local file, directory, or literal value
  tls             Create a TLS secret

Usage:
  kubectl create secret [flags] [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# 

# 显示刚刚创建的tls类型的secret
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# kubectl get secrets myserver-tls-key -n myserver -o yaml
apiVersion: v1
data:
  tls.crt: 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
  tls.key: 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
kind: Secret
metadata:
  creationTimestamp: "2024-07-01T00:32:52Z"
  name: myserver-tls-key
  namespace: myserver
  resourceVersion: "308087"
  uid: dea19358-431a-4825-8256-3ce7d8908921
type: kubernetes.io/tls

PS：以上的公钥私钥都可以解密。


## 创建web服务
[root@k8s-master1:~/yaml/20240629/case11-secret]# vim 4-secret-tls.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: myserver
data:
 default: |
    server {
       listen       80;
       server_name  www.mysite.com;
       listen 443 ssl;                                        #443端口
       ssl_certificate /etc/nginx/conf.d/certs/tls.crt;       #公钥配置
       ssl_certificate_key /etc/nginx/conf.d/certs/tls.key;   #私钥配置

       location / {
           root /usr/share/nginx/html; 
           index index.html;
           if ($scheme = http ){  #未加条件判断，会导致死循环
              rewrite / https://www.mysite.com permanent;
           }  

           if (!-e $request_filename) {
               rewrite ^/(.*) /index.html last;
           }
       }
    }

---
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myserver-myapp-frontend-deployment
  namespace: myserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myserver-myapp-frontend
  template:
    metadata:
      labels:
        app: myserver-myapp-frontend
    spec:
      containers:
      - name: myserver-myapp-frontend
        image: nginx:1.20.2-alpine 
        ports:
          - containerPort: 80
        volumeMounts:
          - name: nginx-config
            mountPath:  /etc/nginx/conf.d/myserver
          - name: myserver-tls-key
            mountPath:  /etc/nginx/conf.d/certs
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
          items:
             - key: default
               path: mysite.conf
      - name: myserver-tls-key
        secret:
          secretName: myserver-tls-key 


---
apiVersion: v1
kind: Service
metadata:
  name: myserver-myapp-frontend
  namespace: myserver
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30018
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    nodePort: 30019
    protocol: TCP
  selector:
    app: myserver-myapp-frontend 

PS：
镜像使用的官方的镜像，是没有公钥和私钥的。怎么办？找secret。
把secret myserver-tls-key 挂到/etc/nginx/conf.d/certs
secret里的key作为文件名挂载。这样，secret就挂到/etc/nginx/conf.d/certs目录下了，可见configmap里的配置。
nginx启动的时候就加载这些证书。
注意下面的写法：
           if ($scheme = http ){  #未加条件判断，会导致死循环
              rewrite / https://www.mysite.com permanent;
           }

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl apply -f 4-secret-tls.yaml
configmap/nginx-config created
deployment.apps/myserver-myapp-frontend-deployment created
service/myserver-myapp-frontend created

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get pods -n myserver
NAME                                                  READY   STATUS        RESTARTS   AGE
myserver-myapp-frontend-deployment-85fb884bcd-pfcml   1/1     Running       0          3m12s

# 进入到容器内部
[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl exec -it -n myserver myserver-myapp-frontend-deployment-85fb884bcd-pfcml -- sh
/ # ls
bin                   etc                   mnt                   run                   tmp
dev                   home                  opt                   sbin                  usr
docker-entrypoint.d   lib                   proc                  srv                   var
docker-entrypoint.sh  media                 root                  sys
/ # cd /etc/nginx/
/etc/nginx # ls
conf.d          fastcgi_params  modules         scgi_params
fastcgi.conf    mime.types      nginx.conf      uwsgi_params
/etc/nginx # cd conf.d/
/etc/nginx/conf.d # ls
certs         default.conf  myserver
/etc/nginx/conf.d # cd certs/
/etc/nginx/conf.d/certs # ls
tls.crt  tls.key
/etc/nginx/conf.d/certs # ls -l
total 0
lrwxrwxrwx    1 root     root            14 Jul  1 01:20 tls.crt -> ..data/tls.crt
lrwxrwxrwx    1 root     root            14 Jul  1 01:20 tls.key -> ..data/tls.key
/etc/nginx/conf.d/certs # cat tls.crt
-----BEGIN CERTIFICATE-----
MIIEojCCAooCAQEwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UEAwwKd3d3LmNhLmNv
bTAeFw0yNDA3MDEwMDMwNDdaFw0zNDA2MjkwMDMwNDdaMBkxFzAVBgNVBAMMDnd3
dy5teXNpdGUuY29tMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAxKsG
xaMSoeHrwxytr39d2KD9RH7omRUQsK7LLl8t2Kf4ygDRIoE9UQWoxS/g2+Jefjxi
w8sIWcyvSjWjqEMsFEyJUC1wpgYtgIDjVeZKbo2VqY40tw7dtVG0qD5k43T0v3uZ
T0ZWyTPlp+xO8GsZDTfp+IWbLHEaYWgqyoTUofcZbktnfgl6rruL+WCcOfKbImoL
rgnrn0tgG0Vy2tjwpGBsOAz+RKIKv6ZwugmxwH68FZ75I60DBgYR4L6xz/e0oCFl
etqURqIhtfbdiP+jI10pGDepkVRq4djMuycQg2O45rW4FgQ1rsFqlCzoZ7iJMzWe
D+WYImz1aJeT9bRgY7hUMM7oH/+9GZVmv/Y90goiJro2/A3Y2oMlkGoBn6MCLF/9
blVbCx/M6tlATuiFhrvLN/4n9SF6bfTjFQqpYjWOpWDmWVMU9JESAUHORkuLv795
HPKj4pIG7PU9GycWzr9Nr6hR7fyCRfdXQ8Ite7MI7KZX6qIfKjkzL5hO4+87lIVn
0kiD0HoNh5KEOOexmxn5pQaeKyikOHKDa7l6Z0clvTXwlqtFipOeiVJsYSXs5gVl
/QAPwFyTlIiPd3wfzEGN2372TB6hV/aPluKltV3H+KnZSAal6QHknxF/jxPotJ8Z
dD5ndZAqgqtg0yEcXxHQFmH9RpT55kpSKLxtAyECAwEAATANBgkqhkiG9w0BAQsF
AAOCAgEALhewlyWJEK8d9KORPg7A2HimpYWW8OHUihNaR/QSDtJL6Tp+hHC1CC4f
B9W+0hy4CaYpfpad53N2LQe1GbqOL7X0OJA5ka+7V84BKVTltWYyURBWnzdXAcYy
2+zXsx+7I/GRDU6w1T9CquNyHpPxrhClejaMcL9q/IvPsZZRZqkYkHCPSttK14od
HTROI6z/U4di5gQgkHFi2XTIkIXN8VJzNQBHslKzqoRaHwiVJkpNfOGMtEZ+EjKv
0nYvltUmnZOzBbwhCtS/g3cJGFohdYjlrE3dp229esvSDWeE7YP+AoY/5GFu5ayP
ANjF2CDUpkH5sqLPwTM9j0n7wQHQVa+aLukRq8yuHNZAJNd3XylCpDVy9nnfWCSl
UqRGoOO8gKSLAt6VV9dYxJOrdiMG+cItQyVACHTQ6sJwcFPsELJxRgEJf3R6slqE
fAC5byLyeaWF8poYx85nrWr0fJqwtrvN1hOkkrNKte+LpJ81rT9Pm8tJ89ZVENHZ
b1HhHbZwoLuxqD8Qk3gXHxPABBeuNdwBNY6HwV5neDldQKEHU7zEO2mytboTgcF9
UwjN9TIWJLhFcKNle+aGFzOyKToqgDl2bPjVnBhJLUW/GH7zxg34drfN/phJ8UMK
2WUqoudvakTylcuHg9bTAbO2+WwdYz2B3NCb0+qz2PTKLFe2ixs=
-----END CERTIFICATE-----
/etc/nginx/conf.d/certs # cat tls.key
-----BEGIN PRIVATE KEY-----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-----END PRIVATE KEY-----
/etc/nginx/conf.d/certs # md5sum tls.crt
e97d6101eb3cd09e76b3c7cede801c34  tls.crt
/etc/nginx/conf.d/certs # exit

# 和上传的server.crt的md5校验值一样
[root@k8s-master1:~/yaml/20240629/case11-secret]# cd certs/
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# ll
total 20
drwxr-xr-x 2 root root   88 Jun 30 20:30 ./
drwxr-xr-x 4 root root  201 Jun 30 20:25 ../
-rw-r--r-- 1 root root 1809 Jun 30 20:26 ca.crt
-rw------- 1 root root 3276 Jun 30 20:26 ca.key
-rw-r--r-- 1 root root 1667 Jun 30 20:30 server.crt
-rw-r--r-- 1 root root 1590 Jun 30 20:28 server.csr
-rw------- 1 root root 3272 Jun 30 20:28 server.key
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# md5sum server.crt
e97d6101eb3cd09e76b3c7cede801c34  server.crt

PS：经过md5校验，内容确实是一样的！！！

# 这里还需要改nginx主配置文件，要把自己的configmap文件include进来，否则就不会监听443端口
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# kubectl exec -it -n myserver myserver-myapp-frontend-deployment-85fb884bcd-pfcml -- sh
/ # netstat -tanl           
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      
tcp        0      0 :::80                   :::*                    LISTEN      
/ # cd /etc/nginx/
/etc/nginx # ls conf.d/myserver/mysite.conf
conf.d/myserver/mysite.conf
/etc/nginx # apk add vim                                # alpine的安装vim的方法
fetch https://dl-cdn.alpinelinux.org/alpine/v3.14/main/x86_64/APKINDEX.tar.gz
fetch https://dl-cdn.alpinelinux.org/alpine/v3.14/community/x86_64/APKINDEX.tar.gz
(1/4) Installing xxd (8.2.4836-r0)
(2/4) Installing lua5.3-libs (5.3.6-r0)
(3/4) Installing vim (8.2.4836-r0)
(4/4) Installing nginx-vim (1.20.2-r1)
Executing busybox-1.33.1-r7.trigger
OK: 45 MiB in 46 packages
/etc/nginx # vim nginx.conf
添加 include /etc/nginx/conf.d/myserver/*.conf;    # 这个必须添加，否则不能监听443端口。
/etc/nginx # nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
/etc/nginx # nginx -s reload
2024/07/01 01:33:05 [notice] 68#68: signal process started
/etc/nginx # netstat -tanlp
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/nginx: master pro
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      1/nginx: master pro
tcp        0      0 :::80                   :::*                    LISTEN      1/nginx: master pro
/etc/nginx # cat conf.d/myserver/mysite.conf 
server {
   listen       80;
   server_name  www.mysite.com;
   listen 443 ssl;
   ssl_certificate /etc/nginx/conf.d/certs/tls.crt;
   ssl_certificate_key /etc/nginx/conf.d/certs/tls.key;

   location / {
       root /usr/share/nginx/html; 
       index index.html;
       if ($scheme = http ){  #未加条件判断，会导致死循环
          rewrite / https://www.mysite.com permanent;
       }  

       if (!-e $request_filename) {
           rewrite ^/(.*) /index.html last;
       }
   }
}
/etc/nginx # exit
[root@k8s-master1:~/yaml/20240629/case11-secret/certs]# 

# 修改负载均衡器
[root@k8s-ha1:~]# systemctl restart haproxy.service 
listen sunnycreek-linux66-nginx-80
  bind 172.31.8.189:80
  mode tcp
  server node1 172.31.8.112:30018 check inter 3s fall 3 rise 1
  server node2 172.31.8.113:30018 check inter 3s fall 3 rise 1

listen sunnycreek-linux66-nginx-443
  bind 172.31.8.189:443
  mode tcp
  server node1 172.31.8.112:30019 check inter 3s fall 3 rise 1
  server node2 172.31.8.113:30019 check inter 3s fall 3 rise 1  

[root@k8s-ha1:~]# systemctl restart haproxy.service 

访问：
http://www.mysite.com
可以看到nginx页面

这是通过configmap加载nginx配置文件和secret实现了证书放到nginx配置文件中来实现。
现在是证书放到了nginx里，实际上后面在ingress中放置证书。
这是一个使用场景。
有些公司放nginx，有些公司放ingress里。
```

![](assets/images/secret01.png)

![secret02](assets/images/secret02.png)

![secret03](assets/images/secret03.png)

### 3.13.6：kubernetes.io/dockerconfigjson使用示例

```sh
## 存储docker registry的认证信息，在下载镜像的时候使用，这样每一个node节点就可以不登录也可以下载私有级别的镜像。
## 用于公有云中的私有镜像下载，也可以用于私有镜像仓库的下载。

## 第一步：制作自己的镜像，从master1（它安装过docker）上传到harbor服务器
# 去官网下载nginx:1.16.1-alpine-perl镜像
[root@k8s-master1:~]# docker pull nginx:1.16.1-alpine-perl

# 给下载的镜像打成自己私有harbor仓库的标签
[root@k8s-master1:~]# docker tag nginx:1.16.1-alpine-perl harbor.sunnycreek.local/baseimages/nginx:1.16.1-alpine-perl

# 上传该镜像到私有harbor仓库
[root@k8s-master1:~]# docker push harbor.sunnycreek.local/baseimages/nginx:1.16.1-alpine-perl

## 第二步：改造目前三个node节点的containerd配置，去除登录私有harbor仓库的信息
# 备份原有配置文件
[root@k8s-node1:~]# cp /etc/containerd/config.toml /etc/containerd/config.toml.bak

# 修改配置文件
[root@k8s-node1:~]# vim /etc/containerd/config.toml
修改如下原内容：
[plugins."io.containerd.grpc.v1.cri".registry.configs]
  [plugins."io.containerd.grpc.v1.cri".registry.configs."harbor.sunnycreek.local".auth]
    username = "admin"
    password = "123456"
  [plugins."io.containerd.grpc.v1.cri".registry.configs."harbor.sunnycreek.local".tls]
    ca_file = "/etc/containerd/certs.d/harbor.sunnycreek.local/harbor-ca.crt"

[plugins."io.containerd.grpc.v1.cri".registry.mirrors]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."harbor.sunnycreek.local"]
    endpoint = ["https://harbor.sunnycreek.local"]
    
新内容：将configs和mirrors下面的关于harbor.sunnycreek.local内容全部去除
[plugins."io.containerd.grpc.v1.cri".registry.configs]
[plugins."io.containerd.grpc.v1.cri".registry.mirrors]

# 重启containerd服务
[root@k8s-node1:~]# systemctl restart containerd

# 同理备份node2和node3节点上的containerd配置文件，并从node1中复制修改过的配置文件，然后重启containerd服务
[root@k8s-node2:~]# cp /etc/containerd/config.toml /etc/containerd/config.toml.bak
[root@k8s-node1:~]# scp /etc/containerd/config.toml 172.31.8.112:/etc/containerd/config.toml
[root@k8s-node2:~]# systemctl restart containerd

[root@k8s-node3:~]# cp /etc/containerd/config.toml /etc/containerd/config.toml.bak
[root@k8s-node1:~]# scp /etc/containerd/config.toml 172.31.8.113:/etc/containerd/config.toml
[root@k8s-node3:~]# systemctl restart containerd

PS：crictl pull harbor.sunnycreek.local/baseimages/nginx:1.16.1-alpine-perl 测试，均不成功。达到预期。

## 第三步：获取私有harbor仓库的登录信息，
# 在上传镜像的docker客户端中可以看到如下文件，只要docker login后就会生成。
# 这里由于node上是使用crictl，需要修改
[root@k8s-master1:~]# ll /root/.docker/config.json
-rw------- 1 root root 83 Jun 27 11:21 /root/.docker/config.json
[root@k8s-master1:~]# vim /root/.docker/config.json
{
	"auths": {
		"harbor.sunnycreek.local": {
            "username": "admin",
            "password": "123456",
            "auth": "YWRtaW46MTIzNDU2"  // This is optional, username:password base64 encoded
		}
	}
}


## 第三步：创建dockerconfigjson类型的secret，这里是docker-registry类型（和dockerconfigjson一样），因为是针对客户端crictl。
# 创建secret
[root@k8s-master1:~]# kubectl create secret docker-registry sunnycreek-registry-image-pull-key --docker-server=harbor.sunnycreek.local --docker-username=admin --docker-password=123456 --docker-email=myemail@sunnycreek.local -n myserver
secret/sunnycreek-registry-image-pull-key created

[root@k8s-master1:~]# kubectl get secrets -n myserver
NAME                                 TYPE                                  DATA   AGE
default-token-kjcvf                  kubernetes.io/service-account-token   3      2d22h
sunnycreek-registry-image-pull-key   kubernetes.io/dockerconfigjson        1      7s


[root@k8s-master1:~]# kubectl get secrets sunnycreek-registry-image-pull-key -n myserver -o yaml
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJoYXJib3Iuc3VubnljcmVlay5sb2NhbCI6eyJ1c2VybmFtZSI6ImFkbWluIiwicGFzc3dvcmQiOiIxMjM0NTYiLCJlbWFpbCI6Im15ZW1haWxAc3VubnljcmVlay5sb2NhbCIsImF1dGgiOiJZV1J0YVc0Nk1USXpORFUyIn19fQ==
kind: Secret
metadata:
  creationTimestamp: "2024-07-01T16:29:34Z"
  name: sunnycreek-registry-image-pull-key
  namespace: myserver
  resourceVersion: "350973"
  uid: ccaf38ee-b398-4e59-b4ff-eeb5d9bfbb79
type: kubernetes.io/dockerconfigjson

PS：
.dockerconfigjson 这个就是key，和tls类型的类似。
后面的加密数据就是value。也就是私有镜像仓库harbor.sunnycreek.local的认证信息。
sunnycreek-registry-image-pull-key 这是secret的名字，要在yaml文件中使用。


## 第四步：创建一个ServiceAccount
[root@k8s-master1:~/yaml/20240629/case11-secret]# vim 6-secret-sa-imagePull.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myserviceaccount
  namespace: myserver
imagePullSecrets:
- name: sunnycreek-registry-image-pull-key 

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl apply -f 6-secret-sa-imagePull.yaml 
serviceaccount/myserviceaccount created

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl get sa -n myserver
NAME               SECRETS   AGE
default            1         2d22h
myserviceaccount   1         23s

## 第五步：修改yaml文件，引用上面创建的ServiceAccount
# kubernetes.io/dockerconfigjson
[root@k8s-master1:~/yaml/20240629/case11-secret]# vim 5-secret-imagePull.yaml 
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myserver-myapp-frontend-deployment
  namespace: myserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myserver-myapp-frontend
  template:
    metadata:
      labels:
        app: myserver-myapp-frontend
    spec:
      serviceAccountName: myserviceaccount  #增加这个ServiceAccount的名字
      containers:
      - name: myserver-myapp-frontend
        image: harbor.sunnycreek.local/baseimages/nginx:1.16.1-alpine-perl  #这是私有镜像仓库的镜像地址 
        ports:
          - containerPort: 80
      imagePullSecrets:   #这里指定镜像拉取的secret
        - name: sunnycreek-registry-image-pull-key

---
apiVersion: v1
kind: Service
metadata:
  name: myserver-myapp-frontend
  namespace: myserver
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30018
    protocol: TCP
  type: NodePort
  selector:
    app: myserver-myapp-frontend 

[root@k8s-master1:~/yaml/20240629/case11-secret]# kubectl apply -f 5-secret-imagePull.yaml 
deployment.apps/myserver-myapp-frontend-deployment created
service/myserver-myapp-frontend created

结果：
这个实验没有成功，还是crictl验证问题。留待以后解决。

# 恢复实验环境
node节点的toml配置恢复
master1节点的/root/.docker/config.json恢复
清除所有刚创建的资源


PS：私有仓库，一般都是通过这样的认证，或者是每个node节点都docker login一下。
这个镜像是谁下的呢？是kubelet还是docker呢？
kubelet是拿的secret。
kubelet从apiserver拿到secret，然后再把请求传给docker，携带认证信息，把镜像请求下来。docker下完镜像后，就把容器拉起来。就可以通过宿主机的端口访问。

怎么确定就是docker去下的呢？
docker events | grep image   # 和镜像相关的一些事件
```

## 3.14：StatefulSet

### 3.14.1：概念

```sh
### StatefulSet
为了解决不同场景的需求。Deployment部署的时候是无顺序的。
比如：创建三个容器，用StatefulSet，先创建第一个pod，创建结束后才会创建第二个pod、第三个pod。只有第一个pod启动后，才会启动第二个pod。因为第二个Pod中的服务会向第一个Pod同步数据。确保服务之间的数据同步是正常的。类似于MySQL主从，ElasticSearch的集群等。

Statefulset为了解决有状态服务的集群部署、集群之间的数据同步问题（MySQL主从等）。

Statefulset所管理的Pod拥有唯一且固定的Pod名称。Pod的名称是固定的，删除重建后还是这个名称，这和deployment不一样。
比如MySQL，第一个Pod永远是-0，0对于MySQL来说，就是master。后面的1和2都会向0同步数据。

StatefulSet按照顺序对Pod进行创建、启停、伸缩和回收。先创建0，再创建1、2。

StatefulSet没有使用service。
Headless Service（无头服务，请求的解析直接解析到Pod的IP）。将Pod的名字直接解析为IP地址。就是没有service的IP地址。没有clusterIP。当然数据也是放到etcd，是从etcd查询过来的。

------------------------------------------------------------------------------------------------------------

创建Statefulset --> apiserver --> Statefulset Controller  --> myserver-mysql-0-->pvc0-->Pv0  --> 存储服务器
                                                         --> myserver-mysql-1-->pvc1-->Pv1
                                                         --> myserver-mysql-2-->pvc2-->Pv2
                                                         
                                                     
一个Pod就类似于MySQL的实体（数据库），单独管理它自己的数据。Ceph，nfs，NAS都可以。3个pod分别连接不同的后端存储是可以的。

官方文档：
https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/
```

### 3.14.2：示例

```sh
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# ll
total 4
drwxr-xr-x  2 root root  32 Jun 29 13:16 ./
drwxr-xr-x 15 root root 292 Jun 29 13:18 ../
-rw-r--r--  1 root root 682 Jun 29 13:16 1-Statefulset.yaml

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# vim 1-Statefulset.yaml 
---
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: StatefulSet 
metadata:
  name: myserver-myapp
  namespace: myserver
spec:
  replicas: 3
  serviceName: "myserver-myapp-service"
  selector:
    matchLabels:
      app: myserver-myapp-frontend
  template:
    metadata:
      labels:
        app: myserver-myapp-frontend
    spec:
      containers:
      - name: myserver-myapp-frontend
        image: nginx:1.20.2-alpine 
        ports:
          - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: myserver-myapp-service
  namespace: myserver
spec:
  clusterIP: None
  ports:
  - name: http
    port: 80
  selector:
    app: myserver-myapp-frontend 

PS：
clusterIP: None
当你访问pod的名字的时候，这个请求会被直接解析为pod的IP。
客户端还是访问service，但是这个service没有地址，而是被直接解析为pod的地址。

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver
NAME               READY   STATUS              RESTARTS   AGE
myserver-myapp-0   1/1     Running             0          7s
myserver-myapp-1   0/1     ContainerCreating   0          6s

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver
NAME               READY   STATUS              RESTARTS   AGE
myserver-myapp-0   1/1     Running             0          26s
myserver-myapp-1   1/1     Running             0          25s
myserver-myapp-2   0/1     ContainerCreating   0          3s

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver
NAME               READY   STATUS    RESTARTS   AGE
myserver-myapp-0   1/1     Running   0          4m50s
myserver-myapp-1   1/1     Running   0          4m49s
myserver-myapp-2   1/1     Running   0          4m27s

PS：
注意创建pod的顺序。
注意pod的名字。
客户端直接访问Pod去了。和以前是不一样的。
每个Pod的名字是通过数字来区别顺序的。按顺序创建。

# 创建一个pod来访问这三个statefulset
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl run net-test-myserver --image=centos:7.9.2009 sleep 10000000 -n myserver
pod/net-test-myserver created

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver
NAME                READY   STATUS    RESTARTS   AGE
myserver-myapp-0    1/1     Running   0          13m
myserver-myapp-1    1/1     Running   0          13m
myserver-myapp-2    1/1     Running   0          12m
net-test-myserver   1/1     Running   0          86s

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver -o wide
NAME                READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-0    1/1     Running   0          16m     10.200.155.185   172.31.8.111   <none>           <none>
myserver-myapp-1    1/1     Running   0          16m     10.200.70.88     172.31.8.113   <none>           <none>
myserver-myapp-2    1/1     Running   0          15m     10.200.55.146    172.31.8.112   <none>           <none>
net-test-myserver   1/1     Running   0          4m41s   10.200.155.186   172.31.8.111   <none>           <none>

# 查svc，是个无头服务
[root@k8s-master1:~]# kubectl get svc -n myserver
NAME                     TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
myserver-myapp-service   ClusterIP   None         <none>        80/TCP    14m

# 进入容器来ping
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl exec -it net-test-myserver -n myserver -- bash
[root@net-test-myserver /]# ping myserver-myapp-service
PING myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88) 56(84) bytes of data.
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=1 ttl=62 time=0.398 ms
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=2 ttl=62 time=0.288 ms
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=3 ttl=62 time=0.376 ms
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=4 ttl=62 time=0.545 ms
^C
--- myserver-myapp-service.myserver.svc.sunnycreek.local ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3039ms
rtt min/avg/max/mdev = 0.288/0.401/0.545/0.095 ms
[root@net-test-myserver /]# ping myserver-myapp-service
PING myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146) 56(84) bytes of data.
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=1 ttl=62 time=0.297 ms
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=2 ttl=62 time=0.442 ms
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=3 ttl=62 time=0.619 ms
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=4 ttl=62 time=0.520 ms
^C
--- myserver-myapp-service.myserver.svc.sunnycreek.local ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3044ms
rtt min/avg/max/mdev = 0.297/0.469/0.619/0.119 ms
[root@net-test-myserver /]# ping myserver-myapp-service
PING myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146) 56(84) bytes of data.
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=1 ttl=62 time=0.247 ms
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=2 ttl=62 time=1.16 ms
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=3 ttl=62 time=0.428 ms
64 bytes from myserver-myapp-2.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.55.146): icmp_seq=4 ttl=62 time=0.366 ms
^C
--- myserver-myapp-service.myserver.svc.sunnycreek.local ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3011ms
rtt min/avg/max/mdev = 0.247/0.550/1.160/0.358 ms
[root@net-test-myserver /]# ping myserver-myapp-service
PING myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88) 56(84) bytes of data.
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=1 ttl=62 time=0.399 ms
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=2 ttl=62 time=0.600 ms
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=3 ttl=62 time=0.518 ms
64 bytes from myserver-myapp-1.myserver-myapp-service.myserver.svc.sunnycreek.local (10.200.70.88): icmp_seq=4 ttl=62 time=0.295 ms
^C
--- myserver-myapp-service.myserver.svc.sunnycreek.local ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3038ms
rtt min/avg/max/mdev = 0.295/0.453/0.600/0.115 ms
[root@net-test-myserver /]# exit
exit
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# 

PS：
可以看到，是以轮询的方式解析到不同的地址上。
每次ping的结果是不同的地址。

客户端通过coredns拿到解析，svc后面有三个pod，每次请求的话都会返回一个地址，中间的请求没有经过svc。这就是无头服务。头可以理解为clusterIP，请求直接丢给后端pod。为什么这样呢？
因为服务是有状态的，需要持久固定地访问主服务。
比如MySQL，master，写数据只能写到mysql-0，读写分离。
读的话，就去mysql-1和mysql-2是无头服务，这个请求会以轮询方式转发到1和2上。
这样的话，方便在代码层业务层实现读写分离。最好的读写分离是在代码层判断。通过mycat实现做读写分离也可以，但是效果不如再代码层控制好。

每个pod的固定名字，会跟随一生。删除后，k8s会自动重建，还会使用原来的名字，还会把之前的存储挂载过来。
slave节点或者backup节点还会找它去同步数据。
域名所对应的A记录会动态更新。对业务的影响很低。比物理机和虚拟机对业务的影响低很多。
# 删除一个pod，k8s自动重建
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl delete pods myserver-myapp-0 -n myserver
pod "myserver-myapp-0" deleted

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver -o wide
NAME                READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-0    1/1     Running   0          4s    10.200.155.187   172.31.8.111   <none>           <none>
myserver-myapp-1    1/1     Running   0          28m   10.200.70.88     172.31.8.113   <none>           <none>
myserver-myapp-2    1/1     Running   0          28m   10.200.55.146    172.31.8.112   <none>           <none>
net-test-myserver   1/1     Running   0          16m   10.200.155.186   172.31.8.111   <none>           <none>

PS：
删除后，还会重建，也还会使用之前的存储。
可能会在别的node上重建，地址也会变，A记录会动态更新。

# 将副本数改成5
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# vim 1-Statefulset.yaml 
改动内容：
replicas: 5

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl apply -f 1-Statefulset.yaml 
statefulset.apps/myserver-myapp configured
service/myserver-myapp-service unchanged

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver
NAME                READY   STATUS    RESTARTS   AGE
myserver-myapp-0    1/1     Running   0          4m55s
myserver-myapp-1    1/1     Running   0          33m
myserver-myapp-2    1/1     Running   0          32m
myserver-myapp-3    1/1     Running   0          5s
myserver-myapp-4    1/1     Running   0          4s
net-test-myserver   1/1     Running   0          21m

PS：
副本数从3开始创建。

# 将副本数改回2
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# vim 1-Statefulset.yaml 
改动内容：
replicas: 2

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl apply -f 1-Statefulset.yaml 
statefulset.apps/myserver-myapp configured
service/myserver-myapp-service unchanged

[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl get pods -n myserver
NAME                READY   STATUS    RESTARTS   AGE
myserver-myapp-0    1/1     Running   0          6m55s
myserver-myapp-1    1/1     Running   0          35m
net-test-myserver   1/1     Running   0          23m

PS：
副本数改成2后，从后往前删除。

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case12-Statefulset]# kubectl delete -f 1-Statefulset.yaml 
statefulset.apps "myserver-myapp" deleted
service "myserver-myapp-service" deleted
```

## 3.15：DaemonSet

### 3.15.1：概念

```sh
## DaemonSet
Daemonset在当前集群中每个node节点上运行同一个Pod，当有新的节点加入集群时也会为新的节点配置相同的Pod，当节点从集群中移除时其Pod也会被kubernetes回收，删除DaemonSet控制器将删除其创建的所有pod。
也可能是部分节点，有些node打些标签，是不参与容器调度的。

比如calico就会用到DaemonSet。
通常就是网络组件，日志收集，还有监控的时候会用到。
每个节点部署收集日志的客户端pod，收集当前节点pod的运行状态，业务的日志。再输出到ES服务器上。

node节点有成百上千，如果一个一个部署就太麻烦了。
pod服务就会自动部署在符合条件的node节点上。这里可以写些污点，容忍，包括一些特定的调度策略等。一旦指定好，这些节点上就会运行基于DaemonSet控制器来创建的pod。这些pod就有可能做些监控，收集当前节点上pod的运行状态。也可能是日志收集，收集完后通过DaemonSet控制的pod来发送给ES等。当有新节点加入的话，也会通过DaemonSet来配置相应的pod。

比如：
1.监控
每个节点上都要部署exporter，发送给Prometheus。几百个节点的话，手动部署就非常麻烦。
让k8s通过DaemonSet去管理，只要写个yaml文件就可以了。

2.日志收集
每个节点上都运行日志收集。用DaemonSet管理。

不需要基于service的报文转发。

DaemonSet --> 进程集合

如果DaemonSet来收集宿主机的日志，就通过volume挂载的方式来处理。
```

### 3.15.2：示例--每个宿主机均部署nginx

```sh
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# ll
total 12
drwxr-xr-x  2 root root  108 Jun 29 13:16 ./
drwxr-xr-x 15 root root  292 Jun 29 13:18 ../
-rw-r--r--  1 root root  964 Jun 29 13:16 1-DaemonSet-webserverp.yaml
-rw-r--r--  1 root root 1182 Jun 29 13:16 2-DaemonSet-fluentd.yaml
-rw-r--r--  1 root root 1479 Jun 29 13:16 3-DaemonSet-prometheus.yaml

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# vim 1-DaemonSet-webserverp.yaml 
---
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
kind: DaemonSet 
metadata:
  name: myserver-myapp
  namespace: myserver
spec:
  selector:
    matchLabels:
      app: myserver-myapp-frontend
  template:
    metadata:
      labels:
        app: myserver-myapp-frontend
    spec:
      tolerations:   #还会对这个节点进行调度，就是创建pod
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master  #这是标签
        operator: Exists
        effect: NoSchedule
      hostNetwork: true
      hostPID: true
      containers:
      - name: myserver-myapp-frontend
        image: nginx:1.20.2-alpine 
        ports:
          - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: myserver-myapp-frontend
  namespace: myserver
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80
    nodePort: 30018
    protocol: TCP
  type: NodePort
  selector:
    app: myserver-myapp-frontend

PS：
key: node-role.kubernetes.io/master 容忍有这个标签的节点
也就是说在master上也可以运行DaemonSet。
hostNetwork: true 表示直接使用宿主机网络，不做跨主机转发，避免service开销。
hostPID：true 使用宿主机的PID。
containerPort: 80 直接使用宿主机的80端口，是直接监听在宿主机的协议栈上。
后面的service就意义不大了，因为会在宿主机直接监听80端口。

有时候会暴露公司某个服务的访问入口，我们可以使用Ingress，也可以使用NodePort
在每个节点上都创建一个nginx。
访问NodePort时，都直接转给service，不需要跨主机转发。如果pod只有一个，虽然说你的NodePort在每个主机都会监听。如果你访问的是node2上，这里没有nginx容器，node2会以跨主机请求的方式将对pod的请求转给node1，这个请求是由损耗的，中间需要隧道封装。到了对端主机后，再转给nginx，然后返回给node2，再返回客户端，就有网络开销。所以就所有节点都运行一个nginx，无论请求哪个nginx，都可以在当前主机完成转发。环境有几十个node节点，提高主机的性能。每个节点上部署一个nginx，直接处理请求，不处理跨主机转发。

# 创建daemonset
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl apply -f 1-DaemonSet-webserverp.yaml 
daemonset.apps/myserver-myapp created
service/myserver-myapp-frontend created

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl get pods -n myserver
NAME                   READY   STATUS    RESTARTS   AGE
myserver-myapp-mdtcv   1/1     Running   0          16s
myserver-myapp-msmjv   1/1     Running   0          16s
myserver-myapp-pkvgk   1/1     Running   0          16s
myserver-myapp-rvdph   1/1     Running   0          16s
myserver-myapp-t5lt5   1/1     Running   0          16s
myserver-myapp-w6k2q   1/1     Running   0          16s
net-test-myserver      1/1     Running   0          98m

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl get pods -n myserver -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-mdtcv   1/1     Running   0          24s   172.31.8.112     172.31.8.112   <none>           <none>
myserver-myapp-msmjv   1/1     Running   0          24s   172.31.8.101     172.31.8.101   <none>           <none>
myserver-myapp-pkvgk   1/1     Running   0          24s   172.31.8.103     172.31.8.103   <none>           <none>
myserver-myapp-rvdph   1/1     Running   0          24s   172.31.8.113     172.31.8.113   <none>           <none>
myserver-myapp-t5lt5   1/1     Running   0          24s   172.31.8.111     172.31.8.111   <none>           <none>
myserver-myapp-w6k2q   1/1     Running   0          24s   172.31.8.102     172.31.8.102   <none>           <none>
net-test-myserver      1/1     Running   0          98m   10.200.155.186   172.31.8.111   <none>           <none>

PS：可以看到，在宿主机的每个节点上都会创建一个pod，包括master和node。

通常是访问量比较大的服务，比如nginx是个入口，通过hostNetwork进行暴露。

# 查看某个节点的端口80
[root@k8s-node3:~]# ss -ntl
State        Recv-Q       Send-Q              Local Address:Port                Peer Address:Port       Process       
LISTEN       0            511                       0.0.0.0:80                       0.0.0.0:*  
......

访问宿主机的地址
http://172.31.8.111-113
http://172.31.8.101-103
就是访问容器，可以看到nginx页面。

PS：这种访问就不需要再基于service进行跨主机的转发。可以提高业务的响应性能。Ingress就是这么做的，在每个主机监听80和443，一访问80或者443，就直接丢给某个容器了。
容器的地址就是宿主机的地址。

DaemonSet会在每个node节点上运行相同的pod。

# 注释掉hostNetwork
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# vim 1-DaemonSet-webserverp.yaml
修改这两行为：（删除也可以）
      #hostNetwork: true
      #hostPID: true
      
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl apply -f 1-DaemonSet-webserverp.yaml 
daemonset.apps/myserver-myapp configured
service/myserver-myapp-frontend unchanged

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl get pods -n myserver -o wide
NAME                   READY   STATUS    RESTARTS   AGE    IP               NODE           NOMINATED NODE   READINESS GATES
myserver-myapp-2lk4c   1/1     Running   0          17s    10.200.55.148    172.31.8.112   <none>           <none>
myserver-myapp-brwvj   1/1     Running   0          15s    10.200.241.193   172.31.8.102   <none>           <none>
myserver-myapp-fhgqq   1/1     Running   0          13s    10.200.155.189   172.31.8.111   <none>           <none>
myserver-myapp-kxh9n   1/1     Running   0          11s    10.200.92.65     172.31.8.103   <none>           <none>
myserver-myapp-scdnh   1/1     Running   0          18s    10.200.175.193   172.31.8.101   <none>           <none>
myserver-myapp-ssd7s   1/1     Running   0          9s     10.200.70.90     172.31.8.113   <none>           <none>
net-test-myserver      1/1     Running   0          108m   10.200.155.186   172.31.8.111   <none>           <none>

PS：可以看到容器的地址都变为calico的网路地址了，就不是宿主机地址。
这种场景最好使用hostNetwork。
否则就可能使用iptabels的规则来进行转发，性能就会有损耗。

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl delete -f 1-DaemonSet-webserverp.yaml 
daemonset.apps "myserver-myapp" deleted
service "myserver-myapp-frontend" deleted
```

### 3.15.3：示例--每个主机均部署日志收集

```sh
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# vim 2-DaemonSet-fluentd.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

PS：
搜集日志。这个日志从宿主机搜集而来。
将宿主机的日志目录挂载容器中。

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl apply -f 2-DaemonSet-fluentd.yaml 
daemonset.apps/fluentd-elasticsearch created

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl get pods -n kube-system -o wide
NAME                                       READY   STATUS    RESTARTS         AGE     IP               NODE           NOMINATED NODE   READINESS GATES
calico-kube-controllers-754966f84c-c2jzd   1/1     Running   11 (7h12m ago)   3d23h   172.31.8.112     172.31.8.112   <none>           <none>
calico-node-2rct4                          1/1     Running   9 (7h13m ago)    2d19h   172.31.8.101     172.31.8.101   <none>           <none>
calico-node-jc27c                          1/1     Running   9 (7h12m ago)    2d19h   172.31.8.103     172.31.8.103   <none>           <none>
calico-node-n6vlx                          1/1     Running   9 (7h12m ago)    2d19h   172.31.8.113     172.31.8.113   <none>           <none>
calico-node-sg5sl                          1/1     Running   9 (7h12m ago)    2d19h   172.31.8.111     172.31.8.111   <none>           <none>
calico-node-twlwk                          1/1     Running   10 (7h13m ago)   2d19h   172.31.8.102     172.31.8.102   <none>           <none>
calico-node-x2d97                          1/1     Running   9 (7h12m ago)    2d19h   172.31.8.112     172.31.8.112   <none>           <none>
coredns-5c8ffdf657-h7gdq                   1/1     Running   10 (7h12m ago)   3d23h   10.200.55.142    172.31.8.112   <none>           <none>
coredns-5c8ffdf657-lxqb4                   1/1     Running   11 (7h12m ago)   4d3h    10.200.55.144    172.31.8.112   <none>           <none>
fluentd-elasticsearch-4q5w8                1/1     Running   0                61s     10.200.241.194   172.31.8.102   <none>           <none>
fluentd-elasticsearch-87679                1/1     Running   0                61s     10.200.155.190   172.31.8.111   <none>           <none>
fluentd-elasticsearch-d6sp7                1/1     Running   0                61s     10.200.175.194   172.31.8.101   <none>           <none>
fluentd-elasticsearch-rwtcv                1/1     Running   0                61s     10.200.70.91     172.31.8.113   <none>           <none>
fluentd-elasticsearch-x5jdf                1/1     Running   0                61s     10.200.92.66     172.31.8.103   <none>           <none>
fluentd-elasticsearch-zvcns                1/1     Running   0                61s     10.200.55.141    172.31.8.112   <none>           <none>

PS：在每个节点上都运行fluentd。具体怎么收集日志，要看具体配置，主要是镜像里需要怎么打。

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl delete -f 2-DaemonSet-fluentd.yaml 
daemonset.apps "fluentd-elasticsearch" deleted
```

### 3.15.4：示例--每个主机均部署prometheus exporter

```sh
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# vim 3-DaemonSet-prometheus.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring 
  labels:
    k8s-app: node-exporter
spec:
  selector:
    matchLabels:
        k8s-app: node-exporter
  template:
    metadata:
      labels:
        k8s-app: node-exporter
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
      containers:
      - image: prom/node-exporter:v1.3.1 
        imagePullPolicy: IfNotPresent
        name: prometheus-node-exporter
        ports:
        - containerPort: 9100
          hostPort: 9100
          protocol: TCP
          name: metrics
        volumeMounts:
        - mountPath: /host/proc
          name: proc
        - mountPath: /host/sys
          name: sys
        - mountPath: /host
          name: rootfs
        args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs
          hostPath:
            path: /
      hostNetwork: true
      hostPID: true
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
  labels:
    k8s-app: node-exporter
  name: node-exporter
  namespace: monitoring 
spec:
  type: NodePort
  ports:
  - name: http
    port: 9100
    nodePort: 39100
    protocol: TCP
  selector:
    k8s-app: node-exporter

PS：
部署prometheus在每个节点。
使用hostNetwork，这个service就用不上的。

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl create ns monitoring
namespace/monitoring created

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl apply -f 3-DaemonSet-prometheus.yaml 
daemonset.apps/node-exporter created
service/node-exporter created

[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl get pods -n monitoring -o wide
NAME                  READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
node-exporter-6d6hr   1/1     Running   0          10m   172.31.8.112   172.31.8.112   <none>           <none>
node-exporter-dcjgd   1/1     Running   0          10m   172.31.8.103   172.31.8.103   <none>           <none>
node-exporter-gkdth   1/1     Running   0          10m   172.31.8.102   172.31.8.102   <none>           <none>
node-exporter-snlxn   1/1     Running   0          10m   172.31.8.101   172.31.8.101   <none>           <none>
node-exporter-tpcnf   1/1     Running   0          10m   172.31.8.111   172.31.8.111   <none>           <none>
node-exporter-wc9pk   1/1     Running   0          10m   172.31.8.113   172.31.8.113   <none>           <none>

PS：需要停掉master1上的docker服务，否则不能正常拉取镜像。
在k8s里大大简化安装的操作。非常适合在每个节点上都部署同一个pod的场景。
日志收集，监控，暴露端口等。

PS：日志收集三种方式
1 原生方式 ---- 直接用logrotate 
2 DaemonSet方式 ---- 部署daemonset容器
3 SideCar方式 ---- pod里部署另外一个容器，收集业务容器的访问日志。它是辅助容器。主容器承载业务请求，sidecar容器就是收集日志。

访问任何一个宿主机的9100端口
http://172.31.8.111:9100/metrics
9100就是prometheus的搜集数据的端口

# 恢复环境
[root@k8s-master1:~/yaml/20240629/case13-DaemonSet]# kubectl delete -f 3-DaemonSet-prometheus.yaml 
daemonset.apps "node-exporter" deleted
service "node-exporter" deleted
```

![](assets/images/daemonset_exporter01.png)

![daemonset_exporter02](assets/images/daemonset_exporter02.png)
